<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Robust Extended Kalman Filter update. Part 2: comparison with outlier rejection approach | Navigating Uncertainty</title>
<meta name=keywords content><meta name=description content="Here I compare properties of the robust EKF update algorithm with a classical outlier rejection approach based on normalized innovation checks.
Outlier rejection approach A well known and efficient approach in Kalman filtering is analyzing whether normalized measurement innovations are reasonable. The innovation $$ e = z - H x^- $$ has theoretical covariance $$ S = H P^- H^T + R $$ The quadratic form $e^T S^{-1} e$ is expected to have a $\chi$-squared distribution and its excessively large values indicate a possible outliers, which should be rejected."><meta name=author content="Nikolay Mayorov"><link rel=canonical href=https://nmayorov.github.io/posts/robust_ekf_vs_thresholds/><link crossorigin=anonymous href=/assets/css/stylesheet.a801c217e3c96db9c9df36cb1d2216a9bc5d9fa77b72e2afcdf47fb847f8f015.css integrity="sha256-qAHCF+PJbbnJ3zbLHSIWqbxdn6d7cuKvzfR/uEf48BU=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://nmayorov.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://nmayorov.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://nmayorov.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://nmayorov.github.io/apple-touch-icon.png><link rel=mask-icon href=https://nmayorov.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script><meta property="og:title" content="Robust Extended Kalman Filter update. Part 2: comparison with outlier rejection approach"><meta property="og:description" content="Here I compare properties of the robust EKF update algorithm with a classical outlier rejection approach based on normalized innovation checks.
Outlier rejection approach A well known and efficient approach in Kalman filtering is analyzing whether normalized measurement innovations are reasonable. The innovation $$ e = z - H x^- $$ has theoretical covariance $$ S = H P^- H^T + R $$ The quadratic form $e^T S^{-1} e$ is expected to have a $\chi$-squared distribution and its excessively large values indicate a possible outliers, which should be rejected."><meta property="og:type" content="article"><meta property="og:url" content="https://nmayorov.github.io/posts/robust_ekf_vs_thresholds/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-08-03T00:00:00+00:00"><meta property="article:modified_time" content="2023-08-03T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Robust Extended Kalman Filter update. Part 2: comparison with outlier rejection approach"><meta name=twitter:description content="Here I compare properties of the robust EKF update algorithm with a classical outlier rejection approach based on normalized innovation checks.
Outlier rejection approach A well known and efficient approach in Kalman filtering is analyzing whether normalized measurement innovations are reasonable. The innovation $$ e = z - H x^- $$ has theoretical covariance $$ S = H P^- H^T + R $$ The quadratic form $e^T S^{-1} e$ is expected to have a $\chi$-squared distribution and its excessively large values indicate a possible outliers, which should be rejected."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://nmayorov.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Robust Extended Kalman Filter update. Part 2: comparison with outlier rejection approach","item":"https://nmayorov.github.io/posts/robust_ekf_vs_thresholds/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Robust Extended Kalman Filter update. Part 2: comparison with outlier rejection approach","name":"Robust Extended Kalman Filter update. Part 2: comparison with outlier rejection approach","description":"Here I compare properties of the robust EKF update algorithm with a classical outlier rejection approach based on normalized innovation checks.\nOutlier rejection approach A well known and efficient approach in Kalman filtering is analyzing whether normalized measurement innovations are reasonable. The innovation $$ e = z - H x^- $$ has theoretical covariance $$ S = H P^- H^T + R $$ The quadratic form $e^T S^{-1} e$ is expected to have a $\\chi$-squared distribution and its excessively large values indicate a possible outliers, which should be rejected.","keywords":[],"articleBody":"Here I compare properties of the robust EKF update algorithm with a classical outlier rejection approach based on normalized innovation checks.\nOutlier rejection approach A well known and efficient approach in Kalman filtering is analyzing whether normalized measurement innovations are reasonable. The innovation $$ e = z - H x^- $$ has theoretical covariance $$ S = H P^- H^T + R $$ The quadratic form $e^T S^{-1} e$ is expected to have a $\\chi$-squared distribution and its excessively large values indicate a possible outliers, which should be rejected. For scalar measurements it is equivalent to rejection measurements when the innovation is larger than (for example) 3$\\sigma$ with $\\sigma = \\sqrt{S}$.\nIn the robust optimization approach the following term appears in the cost function: $$ \\rho\\left((z - H x)^T R^{-1} (z - H x)\\right) $$ So here the innovations are scaled by $R$, but effectively rather “postfit” innovations are involved (i.e. it is evaluated at the final solution $x^+$). Intuitively we want to see that the decision of whether a measurement $z$ is a “soft” outlier is based on the normalized innovations $(z - H x^-)^T S^{-1} (z - H x^-)$ as well. However for the robust loss function approach it’s not obvious at all.\nIt can be shown that $$ (z - H x^-)^T S^{-1} (z - H x^-) \\neq (z - H x^+)^T R^{-1} (z - H x^+) $$ So there is no straightforward way to show the connection or equivalence.\nAnalytical considerations Let’s write a robust optimization problem for a linear measurement model: $$ \\min_x \\frac{1}{2} (x - x^-)^T (P^-)^{-1} (x - x^-) + \\frac{1}{2} \\rho \\left( (z - H x)^T R^{-1} (z - H x) \\right) $$ To find the minimum we equate the gradient to zero and get the equation: $$ (P^-)^{-1} (x^+ - x^-) = \\rho^\\prime H^T R^{-1} (z - H x^+) $$ Note that $\\rho^\\prime$ is evaluated for the unknown $x^+$. This “balancing” nonlinear equation can be solved to find $x^+$. We know that the standard model has $\\rho^\\prime = 1$ and a robust model has $\\rho^\\prime \u003c 1$ and thus the correction $x^+ - x^-$ is expected to be smaller for a robust model compared to the standard model.\nTo get further insides consider a scalar case (robust_ekf_analysis $$ \\rho(u) = \\begin{cases} u \u0026 u \u003c 1 \\\\ 2 \\sqrt{u} - 1 \u0026 u \\geq 1 \\end{cases} $$ For this problem we can find the solution analytically: $$ x^+ = \\begin{cases} \\dfrac{P}{S} z \u0026 z \u003c z_* \\\\[1em] \\dfrac{P}{\\sqrt{R}} \u0026 z \\geq z_* \\end{cases} $$ Where the threshold value $$ z_* = \\sqrt{1 + \\frac{P}{R}} \\sqrt{S} $$ The result is quite interesting, not to say confusing:\nIndeed we get a threshold proportional to $\\sqrt{S}$, however it is additionally scaled by a factor larger than 1 which goes up as $P/R$ growths For $z$ not exceeding the threshold we get the standard optimal linear solution as expected For $z$ exceeding the threshold the solution stays constant These properties are not “bad”, however it’s hard to give them an intuitive interpretation, which I usually consider undesirable.\nNumerical examples Below numerical solutions to the same simplified problem with $R = 1$ and different values of $P$ and different loss functions are shown. The line named “linear+rejection” depicts the solution with the outlier rejection with 3$\\sigma$ threshold.\nThe following can be noted about these plots:\n“Soft-L1” loss works like a smoothed version of Huber loss Cauchy loss works significantly different than “soft-L1” and Huber: for severe outliers the state correction goes to zero Generally for all loss functions the soft rejection threshold (relative to $\\sqrt{S}$) increases with $P/R$ Cauchy loss behaves more similarly to the classical outlier rejection approach and from an engineering standpoint perhaps looks the most appealing In general a particular choice of a loss function significantly changes the behavior Discussion We see that there is indeed a connection between the optimization with robust loss functions and the classical outlier rejection approach. However this connection is not as simple and straightforward as we might’ve expected or hoped for. It means that for filtering algorithms the simple rejection approach might be more preferable. But in batch optimization, a robust loss function might be preferred to properly define the optimization problem as outlier rejection by a threshold is not differentiable. Otherwise some ad-hoc “adjustment” algorithms will be required.\n","wordCount":"722","inLanguage":"en","datePublished":"2023-08-03T00:00:00Z","dateModified":"2023-08-03T00:00:00Z","author":{"@type":"Person","name":"Nikolay Mayorov"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://nmayorov.github.io/posts/robust_ekf_vs_thresholds/"},"publisher":{"@type":"Organization","name":"Navigating Uncertainty","logo":{"@type":"ImageObject","url":"https://nmayorov.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://nmayorov.github.io accesskey=h title="Navigating Uncertainty (Alt + H)">Navigating Uncertainty</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://nmayorov.github.io/archives title="All posts"><span>All posts</span></a></li><li><a href=https://nmayorov.github.io/tags title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://nmayorov.github.io>Home</a>&nbsp;»&nbsp;<a href=https://nmayorov.github.io/posts/>Posts</a></div><h1 class=post-title>Robust Extended Kalman Filter update. Part 2: comparison with outlier rejection approach</h1><div class=post-meta><span title='2023-08-03 00:00:00 +0000 UTC'>August 3, 2023</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;Nikolay Mayorov</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#outlier-rejection-approach aria-label="Outlier rejection approach">Outlier rejection approach</a></li><li><a href=#analytical-considerations aria-label="Analytical considerations">Analytical considerations</a></li><li><a href=#numerical-examples aria-label="Numerical examples">Numerical examples</a></li><li><a href=#discussion aria-label=Discussion>Discussion</a></li></ul></div></details></div><div class=post-content><p>Here I compare properties of the <a href=https://nmayorov.github.io/posts/robust_ekf/>robust EKF update algorithm</a> with a classical outlier rejection approach based on normalized innovation checks.</p><h1 id=outlier-rejection-approach>Outlier rejection approach<a hidden class=anchor aria-hidden=true href=#outlier-rejection-approach>#</a></h1><p>A well known and efficient approach in Kalman filtering is analyzing whether normalized measurement innovations are reasonable.
The innovation
$$
e = z - H x^-
$$
has theoretical covariance
$$
S = H P^- H^T + R
$$
The quadratic form $e^T S^{-1} e$ is expected to have a $\chi$-squared distribution and its excessively large values indicate a possible outliers, which should be rejected.
For scalar measurements it is equivalent to rejection measurements when the innovation is larger than (for example) 3$\sigma$ with $\sigma = \sqrt{S}$.</p><p>In the robust optimization approach the following term appears in the cost function:
$$
\rho\left((z - H x)^T R^{-1} (z - H x)\right)
$$
So here the innovations are scaled by $R$, but effectively rather &ldquo;postfit&rdquo; innovations are involved (i.e. it is evaluated at the final solution $x^+$).
Intuitively we want to see that the decision of whether a measurement $z$ is a &ldquo;soft&rdquo; outlier is based on the normalized innovations $(z - H x^-)^T S^{-1} (z - H x^-)$ as well.
However for the robust loss function approach it&rsquo;s not obvious at all.</p><p>It can be shown that
$$
(z - H x^-)^T S^{-1} (z - H x^-) \neq (z - H x^+)^T R^{-1} (z - H x^+)
$$
So there is no straightforward way to show the connection or equivalence.</p><h1 id=analytical-considerations>Analytical considerations<a hidden class=anchor aria-hidden=true href=#analytical-considerations>#</a></h1><p>Let&rsquo;s write a robust optimization problem for a linear measurement model:
$$
\min_x \frac{1}{2} (x - x^-)^T (P^-)^{-1} (x - x^-) + \frac{1}{2} \rho \left( (z - H x)^T R^{-1} (z - H x) \right)
$$
To find the minimum we equate the gradient to zero and get the equation:
$$
(P^-)^{-1} (x^+ - x^-) = \rho^\prime H^T R^{-1} (z - H x^+)
$$
Note that $\rho^\prime$ is evaluated for the unknown $x^+$.
This &ldquo;balancing&rdquo; nonlinear equation can be solved to find $x^+$.
We know that the standard model has $\rho^\prime = 1$ and a robust model has $\rho^\prime &lt; 1$ and thus the correction $x^+ - x^-$ is expected to be smaller for a robust model compared to the standard model.</p><p>To get further insides consider a scalar case (robust_ekf_analysis
$$
\rho(u) = \begin{cases}
u & u &lt; 1 \\
2 \sqrt{u} - 1 & u \geq 1
\end{cases}
$$
For this problem we can find the solution analytically:
$$
x^+ = \begin{cases}
\dfrac{P}{S} z & z &lt; z_* \\[1em]
\dfrac{P}{\sqrt{R}} & z \geq z_*
\end{cases}
$$
Where the threshold value
$$
z_* = \sqrt{1 + \frac{P}{R}} \sqrt{S}
$$
The result is quite interesting, not to say confusing:</p><ol><li>Indeed we get a threshold proportional to $\sqrt{S}$, however it is additionally scaled by a factor larger than 1 which goes up as $P/R$ growths</li><li>For $z$ not exceeding the threshold we get the standard optimal linear solution as expected</li><li>For $z$ exceeding the threshold the solution stays constant</li></ol><p>These properties are not &ldquo;bad&rdquo;, however it&rsquo;s hard to give them an intuitive interpretation, which I usually consider undesirable.</p><h1 id=numerical-examples>Numerical examples<a hidden class=anchor aria-hidden=true href=#numerical-examples>#</a></h1><p>Below numerical solutions to the same simplified problem with $R = 1$ and different values of $P$ and different loss functions are shown.
The line named &ldquo;linear+rejection&rdquo; depicts the solution with the outlier rejection with 3$\sigma$ threshold.</p><p><img loading=lazy src="figs/P=0.1.svg" alt="P=0.1">
<img loading=lazy src="figs/P=1.0.svg" alt="P=1.0">
<img loading=lazy src="figs/P=10.0.svg" alt="P=10.0"></p><p>The following can be noted about these plots:</p><ol><li>&ldquo;Soft-L1&rdquo; loss works like a smoothed version of Huber loss</li><li>Cauchy loss works significantly different than &ldquo;soft-L1&rdquo; and Huber: for severe outliers the state correction goes to zero</li><li>Generally for all loss functions the soft rejection threshold (relative to $\sqrt{S}$) increases with $P/R$</li><li>Cauchy loss behaves more similarly to the classical outlier rejection approach and from an engineering standpoint perhaps looks the most appealing</li><li>In general a particular choice of a loss function significantly changes the behavior</li></ol><h1 id=discussion>Discussion<a hidden class=anchor aria-hidden=true href=#discussion>#</a></h1><p>We see that there is indeed a connection between the optimization with robust loss functions and the classical outlier rejection approach.
However this connection is not as simple and straightforward as we might&rsquo;ve expected or hoped for.
It means that for filtering algorithms the simple rejection approach might be more preferable.
But in batch optimization, a robust loss function might be preferred to properly define the optimization problem as outlier rejection by a threshold is not differentiable.
Otherwise some ad-hoc &ldquo;adjustment&rdquo; algorithms will be required.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://nmayorov.github.io>Navigating Uncertainty</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>
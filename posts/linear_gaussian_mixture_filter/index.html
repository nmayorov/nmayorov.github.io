<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Linear Gaussian mixture filter | Navigating Uncertainty</title><meta name=keywords content><meta name=description content="Gaussian mixture state representation seems like a powerful approach, which can work as a useful generalization of unimodal Gaussian representation.
However, its coverage in literature is scarce and unsatisfactory, not to say confusing.
It makes sense to start building understanding from a basic, but fundamental problem &mdash; estimation in linear systems.
Here a linear filter which estimates parameters of Gaussian mixture probability density is rigorously developed.
Conceptually it will be very similar to classical Kalman filter."><meta name=author content="Nikolay Mayorov"><link rel=canonical href=https://nmayorov.github.io/posts/linear_gaussian_mixture_filter/><link crossorigin=anonymous href=/assets/css/stylesheet.e0b72195d639b2201de2c00d87cdeb7cb0955cac70a34e10561442999111648d.css integrity="sha256-4LchldY5siAd4sANh83rfLCVXKxwo04QVhRCmZERZI0=" rel="preload stylesheet" as=style><link rel=icon href=https://nmayorov.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://nmayorov.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://nmayorov.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://nmayorov.github.io/apple-touch-icon.png><link rel=mask-icon href=https://nmayorov.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://nmayorov.github.io/posts/linear_gaussian_mixture_filter/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script><meta property="og:url" content="https://nmayorov.github.io/posts/linear_gaussian_mixture_filter/"><meta property="og:site_name" content="Navigating Uncertainty"><meta property="og:title" content="Linear Gaussian mixture filter"><meta property="og:description" content="Gaussian mixture state representation seems like a powerful approach, which can work as a useful generalization of unimodal Gaussian representation. However, its coverage in literature is scarce and unsatisfactory, not to say confusing. It makes sense to start building understanding from a basic, but fundamental problem — estimation in linear systems. Here a linear filter which estimates parameters of Gaussian mixture probability density is rigorously developed. Conceptually it will be very similar to classical Kalman filter."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-09-09T00:00:00+00:00"><meta property="article:modified_time" content="2025-09-09T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Linear Gaussian mixture filter"><meta name=twitter:description content="Gaussian mixture state representation seems like a powerful approach, which can work as a useful generalization of unimodal Gaussian representation.
However, its coverage in literature is scarce and unsatisfactory, not to say confusing.
It makes sense to start building understanding from a basic, but fundamental problem &mdash; estimation in linear systems.
Here a linear filter which estimates parameters of Gaussian mixture probability density is rigorously developed.
Conceptually it will be very similar to classical Kalman filter."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://nmayorov.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Linear Gaussian mixture filter","item":"https://nmayorov.github.io/posts/linear_gaussian_mixture_filter/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Linear Gaussian mixture filter","name":"Linear Gaussian mixture filter","description":"Gaussian mixture state representation seems like a powerful approach, which can work as a useful generalization of unimodal Gaussian representation. However, its coverage in literature is scarce and unsatisfactory, not to say confusing. It makes sense to start building understanding from a basic, but fundamental problem \u0026mdash; estimation in linear systems. Here a linear filter which estimates parameters of Gaussian mixture probability density is rigorously developed. Conceptually it will be very similar to classical Kalman filter.\n","keywords":[],"articleBody":"Gaussian mixture state representation seems like a powerful approach, which can work as a useful generalization of unimodal Gaussian representation. However, its coverage in literature is scarce and unsatisfactory, not to say confusing. It makes sense to start building understanding from a basic, but fundamental problem — estimation in linear systems. Here a linear filter which estimates parameters of Gaussian mixture probability density is rigorously developed. Conceptually it will be very similar to classical Kalman filter.\nNotation and definitions First define a multivariate normal probability density function with parameters of mean $m$ and covariance matrix $P$: $$ \\mathcal{N}(x; m, P) = \\frac{1}{(2 \\pi)^{n/2} |P|^{1/2}} \\exp \\left[ -\\frac{1}{2} (x - m)^T P^{-1} (x - m) \\right] $$ The Gaussian mixture probability density is defined as a weighted sum of such functions: $$ \\mathcal{M}(x; w, m, P) = \\sum_{i = 1}^N w^{(i)} \\mathcal{N}(x; m^{(i)}, P^{(i)}), \\\\ \\text{with } w^{(i)} \u003e 0 \\text{ and } \\sum_{i = 1}^N w^{(i)} = 1 $$ Here $w, m, P$ denote a set of $N$ parameters.\nA sampling from such distribution is done in the following way:\nRandomly select from which component to generate a sample with probabilities equal to $w^{(i)}$ Generate a sample using a normal distribution of the selected component The Gaussian mixture should not be confused in any way with sum of normal variables.\nCharacteristic functions A characteristic function of a random variable $x$ with probability density $f_x(\\xi)$ is defined as the following expectation ($j$ is an imaginary unit): $$ \\overline{f}_x(\\alpha) = \\mathrm{\\mathbb{E}}_x e^{j \\alpha^T x} = \\int f_x(\\xi) e^{j \\alpha^T \\xi} d \\xi $$\nBecause this transform is similar to Fourier, there exists a one-to-one mapping between probability density and characteristic functions. Let’s write out characteristic functions of normal and Gaussian mixture distributions:\nFor normal distribution: $$ \\overline{\\mathcal{N}}(\\alpha; m, P) = \\exp \\left(j \\alpha^T m - \\frac{1}{2} \\alpha^T P \\alpha \\right) $$ This can be shown directly from the definition by computing the integral. For Gaussian mixture distribution: $$ \\overline{\\mathcal{M}}(\\alpha; w, m, P) = \\sum_{i = 1}^N w^{(i)} \\exp \\left(j \\alpha^T m^{(i)} - \\frac{1}{2} \\alpha^T P^{(i)} \\alpha \\right) $$ It follows from linearity of integration and the form of the density as a linear combination of individual normal densities. We will also use two basic properties of a characteristic function:\nCharacteristic function of sum of two independent random variables $z = x + y$: $$ \\overline{f}_z(\\alpha) = \\overline{f}_x(\\alpha) \\overline{f}_y(\\alpha) $$ It follows from the factorization property of exponent and ability to split the integral into independent parts. Characteristic function of linearly transformed random variable $y = A x$: $$ \\overline{f}_y(\\alpha) = \\overline{f}_x(A^T \\alpha) $$ It follows directly from definition. Operations on Gaussian mixture variables First consider how a Gaussian mixture variable transforms when adding an independent normal variable to it: $$ x \\sim \\mathcal{M}(w_x, m_x, P_x) \\\\ y \\sim \\mathcal{N}(m_y, P_y) \\\\ z = x + y $$ By using the sum property of characteristic functions we get: $$ \\begin{split} \\overline{f}_z(\\alpha) = \\overline{\\mathcal{M}}(\\alpha; w_x, m_x, P_x) \\overline{\\mathcal{N}}(\\alpha; m_y, P_y) = \\sum_{i = 1}^N w^{(i)} \\exp \\left(j \\alpha^T m_x^{(i)} - \\frac{1}{2} \\alpha^T P_x^{(i)} \\alpha \\right) \\exp \\left(j \\alpha^T m_y - \\frac{1}{2} \\alpha^T P_y \\alpha \\right) = \\\\ = \\sum_{i = 1}^N w^{(i)} \\exp \\left(j \\alpha^T (m_x^{(i)} + m_y) - \\frac{1}{2} \\alpha^T (P_x^{(i)} + P_y) \\alpha \\right) = \\overline{\\mathcal{M}}(\\alpha; w_x, m_x + m_y, P_x + P_y) \\end{split} $$ We can identify that $z$ has the characteristic function of a Gaussian mixture and thus (the notation assumes that each element of the parameter set is modified): $$ z \\sim \\mathcal{M}(w_x, m_x + m_y, P_x + P_y) $$ In other words, the mixture weights don’t change and parameters of individual components are changed by the usual rules for mean and variance of a sum.\nNow consider how a Gaussian mixture variable changes under a linear transform: $$ x \\sim \\mathcal{M}(w_x, m_x, P_x) \\\\ y = A x $$ By using the linear transform rule of a characteristic function we get (with the same simplified notation): $$ \\overline{f}_y(\\alpha) = \\overline{\\mathcal{M}}(A^T \\alpha; w_x, m_x, P_x) = \\overline{\\mathcal{M}}(\\alpha; w_x, A m_x, A P_x A^T) $$ We see, that $y$ is a Gaussian mixture variable with the following parameters: $$ y \\sim \\mathcal{M}(w_x, A m_x, A P_x A^T) $$ In other words, the mixture weights don’t change and parameters of individual components are transformed in the usual way.\nRecap of the Bayesian derivation of Kalman update step Let’s recap how Kalman update formulas are derived using the Bayes rule. Starting from state and measurement models: $$ x \\sim \\mathcal{N}(m, P) \\\\ z = H x + v, v \\sim \\mathcal{N}(0, R) $$ The Kalman update step infers the probability density of $x$ conditioned on measurement $z$ using the Bayes rule: $$ f_{x | z}(\\xi | \\zeta) = \\frac{f_{z | x} (\\zeta | \\xi) f_x(\\xi)}{f_z(\\zeta)} $$ Let’s write individual probability densities: $$ f_x(\\xi) = \\mathcal{N}(\\xi; m, P) \\\\ f_{z | x}(\\zeta | \\xi) = \\mathcal{N}(\\zeta; H \\xi, R) \\\\ f_z(\\zeta) = \\mathcal{N}(\\zeta; H m, H P H^T + R) $$ The key result is that after substitution it can be reduced to another normal density of the form: $$ f_{x | z}(\\xi | \\zeta) = \\mathcal{N}(\\xi; m^+, P^+) $$ With the updated mean and covariance given by the famous Kalman formulas: $$ m^+ = m + K (\\zeta - H m) \\\\ P^+ = (I - K H) P \\\\ K = P H^T (H P H^T + R)^{-1} $$ Here $\\zeta$ denotes a particular observed realization of $z$.\nLater we will use this result in the following form: $$ \\mathcal{N}(\\zeta; H \\xi, R) \\mathcal{N}(\\xi; m, P) = \\mathcal{N}(\\zeta; H m, H P H^T + R) \\mathcal{N}(\\xi; m^+, P^+) $$\nDerivation of linear Gaussian mixture filter Now we have everything setup to derive a linear Gaussian mixture filter. The filter will estimate probability density function of a vector $x$ given the following transition and measurement equations: $$ x_{k + 1} = F_k x_k + m_k, n_k \\sim \\mathcal{N}(0, Q_k) \\\\ z_k = H_k x_k + v_k, v_k \\sim \\mathcal{N}(0, R_k) $$ But as opposed to Kalman filter the initial distribution is a Gaussian mixture: $$ x_0 \\sim \\mathcal{M}(w_0, m_0, P_0) $$ As will be seen by an induction argument, the conditional distribution of $x$ will remain a Gaussian mixture. And thus the computational state of the filter is the set of weights, mean vectors and covariance matrices — $w_k, x_k, P_k$ (here $k$ denotes the time index).\nNow let’s derive formulas for prediction and update phases of the filter.\nPrediction step The prediction step propagates distribution parameters from epoch $k$ to $k + 1$: $$ x_k \\sim \\mathcal{M}(w_k, m_k, P_k) \\\\ x_{k + 1} = F_k x_k + n_k, $$ Considering that $w_k$ is independent of $x_k$ and applying derived formulas for Gaussian mixture variable transformations we get: $$ w_{k + 1} = w_k \\\\ m_{k + 1}^{(i)} = F_k m_k^{(i)} \\\\ P_{k + 1}^{(i)} = F_k P_k^{(i)} F_k^T + Q_k $$ That is the weights of components don’t change and parameters of individual components are transformed as in Kalman filter.\nUpdate step The update step incorporates measurement information by computing the conditional probability density according to the measurement model: $$ x_k \\sim \\mathcal{M}(w_k, m_k, P_k) \\\\ z_k = H_k x_k + v_k, v_k \\sim \\mathcal{N}(0, R_k) $$ Analogous to Kalman filter we apply the Bayes rule: $$ f_{x | z}(\\xi | \\zeta) = \\frac{f_{z | x} (\\zeta | \\xi) f_x(\\xi)}{f_z(\\zeta)} $$ Where the individual probability densities are (time index $k$ is omitted): $$ f_x(\\xi) = \\mathcal{M}(\\xi; w, m, P) \\\\ f_{z | x}(\\zeta | \\xi) = \\mathcal{N}(\\zeta; H \\xi, R) \\\\ f_z(\\zeta) = \\mathcal{M}(\\zeta; w, H m, H P H^T + R) $$ By substituting the density functions into the Bayes rule we get: $$ f_{x | z}(\\xi | \\zeta) = \\frac{\\sum_{i = 1}^N w^{(i)} \\mathcal{N}(\\zeta; H \\xi, R) \\mathcal{N}(\\xi; m^{(i)}, P^{(i)})}{\\sum_{i = 1}^N w^{(i)} \\mathcal{N}(\\zeta; H m^{(i)}, H P^{(i)} H^T + R)} $$ Now in the numerator use the probability density relation from the Kalman filter update step: $$ f_{x | z}(\\xi | \\zeta) = \\frac{\\sum_{i = 1}^N w^{(i)} \\mathcal{N}\\left(\\zeta; H m^{(i)}, H P^{(i)} H^T + R \\right) \\mathcal{N}\\left(\\xi; m^{(i)+}, P^{(i)+}\\right) }{\\sum_{i = 1}^N w^{(i)} \\mathcal{N}(\\zeta; H m^{(i)}, H P^{(i)} H^T + R)} $$ Now the variable $\\xi$ appears only in one multiplier and the whole probability density can be identified as a Gaussian mixture with the parameters: $$ w^{(i)+} = \\frac{w^{(i)} p^{(i)}}{\\sum_{i = 1}^N w^{(i)} p^{(i)}} \\\\ [3pt] m^{(i)+} = m^{(i)} + K^{(i)} (\\zeta - H m^{(i)}) \\\\ P^{(i)+} = (I - K^{(i)} H) P^{(i)} $$ with $$ K^{(i)} = P^{(i)} H^T (H P^{(i)} H^T + R)^{-1} \\\\ p^{(i)} = \\mathcal{N}\\left(\\zeta; H m^{(i)}, H P^{(i)} H^T + R \\right) $$ The weights adjustment factors $p^{(i)}$ are essentially likelihoods to observe a measurement $\\zeta$ assuming $x$ comes from a particular component.\nFilter summary The filter operates in a standard sequence:\nThe state parameters $w_0, x_0, P_0$ are initialized from a prior knowledge Then the parameters are updated sequentially for each time epoch $k$: Measurements at epoch $k$ are processed as described in the corresponding section to obtain the updated parameters $w_k^+, m_k^+, P_k^+$ The parameters are propagated from epoch $k$ to $k + 1$ as described in the corresponding section to obtain the new parameters $w_{k +1}, m_{k + 1}, P_{k + 1}$ The filter seems to be well and rigorously defined and will correctly propagate the Gaussian mixture distribution according to the given linear model.\nConclusion It was shown that for a linear model we can rigorously build an estimation filter for a Gaussian mixture distribution, which is very similar to Kalman filter. It doesn’t seem to have any problematic or questionable aspects. However, its generalization to nonlinear systems looks to be more difficult and subtle. This is something I want to figure out next.\n","wordCount":"1626","inLanguage":"en","datePublished":"2025-09-09T00:00:00Z","dateModified":"2025-09-09T00:00:00Z","author":{"@type":"Person","name":"Nikolay Mayorov"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://nmayorov.github.io/posts/linear_gaussian_mixture_filter/"},"publisher":{"@type":"Organization","name":"Navigating Uncertainty","logo":{"@type":"ImageObject","url":"https://nmayorov.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://nmayorov.github.io/ accesskey=h title="Navigating Uncertainty (Alt + H)">Navigating Uncertainty</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://nmayorov.github.io/open_source title="Open source"><span>Open source</span></a></li><li><a href=https://nmayorov.github.io/archives title="All posts"><span>All posts</span></a></li><li><a href=https://nmayorov.github.io/tags title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://nmayorov.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://nmayorov.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Linear Gaussian mixture filter</h1><div class=post-meta><span title='2025-09-09 00:00:00 +0000 UTC'>September 9, 2025</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;Nikolay Mayorov</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#notation-and-definitions aria-label="Notation and definitions">Notation and definitions</a></li><li><a href=#characteristic-functions aria-label="Characteristic functions">Characteristic functions</a></li><li><a href=#operations-on-gaussian-mixture-variables aria-label="Operations on Gaussian mixture variables">Operations on Gaussian mixture variables</a></li><li><a href=#recap-of-the-bayesian-derivation-of-kalman-update-step aria-label="Recap of the Bayesian derivation of Kalman update step">Recap of the Bayesian derivation of Kalman update step</a></li><li><a href=#derivation-of-linear-gaussian-mixture-filter aria-label="Derivation of linear Gaussian mixture filter">Derivation of linear Gaussian mixture filter</a><ul><li><a href=#prediction-step aria-label="Prediction step">Prediction step</a></li><li><a href=#update-step aria-label="Update step">Update step</a></li><li><a href=#filter-summary aria-label="Filter summary">Filter summary</a></li></ul></li><li><a href=#conclusion aria-label=Conclusion>Conclusion</a></li></ul></div></details></div><div class=post-content><p>Gaussian mixture state representation seems like a powerful approach, which can work as a useful generalization of unimodal Gaussian representation.
However, its coverage in literature is scarce and unsatisfactory, not to say confusing.
It makes sense to start building understanding from a basic, but fundamental problem &mdash; estimation in linear systems.
Here a linear filter which estimates parameters of Gaussian mixture probability density is rigorously developed.
Conceptually it will be very similar to classical Kalman filter.</p><h1 id=notation-and-definitions>Notation and definitions<a hidden class=anchor aria-hidden=true href=#notation-and-definitions>#</a></h1><p>First define a multivariate normal probability density function with parameters of mean $m$ and covariance matrix $P$:
$$
\mathcal{N}(x; m, P) = \frac{1}{(2 \pi)^{n/2} |P|^{1/2}} \exp \left[ -\frac{1}{2} (x - m)^T P^{-1} (x - m) \right]
$$
The Gaussian mixture probability density is defined as a weighted sum of such functions:
$$
\mathcal{M}(x; w, m, P) = \sum_{i = 1}^N w^{(i)} \mathcal{N}(x; m^{(i)}, P^{(i)}), \\
\text{with } w^{(i)} > 0 \text{ and } \sum_{i = 1}^N w^{(i)} = 1
$$
Here $w, m, P$ denote a set of $N$ parameters.</p><p>A sampling from such distribution is done in the following way:</p><ol><li>Randomly select from which component to generate a sample with probabilities equal to $w^{(i)}$</li><li>Generate a sample using a normal distribution of the selected component</li></ol><p>The Gaussian mixture should not be confused in any way with sum of normal variables.</p><h1 id=characteristic-functions>Characteristic functions<a hidden class=anchor aria-hidden=true href=#characteristic-functions>#</a></h1><p>A characteristic function of a random variable $x$ with probability density $f_x(\xi)$ is defined as the following expectation ($j$ is an imaginary unit):
$$
\overline{f}_x(\alpha) = \mathrm{\mathbb{E}}_x e^{j \alpha^T x} = \int f_x(\xi) e^{j \alpha^T \xi} d \xi
$$</p><p>Because this transform is similar to Fourier, there exists a one-to-one mapping between probability density and characteristic functions.
Let&rsquo;s write out characteristic functions of normal and Gaussian mixture distributions:</p><ol><li>For normal distribution:
$$
\overline{\mathcal{N}}(\alpha; m, P) = \exp \left(j \alpha^T m - \frac{1}{2} \alpha^T P \alpha \right)
$$
This can be shown directly from the definition by computing the integral.</li><li>For Gaussian mixture distribution:
$$
\overline{\mathcal{M}}(\alpha; w, m, P) = \sum_{i = 1}^N w^{(i)} \exp \left(j \alpha^T m^{(i)} - \frac{1}{2} \alpha^T P^{(i)} \alpha \right)
$$
It follows from linearity of integration and the form of the density as a linear combination of individual normal densities.</li></ol><p>We will also use two basic properties of a characteristic function:</p><ol><li>Characteristic function of sum of two independent random variables $z = x + y$:
$$
\overline{f}_z(\alpha) = \overline{f}_x(\alpha) \overline{f}_y(\alpha)
$$
It follows from the factorization property of exponent and ability to split the integral into independent parts.</li><li>Characteristic function of linearly transformed random variable $y = A x$:
$$
\overline{f}_y(\alpha) = \overline{f}_x(A^T \alpha)
$$
It follows directly from definition.</li></ol><h1 id=operations-on-gaussian-mixture-variables>Operations on Gaussian mixture variables<a hidden class=anchor aria-hidden=true href=#operations-on-gaussian-mixture-variables>#</a></h1><p>First consider how a Gaussian mixture variable transforms when adding an independent normal variable to it:
$$
x \sim \mathcal{M}(w_x, m_x, P_x) \\
y \sim \mathcal{N}(m_y, P_y) \\
z = x + y
$$
By using the sum property of characteristic functions we get:
$$
\begin{split}
\overline{f}_z(\alpha) = \overline{\mathcal{M}}(\alpha; w_x, m_x, P_x) \overline{\mathcal{N}}(\alpha; m_y, P_y) = \sum_{i = 1}^N w^{(i)} \exp \left(j \alpha^T m_x^{(i)} - \frac{1}{2} \alpha^T P_x^{(i)} \alpha \right) \exp \left(j \alpha^T m_y - \frac{1}{2} \alpha^T P_y \alpha \right) = \\ = \sum_{i = 1}^N w^{(i)} \exp \left(j \alpha^T (m_x^{(i)} + m_y) - \frac{1}{2} \alpha^T (P_x^{(i)} + P_y) \alpha \right) = \overline{\mathcal{M}}(\alpha; w_x, m_x + m_y, P_x + P_y)
\end{split}
$$
We can identify that $z$ has the characteristic function of a Gaussian mixture and thus (the notation assumes that each element of the parameter set is modified):
$$
z \sim \mathcal{M}(w_x, m_x + m_y, P_x + P_y)
$$
In other words, the mixture weights don&rsquo;t change and parameters of individual components are changed by the usual rules for mean and variance of a sum.</p><p>Now consider how a Gaussian mixture variable changes under a linear transform:
$$
x \sim \mathcal{M}(w_x, m_x, P_x) \\
y = A x
$$
By using the linear transform rule of a characteristic function we get (with the same simplified notation):
$$
\overline{f}_y(\alpha) = \overline{\mathcal{M}}(A^T \alpha; w_x, m_x, P_x) = \overline{\mathcal{M}}(\alpha; w_x, A m_x, A P_x A^T)
$$
We see, that $y$ is a Gaussian mixture variable with the following parameters:
$$
y \sim \mathcal{M}(w_x, A m_x, A P_x A^T)
$$
In other words, the mixture weights don&rsquo;t change and parameters of individual components are transformed in the usual way.</p><h1 id=recap-of-the-bayesian-derivation-of-kalman-update-step>Recap of the Bayesian derivation of Kalman update step<a hidden class=anchor aria-hidden=true href=#recap-of-the-bayesian-derivation-of-kalman-update-step>#</a></h1><p>Let&rsquo;s recap how Kalman update formulas are derived using the Bayes rule.
Starting from state and measurement models:
$$
x \sim \mathcal{N}(m, P) \\
z = H x + v, v \sim \mathcal{N}(0, R)
$$
The Kalman update step infers the probability density of $x$ conditioned on measurement $z$ using the Bayes rule:
$$
f_{x | z}(\xi | \zeta) = \frac{f_{z | x} (\zeta | \xi) f_x(\xi)}{f_z(\zeta)}
$$
Let&rsquo;s write individual probability densities:
$$
f_x(\xi) = \mathcal{N}(\xi; m, P) \\
f_{z | x}(\zeta | \xi) = \mathcal{N}(\zeta; H \xi, R) \\
f_z(\zeta) = \mathcal{N}(\zeta; H m, H P H^T + R)
$$
The key result is that after substitution it can be reduced to another normal density of the form:
$$
f_{x | z}(\xi | \zeta) = \mathcal{N}(\xi; m^+, P^+)
$$
With the updated mean and covariance given by the famous Kalman formulas:
$$
m^+ = m + K (\zeta - H m) \\
P^+ = (I - K H) P \\
K = P H^T (H P H^T + R)^{-1}
$$
Here $\zeta$ denotes a particular observed realization of $z$.</p><p>Later we will use this result in the following form:
$$
\mathcal{N}(\zeta; H \xi, R) \mathcal{N}(\xi; m, P) = \mathcal{N}(\zeta; H m, H P H^T + R) \mathcal{N}(\xi; m^+, P^+)
$$</p><h1 id=derivation-of-linear-gaussian-mixture-filter>Derivation of linear Gaussian mixture filter<a hidden class=anchor aria-hidden=true href=#derivation-of-linear-gaussian-mixture-filter>#</a></h1><p>Now we have everything setup to derive a linear Gaussian mixture filter.
The filter will estimate probability density function of a vector $x$ given the following transition and measurement equations:
$$
x_{k + 1} = F_k x_k + m_k, n_k \sim \mathcal{N}(0, Q_k) \\
z_k = H_k x_k + v_k, v_k \sim \mathcal{N}(0, R_k)
$$
But as opposed to Kalman filter the initial distribution is a Gaussian mixture:
$$
x_0 \sim \mathcal{M}(w_0, m_0, P_0)
$$
As will be seen by an induction argument, the conditional distribution of $x$ will remain a Gaussian mixture.
And thus the computational state of the filter is the set of weights, mean vectors and covariance matrices &mdash; $w_k, x_k, P_k$ (here $k$ denotes the time index).</p><p>Now let&rsquo;s derive formulas for prediction and update phases of the filter.</p><h2 id=prediction-step>Prediction step<a hidden class=anchor aria-hidden=true href=#prediction-step>#</a></h2><p>The prediction step propagates distribution parameters from epoch $k$ to $k + 1$:
$$
x_k \sim \mathcal{M}(w_k, m_k, P_k) \\
x_{k + 1} = F_k x_k + n_k,
$$
Considering that $w_k$ is independent of $x_k$ and applying derived formulas for Gaussian mixture variable transformations we get:
$$
w_{k + 1} = w_k \\
m_{k + 1}^{(i)} = F_k m_k^{(i)} \\
P_{k + 1}^{(i)} = F_k P_k^{(i)} F_k^T + Q_k
$$
That is the weights of components don&rsquo;t change and parameters of individual components are transformed as in Kalman filter.</p><h2 id=update-step>Update step<a hidden class=anchor aria-hidden=true href=#update-step>#</a></h2><p>The update step incorporates measurement information by computing the conditional probability density according to the measurement model:
$$
x_k \sim \mathcal{M}(w_k, m_k, P_k) \\
z_k = H_k x_k + v_k, v_k \sim \mathcal{N}(0, R_k)
$$
Analogous to Kalman filter we apply the Bayes rule:
$$
f_{x | z}(\xi | \zeta) = \frac{f_{z | x} (\zeta | \xi) f_x(\xi)}{f_z(\zeta)}
$$
Where the individual probability densities are (time index $k$ is omitted):
$$
f_x(\xi) = \mathcal{M}(\xi; w, m, P) \\
f_{z | x}(\zeta | \xi) = \mathcal{N}(\zeta; H \xi, R) \\
f_z(\zeta) = \mathcal{M}(\zeta; w, H m, H P H^T + R)
$$
By substituting the density functions into the Bayes rule we get:
$$
f_{x | z}(\xi | \zeta) = \frac{\sum_{i = 1}^N w^{(i)} \mathcal{N}(\zeta; H \xi, R) \mathcal{N}(\xi; m^{(i)}, P^{(i)})}{\sum_{i = 1}^N w^{(i)} \mathcal{N}(\zeta; H m^{(i)}, H P^{(i)} H^T + R)}
$$
Now in the numerator use the probability density relation from the Kalman filter update step:
$$
f_{x | z}(\xi | \zeta) = \frac{\sum_{i = 1}^N w^{(i)} \mathcal{N}\left(\zeta; H m^{(i)}, H P^{(i)} H^T + R \right) \mathcal{N}\left(\xi; m^{(i)+}, P^{(i)+}\right) }{\sum_{i = 1}^N w^{(i)} \mathcal{N}(\zeta; H m^{(i)}, H P^{(i)} H^T + R)}
$$
Now the variable $\xi$ appears only in one multiplier and the whole probability density can be identified as a Gaussian mixture with the parameters:
$$
w^{(i)+} = \frac{w^{(i)} p^{(i)}}{\sum_{i = 1}^N w^{(i)} p^{(i)}} \\ [3pt]
m^{(i)+} = m^{(i)} + K^{(i)} (\zeta - H m^{(i)}) \\
P^{(i)+} = (I - K^{(i)} H) P^{(i)}
$$
with
$$
K^{(i)} = P^{(i)} H^T (H P^{(i)} H^T + R)^{-1} \\
p^{(i)} = \mathcal{N}\left(\zeta; H m^{(i)}, H P^{(i)} H^T + R \right)
$$
The weights adjustment factors $p^{(i)}$ are essentially likelihoods to observe a measurement $\zeta$ assuming $x$ comes from a particular component.</p><h2 id=filter-summary>Filter summary<a hidden class=anchor aria-hidden=true href=#filter-summary>#</a></h2><p>The filter operates in a standard sequence:</p><ol><li>The state parameters $w_0, x_0, P_0$ are initialized from a prior knowledge</li><li>Then the parameters are updated sequentially for each time epoch $k$:<ol><li>Measurements at epoch $k$ are processed as described in the corresponding section to obtain the updated parameters $w_k^+, m_k^+, P_k^+$</li><li>The parameters are propagated from epoch $k$ to $k + 1$ as described in the corresponding section to obtain the new parameters $w_{k +1}, m_{k + 1}, P_{k + 1}$</li></ol></li></ol><p>The filter seems to be well and rigorously defined and will correctly propagate the Gaussian mixture distribution according to the given linear model.</p><h1 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h1><p>It was shown that for a linear model we can rigorously build an estimation filter for a Gaussian mixture distribution, which is very similar to Kalman filter.
It doesn&rsquo;t seem to have any problematic or questionable aspects.
However, its generalization to nonlinear systems looks to be more difficult and subtle.
This is something I want to figure out next.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://nmayorov.github.io/>Navigating Uncertainty</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>
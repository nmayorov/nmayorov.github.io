<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Derivation of Kalman smoother from an optimization perspective | Navigating Uncertainty</title>
<meta name=keywords content><meta name=description content="In this post Kalman smoother formulas are derived as a solution to an optimization problem.
Problem formulation We consider an estimation problem for a discrete time linear stochastic system: $$ \begin{gather*} x_{k+1} = F_k x_k + G_k w_k \\ z_k = H_k x_k + v_k \\ \operatorname{E} x_0 = x_0^- \\ \operatorname{E} (x - x_0^-) (x - x_0^-)^T = P_0^- \succ 0 \\ \operatorname{E} w_k = 0 \\ \operatorname{E} w_k w_k^T = Q_k \succ 0 \\ \operatorname{E} v_k = 0 \\ \operatorname{E} v_k v_k^T = R_k \succ 0 \\ \operatorname{E} w_i w_j^T = 0 \text{ for } i \neq j \\ \operatorname{E} v_i v_j^T = 0 \text{ for } i \neq j \\ \operatorname{E} w_i v_j^T = 0 \\ \end{gather*} $$"><meta name=author content="Nikolay Mayorov"><link rel=canonical href=https://nmayorov.github.io/posts/rts_as_optimization/><link crossorigin=anonymous href=/assets/css/stylesheet.a801c217e3c96db9c9df36cb1d2216a9bc5d9fa77b72e2afcdf47fb847f8f015.css integrity="sha256-qAHCF+PJbbnJ3zbLHSIWqbxdn6d7cuKvzfR/uEf48BU=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://nmayorov.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://nmayorov.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://nmayorov.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://nmayorov.github.io/apple-touch-icon.png><link rel=mask-icon href=https://nmayorov.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script><meta property="og:title" content="Derivation of Kalman smoother from an optimization perspective"><meta property="og:description" content="In this post Kalman smoother formulas are derived as a solution to an optimization problem.
Problem formulation We consider an estimation problem for a discrete time linear stochastic system: $$ \begin{gather*} x_{k+1} = F_k x_k + G_k w_k \\ z_k = H_k x_k + v_k \\ \operatorname{E} x_0 = x_0^- \\ \operatorname{E} (x - x_0^-) (x - x_0^-)^T = P_0^- \succ 0 \\ \operatorname{E} w_k = 0 \\ \operatorname{E} w_k w_k^T = Q_k \succ 0 \\ \operatorname{E} v_k = 0 \\ \operatorname{E} v_k v_k^T = R_k \succ 0 \\ \operatorname{E} w_i w_j^T = 0 \text{ for } i \neq j \\ \operatorname{E} v_i v_j^T = 0 \text{ for } i \neq j \\ \operatorname{E} w_i v_j^T = 0 \\ \end{gather*} $$"><meta property="og:type" content="article"><meta property="og:url" content="https://nmayorov.github.io/posts/rts_as_optimization/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-11-27T00:00:00+00:00"><meta property="article:modified_time" content="2022-11-27T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Derivation of Kalman smoother from an optimization perspective"><meta name=twitter:description content="In this post Kalman smoother formulas are derived as a solution to an optimization problem.
Problem formulation We consider an estimation problem for a discrete time linear stochastic system: $$ \begin{gather*} x_{k+1} = F_k x_k + G_k w_k \\ z_k = H_k x_k + v_k \\ \operatorname{E} x_0 = x_0^- \\ \operatorname{E} (x - x_0^-) (x - x_0^-)^T = P_0^- \succ 0 \\ \operatorname{E} w_k = 0 \\ \operatorname{E} w_k w_k^T = Q_k \succ 0 \\ \operatorname{E} v_k = 0 \\ \operatorname{E} v_k v_k^T = R_k \succ 0 \\ \operatorname{E} w_i w_j^T = 0 \text{ for } i \neq j \\ \operatorname{E} v_i v_j^T = 0 \text{ for } i \neq j \\ \operatorname{E} w_i v_j^T = 0 \\ \end{gather*} $$"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://nmayorov.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Derivation of Kalman smoother from an optimization perspective","item":"https://nmayorov.github.io/posts/rts_as_optimization/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Derivation of Kalman smoother from an optimization perspective","name":"Derivation of Kalman smoother from an optimization perspective","description":"In this post Kalman smoother formulas are derived as a solution to an optimization problem.\nProblem formulation We consider an estimation problem for a discrete time linear stochastic system: $$ \\begin{gather*} x_{k+1} = F_k x_k + G_k w_k \\\\ z_k = H_k x_k + v_k \\\\ \\operatorname{E} x_0 = x_0^- \\\\ \\operatorname{E} (x - x_0^-) (x - x_0^-)^T = P_0^- \\succ 0 \\\\ \\operatorname{E} w_k = 0 \\\\ \\operatorname{E} w_k w_k^T = Q_k \\succ 0 \\\\ \\operatorname{E} v_k = 0 \\\\ \\operatorname{E} v_k v_k^T = R_k \\succ 0 \\\\ \\operatorname{E} w_i w_j^T = 0 \\text{ for } i \\neq j \\\\ \\operatorname{E} v_i v_j^T = 0 \\text{ for } i \\neq j \\\\ \\operatorname{E} w_i v_j^T = 0 \\\\ \\end{gather*} $$","keywords":[],"articleBody":"In this post Kalman smoother formulas are derived as a solution to an optimization problem.\nProblem formulation We consider an estimation problem for a discrete time linear stochastic system: $$ \\begin{gather*} x_{k+1} = F_k x_k + G_k w_k \\\\ z_k = H_k x_k + v_k \\\\ \\operatorname{E} x_0 = x_0^- \\\\ \\operatorname{E} (x - x_0^-) (x - x_0^-)^T = P_0^- \\succ 0 \\\\ \\operatorname{E} w_k = 0 \\\\ \\operatorname{E} w_k w_k^T = Q_k \\succ 0 \\\\ \\operatorname{E} v_k = 0 \\\\ \\operatorname{E} v_k v_k^T = R_k \\succ 0 \\\\ \\operatorname{E} w_i w_j^T = 0 \\text{ for } i \\neq j \\\\ \\operatorname{E} v_i v_j^T = 0 \\text{ for } i \\neq j \\\\ \\operatorname{E} w_i v_j^T = 0 \\\\ \\end{gather*} $$\nAll symbols above are indexed by integer epoch and have the following meanings:\n$x_k$ – state vector, shape $n \\times 1$ $F_k$ – state transition matrix, shape $n \\times n$ $w_k$ – process noise vector, shape $m \\times 1$ $G_k$ – noise input matrix, shape $n \\times m$ $z_k$ – measurement vector, shape $l \\times 1$ $H_k$ – measurement matrix, shape $l \\times n$ $v_k$ – measurement noise vector, shape $l \\times 1$ $x_0^-$ – apriori state mean, shape $n \\times 1$ $P_0^-$ – apriori state error covariance, shape $n \\times n$ $Q_k$ – covariance of process noise, shape $m \\times m$ $R_k$ – covariance of measurement noise, shape $l \\times l$ Additionally the following notation is used:\n$\\operatorname{E} \\ldots$ – operation of taking mean $C \\succ 0$ – matrix $C$ is positive definite The last 3 equations say that process and measurement noises are not time correlated and not correlated to each other.\nThe task is to find optimal estimates of $x_k$ for epochs $k = 0, 1, \\ldots, N$ taking into account the system model and the observed measurements $z_0, z_1, \\ldots, z_{N-1}$.\nDefining the optimization problem We will determine optimal estimates of $x_k$ by minimizing a cost function. The proposed cost function will be quadratic in $x_k$ and expressed as a sum of terms of 3 kinds:\nA term for prior on $x_0$ Terms for measurements (associated with $z_k$) Terms for the time propagation (relating $x_k$ and $x_{k+1}$) In the transition equation we can define the effective noise term as $$w_k^\\prime = G_k w_k$$ $$Q_k^\\prime \\coloneqq \\operatorname{E} w_k^\\prime \\left(w_k^\\prime\\right)^T = G_k Q_k G_k^T$$ Depending on $G_k$ the matrix $Q_k^\\prime$ can be positive definite and invertible or only positive semidefinite and singular.\nCase of invertible $Q_k^\\prime$ In this case each state is influenced by noise and there is no deterministic part in the state propagation. The cost function can be defined naturally as follows:\n$$ \\begin{split} J^\\prime(x) =\u0026 \\frac{1}{2} \\left(x_0 - x_0^-\\right)^T \\left(P_0^-\\right)^{-1} \\left(x_0 - x_0^-\\right) \\\\ +\u0026 \\frac{1}{2} \\sum_{k = 0}^{N - 1} (H_k x_k - z_k)^T R_k^{-1} (H_k x_k - z_k) \\\\ +\u0026 \\frac{1}{2} \\sum_{k = 0}^{N - 1} (F_k x_k - x_{k + 1})^T \\left(Q_k^\\prime\\right)^{-1} (F_k x_k - x_{k + 1}) \\end{split} $$\nEach term penalizes deviation of an actual measurement (or state) from its expectation weighed by inverses of corresponding covariance matrices. It is also equal to the negative log probability of the joint distribution of all $x_k$ and $z_k$ assuming normal probability densities.\nMinimization of $J^\\prime(x)$ is a linear least-squares problem. The normal equation for it is a large positive definite tridiagonal system of $nN$ equations. It can be solved efficiently by a two-pass algorithm in a time linear in $N$. The resulting formulas can be interpreted as another form of a linear smoother (involving inverse covariance matrices). Details can be found in [1].\nThis formulation is quite simple and has a straightforward solution. Unfortunately the assumption of invertible $Q_k^\\prime$ is limiting. The most common such case is when the number of states is larger that the number of noise source. There is no simple workaround for this issue and another approach needs to be developed.\nCase of singular $Q_k^\\prime$ In this case the inverse of $Q_k^\\prime$ can’t be computed and thus the time propagation terms can’t be formed directly. To overcome this problem the noise vectors should be considered as unknown variables which need to be estimated along with $x$. The following cost function is minimized:\n$$ \\begin{split} J(x, w) =\u0026 \\frac{1}{2} \\left(x_0 - x_0^-\\right)^T \\left(P_0^-\\right)^{-1} \\left(x_0 - x_0^-\\right) \\\\ +\u0026 \\frac{1}{2} \\sum_{k = 0}^{N - 1} (H_k x_k - z_k)^T R_k^{-1} (H_k x_k - z_k) \\\\ +\u0026 \\frac{1}{2} \\sum_{k = 0}^{N - 1} w_k^T Q_k^{-1} w_k \\end{split} $$ Here $x$ and $w$ are not independent and the time propagation equations must be taken into account. Doing this, we’ll get the following constrained optimization problem:\n$$ \\min_{x, w} J(x, w) \\text{ subject to} \\space x_{k + 1} = F_k x_k + G_k w_k $$\nThis is a linear least squares problem with linear equality constraints on the variables. Such problem can be solved by a known linear algebra method. The issue however is that this problem is large (in practically valuable cases) and sparse. Whether there are suitable general numerical solvers for this kind of sparse problems is a topic for another research. In what follows we will derive an efficient solver specifically for this problem.\nLagrange function and its stationary points In order to solve an equality constrained optimization problem one should form the Lagrange function as a sum of an original cost function and constraint equations multiplied by Lagrange multipliers. Let’s denote the multiplier for the time propagation equation from epoch $k$ to epoch $k + 1$ as $\\lambda_{k+1}$. The Lagrange function is then $$ \\begin{split} L(x, w, \\lambda) =\u0026 \\frac{1}{2} \\left(x_0 - x_0^-\\right)^T \\left(P_0^-\\right)^{-1} \\left(x_0 - x_0^-\\right) \\\\ +\u0026 \\frac{1}{2} \\sum_{k = 0}^{N - 1} (H_k x_k - z_k)^T R_k^{-1} (H_k x_k - z_k) \\\\ +\u0026 \\frac{1}{2} \\sum_{k = 0}^{N - 1} w_k^T Q_k^{-1} w_k \\\\ +\u0026 \\sum_{k = 0}^{N - 1} \\lambda_{k + 1}^T (x_{k + 1} - F_k x_k - G_k w_k) \\end{split} $$\nCandidates for the minimum are searched as stationary points of the Lagrange function: $$\\frac{\\partial L}{\\partial x_k} = 0; \\frac{\\partial L}{\\partial w_k} = 0; \\frac{\\partial L}{\\partial \\lambda_k} = 0$$ The partial derivative with respect to $x_k$ requires a separate treatment depending on $k$.\nFor $k = 0$: $$ H_0^T R_0^{-1} \\left(H_0 x_0 - z_0\\right) + \\left(P_0^-\\right)^{-1} \\left(x_0 - x_0^-\\right) - F_0^T \\lambda_1 = 0 $$\nFor $0 \u003c k \u003c N$: $$H_k^T R_k^{-1} \\left(H_k x_k - z_k\\right) + \\lambda_k - F_k^T \\lambda_{k + 1} = 0$$\nFor $k = N$: $$\\lambda_N = 0$$\nTo conform the first equation with the second one we introduce $$\\lambda_0 \\coloneqq \\left(P_0^-\\right)^{-1} \\left(x_0 - x_0^-\\right)$$\nThen computing partial derivatives with respect to $w_k$ and $\\lambda_k$ and joining all the equations together we get the following system: $$ \\begin{gather*} x_{k + 1} = F_k x_k + G_k w_k \\\\ \\lambda_k = F_k^T \\lambda_{k + 1} + H_k^T R_k^{-1} \\left(z_k - H_k x_k\\right) \\\\ w_k = Q_k G_k^T \\lambda_{k+1} \\\\ \\lambda_0 = \\left(P_0^-\\right)^{-1} \\left(x_0 - x_0^-\\right) \\\\ \\lambda_N = 0 \\end{gather*} $$\nLet’s analyze what is this system and how it can be solved. First we can eliminate $w_k$ as it’s directly expressed from $\\lambda_{k+1}$. Then we see that it’s a system of difference equations for epochs $k = 0, 1, \\ldots, N$ for vectors $x_k$ and $\\lambda_k$ (both with $n$ elements). To solve it, $2n$ initial or boundary conditions are required. We have $n$ conditions at $k = 0$ (involving $\\lambda_0$ and $x_0$) and $n$ conditions at $k = N$ (as $\\lambda_N = 0$). So this is a boundary value problem which potentially has a solution (if boundary conditions are consistent) and if it’s the case we can find it.\nKalman filter notation and formulas To derive the solution we will use Kalman filter notation and formulas. So here is a recap for these. The following notation is used:\n$x_k^-, P_k^-$ – a priori estimate and covariance at $k$, i.e. before processing measurement $z_k$ $x_k^+, P_k^+$ – a posteriori estimate and covariance at $k$, i.e. after processing measurement $z_k$ The correction equations (processing of $z_k$) are $$ x_k^+ = x_k^- + K_k (z_k - H_k x_k^-) \\\\ P_k^+ = (I - K_k H_k) P_k^- \\\\ \\text{ with } K_k = P_k^- H_k^T (H_k P_k H_k^T + R_k)^{-1} $$\nThey can be expressed alternatively as $$ (P_k^+)^{-1} = (P_k^-)^{-1} + H_k^T R_k^{-1} H_k \\\\ (P_k^+)^{-1} x_k^+ = (P_k^-)^{-1} x_k^- + H_k^T R_k^{-1} z_k $$\nAnd the prediction equations are $$ x_{k+1}^- = F_k x_k^+ \\\\ P_{k+1}^- = F_k P_k^+ F_k^T + G_k Q_k G_k^T $$\nSolving the difference equations First rewrite the equations with $w_k$ eliminated: $$ x_{k + 1} = F_k x_k + G_k Q_k G_k^T \\lambda_{k+1} \\\\ \\lambda_k = F_k^T \\lambda_{k + 1} + H_k^T R_k^{-1} \\left(z_k - H_k x_k\\right) \\\\ \\lambda_0 = \\left(P_0^-\\right)^{-1} \\left(x_0 - x_0^-\\right) \\\\ \\lambda_N = 0 $$\nLooking at the expression for $\\lambda_0$ we make an assumption that $$ x_k = x_k^- + P_k^- \\lambda_k $$ This assumption holds for $k = 0$ as the boundary condition. Let’s prove that if it’s true for $k$ then it’s true for $k + 1$ as well. By induction principle then it will be true for any $k$.\nSubstituting $\\lambda_k$ from the second equation into the assumption equation we get $$ \\begin{gather*} x_k = x_k^- + P_k^- F_k^T \\lambda_{k + 1} + P_k^- H_k^T R_k^{-1} z_k - P_k^- H_k^T R_k^{-1} H_k x_k \\\\ (I + P_k^- H_k^T R_k^{-1} H_k) x_k = x_k^- + P_k^- F_k^T \\lambda_{k + 1} + P_k^- H_k^T R_k^{-1} z_k \\\\ P_k^-((P_k^-)^{-1} + H_k^T R_k^{-1} H_k) x_k = x_k^- + P_k^- F_k^T \\lambda_{k + 1} + P_k^- H_k^T R_k^{-1} z_k \\\\ P_k^- (P_k^+)^{-1} x_k = x_k^- + P_k^- F_k^T \\lambda_{k + 1} + P_k^- H_k^T R_k^{-1} z_k \\\\ x_k = P_k^+ \\left( (P_k^-)^{-1} x_k^- + H_k^T R_k^{-1} z_k \\right) + P_k^+ F_k^T \\lambda_{k + 1} \\\\ x_k = x_k^+ + P_k^+ F_k^T \\lambda_{k + 1} \\end{gather*} $$\nThen substituting the new expression for $x_k$ into the time propagation equations we get $$ \\begin{gather*} x_{k + 1} = F_k x_k^+ + (F_k^T P_k^+ F_k + G_k Q_k G_k^T) \\lambda_{k + 1} \\\\ x_{k + 1} = x_{k + 1}^- + P_{k + 1}^- \\lambda_{k + 1} \\end{gather*} $$ Starting from the identity for $k$ we derived the same identity for $k + 1$ which proves the step of induction and with that the identity for any $k$.\nThen consider the two derived identities together: $$ \\begin{gather*} x_k = x_k^+ + P_k^+ F_k^T \\lambda_{k + 1} \\\\ x_{k + 1} = x_{k + 1}^- + P_{k + 1}^- \\lambda_{k + 1} \\end{gather*} $$ Expressing $\\lambda_{k + 1}$ from the second equation and substituting it into the first gets us $$ \\begin{gather*} x_k^s = x_k^+ + C_k \\left(x_{k + 1}^s - x_{k + 1}^-\\right) \\text{with} \\space C_k = P_k^+ F_k^T \\left(P_{k + 1}^-\\right)^{-1} \\end{gather*} $$ Here the superscript «s» denotes an optimal «smoothed» estimate. This is a formula for the RTS smoother backward recursion [2].\nFrom the boundary condition $\\lambda_N = 0$ we get the starting point for this recursion $$x_{N}^s = x_{N}^-$$ It means that the optimal estimate at $k = N$ is equal to the filter estimate.\nError covariance computation The errors of the estimates are defined as follows $$ \\Delta x_k^- = x_k^- - x_k \\\\ \\Delta x_k^+ = x_k^+ - x_k \\\\ \\Delta x_k^s = x_k^s - x_k \\\\ $$ with $x_k$ being the true state. The error covariances: $$ \\begin{gather*} P_k^- = \\operatorname{E} \\Delta x_k^- \\left(\\Delta x_k^-\\right)^T \\\\ P_k^+ = \\operatorname{E} \\Delta x_k^+ \\left(\\Delta x_k^+\\right)^T \\\\ P_k^s = \\operatorname{E} \\Delta x_k^s \\left(\\Delta x_k^s\\right)^T \\end{gather*} $$ The filter covariances $P_k^-$ and $P_k^+$ are known and the task is to determine $P_k^s$.\nTurns out that derivation of the error covariance straight from the RTS recursion formula is difficult. In fact I haven’t seen such derivation anywhere. Instead authors rely on some indirect properties to take a shortcut to the result. I will resort to that too with a solid justification.\nKalman filter errors correlation property Consider a state vector estimate before ($x^-$) and after ($x^+$) a vector $z$ is processed in a Kalman filter: $$ x^+ = x^- + K (z - H x^-) $$ For the errors we have $$ \\Delta x^+ = \\Delta x^- + K (v - H \\Delta x^-) $$ The cross covariance is then $$ \\operatorname E \\Delta x^+ (\\Delta x^-)^T = (I - K H) P^- = P^+ $$ Using this property we get $$ \\operatorname{E}(\\Delta x^+ - \\Delta x^-)(\\Delta x^+ - \\Delta x^-)^T = P^- - P^+ $$ To derive the smoother covariance formula we need this property for pairs $\\Delta x^s, \\Delta x^+$ and $\\Delta x^s, \\Delta x^-$. It holds for them because $x^s_k$ is a corrected version (in an optimal Kalman sense) of $x^-_k$ or $x^+_k$ using all measurements $z_i$ for $i \\geq k$ or $i \u003e k$ respectively. This can be rigorously shown by forming an augmented state vector and considering the smoothing problem as a filtering problem for this augmented vector.\nDeriving the covariance recursion formula Rewrite the equation for $x_k^s$ as $$ x_k^s - x_k^+= C_k \\left(x_{k + 1}^s - x_{k + 1}^-\\right) $$ From it the equivalent equation for the errors follows: $$ \\Delta x_k^s - \\Delta x_k^+= C_k \\left(\\Delta x_{k + 1}^s - \\Delta x_{k + 1}^-\\right) $$ Taking the covariance of both sides using the above stated properties we get: $$ P_k^+ - P_k^s = C_k \\left( P_{k+1}^- - P_{k+1}^s\\right) C_k^T $$ Or finally: $$ P_k^s = P_k^+ + C_k \\left(P_{k+1}^s - P_{k+1}^-\\right) C_k^T $$\nEstimation of noise vectors Noise estimates are computed directly from $\\lambda_{k+1}$ as $$ w_k^s = Q_k G_k^T \\lambda_{k+1}^s = Q_k G_k^T \\left( P_{k+1}^- \\right)^{-1} \\left(x_{k+1}^s - x_{k+1}^-\\right) $$ We are also interested in its error: $$ \\Delta w_k^s = w_k^s - w_k $$ The covariance of $\\Delta w_k^s$ can be computed using the same approach as for the state estimate error covariance. The derivation is omitted for the sake of brevity and the final formula looks like this:\n$$ Q_k^s \\coloneqq \\operatorname{E} \\Delta w_k^s \\left(\\Delta w_k^s\\right)^T = Q_k + B_k \\left(P^s_{k + 1} - P^-_{k + 1} \\right) B_k^T \\\\ \\text{with } B_k = Q_k G_k^T \\left(P_{k+1}^-\\right)^{-1} $$\nSmoothness of $x^s$ To assess smoothness properties of $x^s$ it is convenient to consider a continuous time problem. In this case $x^s$ satisfies the differential equation (as constraints in the optimization problem) $$ \\dot{x}^s = f(x^s, w^s) $$ Thus $x^s$ is guaranteed to be at least continuous.\nThe question whether $w^s$ is continuous is more difficult. We note that $w^s$ is proportional to $\\lambda^s$, which for a discrete-time system satisfies the difference equation with $z_k$ involved. Considering that $z_k$ contains random uncorrelated noise we can conclude that $\\lambda^s$ and $w^s$ are not continuous (with discrete time measurements). Of course it is not a rigorous proof, but the conclusion seems to be correct. There is also empirical evidence for that – $w^s$ estimates don’t look at all as samples of a continuous function.\nFrom the fact that $w^s$ is not continuos it follows that $\\dot{x}^s = f(x^s, w^s)$ is not continuous as well. So for $x^s$ only continuity holds, its derivative is not continuous.\nDiscrete-time systems are usually built as discretization of continuous time systems. Thus $x_k^s$ can be though of as samples of a continuous function $x^s(t)$.\nMinor generalization of the model When applying the linear smoother just derived to solve subproblems arising in nonlinear estimation a minor generalization of the model is required:\nConsider a deterministic control signal $u_k$ in the time propagation equations Consider that the process noise vectors have known nonzero mean values $w_k^-$ The Kalman prediction equation for the state then becomes $$ x_{k + 1}^- = F_k x_k^+ + G_k w_k^- + u_k $$\nAnd the noise estimates are computed as $$ w_k^s = w_k^- + Q_k G_k^T \\left( P_{k+1}^- \\right)^{-1} \\left(x_{k+1}^s - x_{k+1}^-\\right) $$\nAnd everything else stays the same.\nSummary of the algorithm To compute the state estimate and its covariance as the solution of the equality constrained linear least-squares problem the following algorithm must be used.\nForward pass The initial filter state and covariance estimates are given as $x_0^-$ and $P_0^-$. Alternate correction and prediction steps for $k = 0, 1, \\ldots, N - 1$.\nCorrection step $$ x_k^+ = x_k^- + K_k (z_k - H_k x_k^-) \\\\ P_k^+ = (I - K_k H_k) P_k^- \\\\ \\text{with} \\space K_k = P_k^- H_k^T \\left(H_k P_k^- H_k^T + R_k \\right)^{-1} $$\nPrediction step $$x_{k + 1}^- = F_k x_k^+ + G_k w_k^- + u_k$$ $$P_{k + 1}^- = F_k P_k^+ F_k^T + G_k Q_k G_k^T$$\nAll the variables $x_k^-, x_k^+, P_k^-, P_k^+$ are saved for the backward pass.\nBackward pass Assign the smoothed estimate and covariance for epoch $N$ to the forward pass variables: $$ x_N^s = x_N^- \\\\ P_N^s = P_N^- $$\nFor $k = N - 1, N - 2, \\ldots, 0$ apply the following formulas: $$x_k^s = x_k^+ + C_k (x_{k+1}^s - x_{k+1}^-)$$ $$w_k^s = w_k^- + B_k (x_{k+1}^s - x_{k+1}^-)$$ $$P_k^s = P_k^+ + C_k (P_{k+1}^s - P_{k+1}^-) C_k^T$$ $$Q_k^s = Q_k + B_k (P_{k+1}^s - P_{k+1}^-) B_k^T$$ $$\\text{with } C_k = P_k^+ F_k^T \\left(P_{k + 1}^-\\right)^{-1} \\text{ and } B_k = Q_k G_k^T \\left( P_{k+1}^- \\right)^{-1}$$\nIn the end, the variables $x_k^s, P_k^s$ and $w_k^s, Q_k^s$ contain optimal estimates and error covariances of state and noise vectors respectively for $k = 0, 1, \\ldots, N$.\nConclusion A step-by-step derivation of Kalman smoother (RTS form [2]) as a solution to an optimization problem was presented. Expressing this state estimation algorithm as a solution to the optimization problem opens up opportunities to develop other advanced estimation methods (nonlinear, robust, etc.) based on an optimization approach.\nReferences Optimization viewpoint on Kalman smoothing, with applications to robust and sparse estimation H. E. Rauch, F. Tung, C. T. Striebel «Maximum Likelihood Estimates of Linear Dynamic Systems» ","wordCount":"2957","inLanguage":"en","datePublished":"2022-11-27T00:00:00Z","dateModified":"2022-11-27T00:00:00Z","author":{"@type":"Person","name":"Nikolay Mayorov"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://nmayorov.github.io/posts/rts_as_optimization/"},"publisher":{"@type":"Organization","name":"Navigating Uncertainty","logo":{"@type":"ImageObject","url":"https://nmayorov.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://nmayorov.github.io accesskey=h title="Navigating Uncertainty (Alt + H)">Navigating Uncertainty</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://nmayorov.github.io/archives title="All posts"><span>All posts</span></a></li><li><a href=https://nmayorov.github.io/tags title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://nmayorov.github.io>Home</a>&nbsp;»&nbsp;<a href=https://nmayorov.github.io/posts/>Posts</a></div><h1 class=post-title>Derivation of Kalman smoother from an optimization perspective</h1><div class=post-meta><span title='2022-11-27 00:00:00 +0000 UTC'>November 27, 2022</span>&nbsp;·&nbsp;14 min&nbsp;·&nbsp;Nikolay Mayorov</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#problem-formulation aria-label="Problem formulation">Problem formulation</a></li><li><a href=#defining-the-optimization-problem aria-label="Defining the optimization problem">Defining the optimization problem</a><ul><li><a href=#case-of-invertible-q_kprime aria-label="Case of invertible $Q_k^\prime$">Case of invertible $Q_k^\prime$</a></li><li><a href=#case-of-singular-q_kprime aria-label="Case of singular $Q_k^\prime$">Case of singular $Q_k^\prime$</a></li></ul></li><li><a href=#lagrange-function-and-its-stationary-points aria-label="Lagrange function and its stationary points">Lagrange function and its stationary points</a></li><li><a href=#kalman-filter-notation-and-formulas aria-label="Kalman filter notation and formulas">Kalman filter notation and formulas</a></li><li><a href=#solving-the-difference-equations aria-label="Solving the difference equations">Solving the difference equations</a></li><li><a href=#error-covariance-computation aria-label="Error covariance computation">Error covariance computation</a><ul><li><a href=#kalman-filter-errors-correlation-property aria-label="Kalman filter errors correlation property">Kalman filter errors correlation property</a></li><li><a href=#deriving-the-covariance-recursion-formula aria-label="Deriving the covariance recursion formula">Deriving the covariance recursion formula</a></li></ul></li><li><a href=#estimation-of-noise-vectors aria-label="Estimation of noise vectors">Estimation of noise vectors</a></li><li><a href=#smoothness-of-xs aria-label="Smoothness of $x^s$">Smoothness of $x^s$</a></li><li><a href=#minor-generalization-of-the-model aria-label="Minor generalization of the model">Minor generalization of the model</a></li><li><a href=#summary-of-the-algorithm aria-label="Summary of the algorithm">Summary of the algorithm</a><ul><li><a href=#forward-pass aria-label="Forward pass">Forward pass</a><ul><li><a href=#correction-step aria-label="Correction step">Correction step</a></li><li><a href=#prediction-step aria-label="Prediction step">Prediction step</a></li></ul></li><li><a href=#backward-pass aria-label="Backward pass">Backward pass</a></li></ul></li><li><a href=#conclusion aria-label=Conclusion>Conclusion</a></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><p>In this post Kalman smoother formulas are derived as a solution to an optimization problem.</p><h1 id=problem-formulation>Problem formulation<a hidden class=anchor aria-hidden=true href=#problem-formulation>#</a></h1><p>We consider an estimation problem for a discrete time linear stochastic system:
$$
\begin{gather*}
x_{k+1} = F_k x_k + G_k w_k \\
z_k = H_k x_k + v_k \\
\operatorname{E} x_0 = x_0^- \\
\operatorname{E} (x - x_0^-) (x - x_0^-)^T = P_0^- \succ 0 \\
\operatorname{E} w_k = 0 \\
\operatorname{E} w_k w_k^T = Q_k \succ 0 \\
\operatorname{E} v_k = 0 \\
\operatorname{E} v_k v_k^T = R_k \succ 0 \\
\operatorname{E} w_i w_j^T = 0 \text{ for } i \neq j \\
\operatorname{E} v_i v_j^T = 0 \text{ for } i \neq j \\
\operatorname{E} w_i v_j^T = 0 \\
\end{gather*}
$$</p><p>All symbols above are indexed by integer epoch and have the following meanings:</p><ul><li>$x_k$ &ndash; state vector, shape $n \times 1$</li><li>$F_k$ &ndash; state transition matrix, shape $n \times n$</li><li>$w_k$ &ndash; process noise vector, shape $m \times 1$</li><li>$G_k$ &ndash; noise input matrix, shape $n \times m$</li><li>$z_k$ &ndash; measurement vector, shape $l \times 1$</li><li>$H_k$ &ndash; measurement matrix, shape $l \times n$</li><li>$v_k$ &ndash; measurement noise vector, shape $l \times 1$</li><li>$x_0^-$ &ndash; apriori state mean, shape $n \times 1$</li><li>$P_0^-$ &ndash; apriori state error covariance, shape $n \times n$</li><li>$Q_k$ &ndash; covariance of process noise, shape $m \times m$</li><li>$R_k$ &ndash; covariance of measurement noise, shape $l \times l$</li></ul><p>Additionally the following notation is used:</p><ul><li>$\operatorname{E} \ldots$ &ndash; operation of taking mean</li><li>$C \succ 0$ &ndash; matrix $C$ is positive definite</li></ul><p>The last 3 equations say that process and measurement noises are not time correlated and not correlated to each other.</p><p>The task is to find optimal estimates of $x_k$ for epochs $k = 0, 1, \ldots, N$ taking into account the system model and the observed measurements $z_0, z_1, \ldots, z_{N-1}$.</p><h1 id=defining-the-optimization-problem>Defining the optimization problem<a hidden class=anchor aria-hidden=true href=#defining-the-optimization-problem>#</a></h1><p>We will determine optimal estimates of $x_k$ by minimizing a cost function.
The proposed cost function will be quadratic in $x_k$ and expressed as a sum of terms of 3 kinds:</p><ol><li>A term for prior on $x_0$</li><li>Terms for measurements (associated with $z_k$)</li><li>Terms for the time propagation (relating $x_k$ and $x_{k+1}$)</li></ol><p>In the transition equation we can define the effective noise term as
$$w_k^\prime = G_k w_k$$
$$Q_k^\prime \coloneqq \operatorname{E} w_k^\prime \left(w_k^\prime\right)^T = G_k Q_k G_k^T$$
Depending on $G_k$ the matrix $Q_k^\prime$ can be positive definite and invertible or only positive semidefinite and singular.</p><h2 id=case-of-invertible-q_kprime>Case of invertible $Q_k^\prime$<a hidden class=anchor aria-hidden=true href=#case-of-invertible-q_kprime>#</a></h2><p>In this case each state is influenced by noise and there is no deterministic part in the state propagation.
The cost function can be defined naturally as follows:</p><p>$$
\begin{split}
J^\prime(x) =& \frac{1}{2} \left(x_0 - x_0^-\right)^T \left(P_0^-\right)^{-1} \left(x_0 - x_0^-\right) \\
+& \frac{1}{2} \sum_{k = 0}^{N - 1} (H_k x_k - z_k)^T R_k^{-1} (H_k x_k - z_k) \\
+& \frac{1}{2} \sum_{k = 0}^{N - 1} (F_k x_k - x_{k + 1})^T \left(Q_k^\prime\right)^{-1} (F_k x_k - x_{k + 1})
\end{split}
$$</p><p>Each term penalizes deviation of an actual measurement (or state) from its expectation weighed by inverses of corresponding covariance matrices.
It is also equal to the negative log probability of the joint distribution of all $x_k$ and $z_k$ assuming normal probability densities.</p><p>Minimization of $J^\prime(x)$ is a linear least-squares problem.
The normal equation for it is a large positive definite tridiagonal system of $nN$ equations.
It can be solved efficiently by a two-pass algorithm in a time linear in $N$.
The resulting formulas can be interpreted as another form of a linear smoother (involving inverse covariance matrices).
Details can be found in [1].</p><p>This formulation is quite simple and has a straightforward solution.
Unfortunately the assumption of invertible $Q_k^\prime$ is limiting.
The most common such case is when the number of states is larger that the number of noise source.
There is no simple workaround for this issue and another approach needs to be developed.</p><h2 id=case-of-singular-q_kprime>Case of singular $Q_k^\prime$<a hidden class=anchor aria-hidden=true href=#case-of-singular-q_kprime>#</a></h2><p>In this case the inverse of $Q_k^\prime$ can&rsquo;t be computed and thus the time propagation terms can&rsquo;t be formed directly.
To overcome this problem the noise vectors should be considered as unknown variables which need to be estimated along with $x$.
The following cost function is minimized:</p><p>$$
\begin{split}
J(x, w) =& \frac{1}{2} \left(x_0 - x_0^-\right)^T \left(P_0^-\right)^{-1} \left(x_0 - x_0^-\right) \\
+& \frac{1}{2} \sum_{k = 0}^{N - 1} (H_k x_k - z_k)^T R_k^{-1} (H_k x_k - z_k) \\
+& \frac{1}{2} \sum_{k = 0}^{N - 1} w_k^T Q_k^{-1} w_k
\end{split}
$$
Here $x$ and $w$ are not independent and the time propagation equations must be taken into account.
Doing this, we&rsquo;ll get the following constrained optimization problem:</p><p>$$
\min_{x, w} J(x, w) \text{ subject to} \space x_{k + 1} = F_k x_k + G_k w_k
$$</p><p>This is a linear least squares problem with linear equality constraints on the variables.
Such problem can be solved by a known linear algebra method.
The issue however is that this problem is large (in practically valuable cases) and sparse.
Whether there are suitable general numerical solvers for this kind of sparse problems is a topic for another research.
In what follows we will derive an efficient solver specifically for this problem.</p><h1 id=lagrange-function-and-its-stationary-points>Lagrange function and its stationary points<a hidden class=anchor aria-hidden=true href=#lagrange-function-and-its-stationary-points>#</a></h1><p>In order to solve an equality constrained optimization problem one should form the Lagrange function as a sum of an original cost function and constraint equations multiplied by Lagrange multipliers.
Let&rsquo;s denote the multiplier for the time propagation equation from epoch $k$ to epoch $k + 1$ as $\lambda_{k+1}$.
The Lagrange function is then
$$
\begin{split}
L(x, w, \lambda) =& \frac{1}{2} \left(x_0 - x_0^-\right)^T \left(P_0^-\right)^{-1} \left(x_0 - x_0^-\right) \\
+& \frac{1}{2} \sum_{k = 0}^{N - 1} (H_k x_k - z_k)^T R_k^{-1} (H_k x_k - z_k) \\
+& \frac{1}{2} \sum_{k = 0}^{N - 1} w_k^T Q_k^{-1} w_k \\
+& \sum_{k = 0}^{N - 1} \lambda_{k + 1}^T (x_{k + 1} - F_k x_k - G_k w_k)
\end{split}
$$</p><p>Candidates for the minimum are searched as stationary points of the Lagrange function:
$$\frac{\partial L}{\partial x_k} = 0; \frac{\partial L}{\partial w_k} = 0; \frac{\partial L}{\partial \lambda_k} = 0$$
The partial derivative with respect to $x_k$ requires a separate treatment depending on $k$.</p><p>For $k = 0$:
$$
H_0^T R_0^{-1} \left(H_0 x_0 - z_0\right) + \left(P_0^-\right)^{-1} \left(x_0 - x_0^-\right) - F_0^T \lambda_1 = 0
$$</p><p>For $0 &lt; k &lt; N$:
$$H_k^T R_k^{-1} \left(H_k x_k - z_k\right) + \lambda_k - F_k^T \lambda_{k + 1} = 0$$</p><p>For $k = N$:
$$\lambda_N = 0$$</p><p>To conform the first equation with the second one we introduce $$\lambda_0 \coloneqq \left(P_0^-\right)^{-1} \left(x_0 - x_0^-\right)$$</p><p>Then computing partial derivatives with respect to $w_k$ and $\lambda_k$ and joining all the equations together we get the following system:
$$
\begin{gather*}
x_{k + 1} = F_k x_k + G_k w_k \\
\lambda_k = F_k^T \lambda_{k + 1} + H_k^T R_k^{-1} \left(z_k - H_k x_k\right) \\
w_k = Q_k G_k^T \lambda_{k+1} \\
\lambda_0 = \left(P_0^-\right)^{-1} \left(x_0 - x_0^-\right) \\
\lambda_N = 0
\end{gather*}
$$</p><p>Let&rsquo;s analyze what is this system and how it can be solved.
First we can eliminate $w_k$ as it&rsquo;s directly expressed from $\lambda_{k+1}$.
Then we see that it&rsquo;s a system of difference equations for epochs $k = 0, 1, \ldots, N$ for vectors $x_k$ and $\lambda_k$ (both with $n$ elements).
To solve it, $2n$ initial or boundary conditions are required.
We have $n$ conditions at $k = 0$ (involving $\lambda_0$ and $x_0$) and $n$ conditions at $k = N$ (as $\lambda_N = 0$).
So this is a boundary value problem which potentially has a solution (if boundary conditions are consistent) and if it&rsquo;s the case we can find it.</p><h1 id=kalman-filter-notation-and-formulas>Kalman filter notation and formulas<a hidden class=anchor aria-hidden=true href=#kalman-filter-notation-and-formulas>#</a></h1><p>To derive the solution we will use Kalman filter notation and formulas.
So here is a recap for these.
The following notation is used:</p><ul><li>$x_k^-, P_k^-$ &ndash; a priori estimate and covariance at $k$, i.e. before processing measurement $z_k$</li><li>$x_k^+, P_k^+$ &ndash; a posteriori estimate and covariance at $k$, i.e. after processing measurement $z_k$</li></ul><p>The correction equations (processing of $z_k$) are
$$
x_k^+ = x_k^- + K_k (z_k - H_k x_k^-) \\
P_k^+ = (I - K_k H_k) P_k^- \\
\text{ with } K_k = P_k^- H_k^T (H_k P_k H_k^T + R_k)^{-1}
$$</p><p>They can be expressed alternatively as
$$
(P_k^+)^{-1} = (P_k^-)^{-1} + H_k^T R_k^{-1} H_k \\
(P_k^+)^{-1} x_k^+ = (P_k^-)^{-1} x_k^- + H_k^T R_k^{-1} z_k
$$</p><p>And the prediction equations are
$$
x_{k+1}^- = F_k x_k^+ \\
P_{k+1}^- = F_k P_k^+ F_k^T + G_k Q_k G_k^T
$$</p><h1 id=solving-the-difference-equations>Solving the difference equations<a hidden class=anchor aria-hidden=true href=#solving-the-difference-equations>#</a></h1><p>First rewrite the equations with $w_k$ eliminated:
$$
x_{k + 1} = F_k x_k + G_k Q_k G_k^T \lambda_{k+1} \\
\lambda_k = F_k^T \lambda_{k + 1} + H_k^T R_k^{-1} \left(z_k - H_k x_k\right) \\
\lambda_0 = \left(P_0^-\right)^{-1} \left(x_0 - x_0^-\right) \\
\lambda_N = 0
$$</p><p>Looking at the expression for $\lambda_0$ we make an assumption that
$$
x_k = x_k^- + P_k^- \lambda_k
$$
This assumption holds for $k = 0$ as the boundary condition.
Let&rsquo;s prove that if it&rsquo;s true for $k$ then it&rsquo;s true for $k + 1$ as well.
By induction principle then it will be true for any $k$.</p><p>Substituting $\lambda_k$ from the second equation into the assumption equation we get
$$
\begin{gather*}
x_k = x_k^- + P_k^- F_k^T \lambda_{k + 1} + P_k^- H_k^T R_k^{-1} z_k - P_k^- H_k^T R_k^{-1} H_k x_k \\
(I + P_k^- H_k^T R_k^{-1} H_k) x_k = x_k^- + P_k^- F_k^T \lambda_{k + 1} + P_k^- H_k^T R_k^{-1} z_k \\
P_k^-((P_k^-)^{-1} + H_k^T R_k^{-1} H_k) x_k = x_k^- + P_k^- F_k^T \lambda_{k + 1} + P_k^- H_k^T R_k^{-1} z_k \\
P_k^- (P_k^+)^{-1} x_k = x_k^- + P_k^- F_k^T \lambda_{k + 1} + P_k^- H_k^T R_k^{-1} z_k \\
x_k = P_k^+ \left( (P_k^-)^{-1} x_k^- + H_k^T R_k^{-1} z_k \right) + P_k^+ F_k^T \lambda_{k + 1} \\
x_k = x_k^+ + P_k^+ F_k^T \lambda_{k + 1}
\end{gather*}
$$</p><p>Then substituting the new expression for $x_k$ into the time propagation equations we get
$$
\begin{gather*}
x_{k + 1} = F_k x_k^+ + (F_k^T P_k^+ F_k + G_k Q_k G_k^T) \lambda_{k + 1} \\
x_{k + 1} = x_{k + 1}^- + P_{k + 1}^- \lambda_{k + 1}
\end{gather*}
$$
Starting from the identity for $k$ we derived the same identity for $k + 1$ which proves the step of induction and with that the identity for any $k$.</p><p>Then consider the two derived identities together:
$$
\begin{gather*}
x_k = x_k^+ + P_k^+ F_k^T \lambda_{k + 1} \\
x_{k + 1} = x_{k + 1}^- + P_{k + 1}^- \lambda_{k + 1}
\end{gather*}
$$
Expressing $\lambda_{k + 1}$ from the second equation and substituting it into the first gets us
$$
\begin{gather*}
x_k^s = x_k^+ + C_k \left(x_{k + 1}^s - x_{k + 1}^-\right) \text{with} \space C_k = P_k^+ F_k^T \left(P_{k + 1}^-\right)^{-1}
\end{gather*}
$$
Here the superscript &#171;s&#187; denotes an optimal &#171;smoothed&#187; estimate.
This is a formula for the RTS smoother backward recursion [2].</p><p>From the boundary condition $\lambda_N = 0$ we get the starting point for this recursion $$x_{N}^s = x_{N}^-$$
It means that the optimal estimate at $k = N$ is equal to the filter estimate.</p><h1 id=error-covariance-computation>Error covariance computation<a hidden class=anchor aria-hidden=true href=#error-covariance-computation>#</a></h1><p>The errors of the estimates are defined as follows
$$
\Delta x_k^- = x_k^- - x_k \\
\Delta x_k^+ = x_k^+ - x_k \\
\Delta x_k^s = x_k^s - x_k \\
$$
with $x_k$ being the true state. The error covariances:
$$
\begin{gather*}
P_k^- = \operatorname{E} \Delta x_k^- \left(\Delta x_k^-\right)^T \\
P_k^+ = \operatorname{E} \Delta x_k^+ \left(\Delta x_k^+\right)^T \\
P_k^s = \operatorname{E} \Delta x_k^s \left(\Delta x_k^s\right)^T
\end{gather*}
$$
The filter covariances $P_k^-$ and $P_k^+$ are known and the task is to determine $P_k^s$.</p><p>Turns out that derivation of the error covariance straight from the RTS recursion formula is difficult.
In fact I haven&rsquo;t seen such derivation anywhere.
Instead authors rely on some indirect properties to take a shortcut to the result.
I will resort to that too with a solid justification.</p><h2 id=kalman-filter-errors-correlation-property>Kalman filter errors correlation property<a hidden class=anchor aria-hidden=true href=#kalman-filter-errors-correlation-property>#</a></h2><p>Consider a state vector estimate before ($x^-$) and after ($x^+$) a vector $z$ is processed in a Kalman filter:
$$
x^+ = x^- + K (z - H x^-)
$$
For the errors we have
$$
\Delta x^+ = \Delta x^- + K (v - H \Delta x^-)
$$
The cross covariance is then
$$
\operatorname E \Delta x^+ (\Delta x^-)^T = (I - K H) P^- = P^+
$$
Using this property we get
$$
\operatorname{E}(\Delta x^+ - \Delta x^-)(\Delta x^+ - \Delta x^-)^T = P^- - P^+
$$
To derive the smoother covariance formula we need this property for pairs $\Delta x^s, \Delta x^+$ and $\Delta x^s, \Delta x^-$.
It holds for them because $x^s_k$ is a corrected version (in an optimal Kalman sense) of $x^-_k$ or $x^+_k$ using all measurements $z_i$ for $i \geq k$ or $i > k$ respectively.
This can be rigorously shown by forming an augmented state vector and considering the smoothing problem as a filtering problem for this augmented vector.</p><h2 id=deriving-the-covariance-recursion-formula>Deriving the covariance recursion formula<a hidden class=anchor aria-hidden=true href=#deriving-the-covariance-recursion-formula>#</a></h2><p>Rewrite the equation for $x_k^s$ as
$$
x_k^s - x_k^+= C_k \left(x_{k + 1}^s - x_{k + 1}^-\right)
$$
From it the equivalent equation for the errors follows:
$$
\Delta x_k^s - \Delta x_k^+= C_k \left(\Delta x_{k + 1}^s - \Delta x_{k + 1}^-\right)
$$
Taking the covariance of both sides using the above stated properties we get:
$$
P_k^+ - P_k^s = C_k \left( P_{k+1}^- - P_{k+1}^s\right) C_k^T
$$
Or finally:
$$
P_k^s = P_k^+ + C_k \left(P_{k+1}^s - P_{k+1}^-\right) C_k^T
$$</p><h1 id=estimation-of-noise-vectors>Estimation of noise vectors<a hidden class=anchor aria-hidden=true href=#estimation-of-noise-vectors>#</a></h1><p>Noise estimates are computed directly from $\lambda_{k+1}$ as
$$
w_k^s = Q_k G_k^T \lambda_{k+1}^s = Q_k G_k^T \left( P_{k+1}^- \right)^{-1} \left(x_{k+1}^s - x_{k+1}^-\right)
$$
We are also interested in its error:
$$
\Delta w_k^s = w_k^s - w_k
$$
The covariance of $\Delta w_k^s$ can be computed using the same approach as for the state estimate error covariance.
The derivation is omitted for the sake of brevity and the final formula looks like this:</p><p>$$
Q_k^s \coloneqq \operatorname{E} \Delta w_k^s \left(\Delta w_k^s\right)^T = Q_k + B_k \left(P^s_{k + 1} - P^-_{k + 1} \right) B_k^T \\
\text{with } B_k = Q_k G_k^T \left(P_{k+1}^-\right)^{-1}
$$</p><h1 id=smoothness-of-xs>Smoothness of $x^s$<a hidden class=anchor aria-hidden=true href=#smoothness-of-xs>#</a></h1><p>To assess smoothness properties of $x^s$ it is convenient to consider a continuous time problem.
In this case $x^s$ satisfies the differential equation (as constraints in the optimization problem)
$$
\dot{x}^s = f(x^s, w^s)
$$
Thus $x^s$ is guaranteed to be at least continuous.</p><p>The question whether $w^s$ is continuous is more difficult.
We note that $w^s$ is proportional to $\lambda^s$, which for a discrete-time system satisfies the difference equation with $z_k$ involved.
Considering that $z_k$ contains random uncorrelated noise we can conclude that $\lambda^s$ and $w^s$ are not continuous (with discrete time measurements).
Of course it is not a rigorous proof, but the conclusion seems to be correct.
There is also empirical evidence for that &ndash; $w^s$ estimates don&rsquo;t look at all as samples of a continuous function.</p><p>From the fact that $w^s$ is not continuos it follows that $\dot{x}^s = f(x^s, w^s)$ is not continuous as well.
So for $x^s$ only continuity holds, its derivative is not continuous.</p><p>Discrete-time systems are usually built as discretization of continuous time systems.
Thus $x_k^s$ can be though of as samples of a continuous function $x^s(t)$.</p><h1 id=minor-generalization-of-the-model>Minor generalization of the model<a hidden class=anchor aria-hidden=true href=#minor-generalization-of-the-model>#</a></h1><p>When applying the linear smoother just derived to solve subproblems arising in nonlinear estimation a minor generalization of the model is required:</p><ol><li>Consider a deterministic control signal $u_k$ in the time propagation equations</li><li>Consider that the process noise vectors have known nonzero mean values $w_k^-$</li></ol><p>The Kalman prediction equation for the state then becomes
$$
x_{k + 1}^- = F_k x_k^+ + G_k w_k^- + u_k
$$</p><p>And the noise estimates are computed as
$$
w_k^s = w_k^- + Q_k G_k^T \left( P_{k+1}^- \right)^{-1} \left(x_{k+1}^s - x_{k+1}^-\right)
$$</p><p>And everything else stays the same.</p><h1 id=summary-of-the-algorithm>Summary of the algorithm<a hidden class=anchor aria-hidden=true href=#summary-of-the-algorithm>#</a></h1><p>To compute the state estimate and its covariance as the solution of the equality constrained linear least-squares problem the following algorithm must be used.</p><h2 id=forward-pass>Forward pass<a hidden class=anchor aria-hidden=true href=#forward-pass>#</a></h2><p>The initial filter state and covariance estimates are given as $x_0^-$ and $P_0^-$.
Alternate correction and prediction steps for $k = 0, 1, \ldots, N - 1$.</p><h3 id=correction-step>Correction step<a hidden class=anchor aria-hidden=true href=#correction-step>#</a></h3><p>$$
x_k^+ = x_k^- + K_k (z_k - H_k x_k^-) \\
P_k^+ = (I - K_k H_k) P_k^- \\
\text{with} \space K_k = P_k^- H_k^T \left(H_k P_k^- H_k^T + R_k \right)^{-1}
$$</p><h3 id=prediction-step>Prediction step<a hidden class=anchor aria-hidden=true href=#prediction-step>#</a></h3><p>$$x_{k + 1}^- = F_k x_k^+ + G_k w_k^- + u_k$$
$$P_{k + 1}^- = F_k P_k^+ F_k^T + G_k Q_k G_k^T$$</p><p>All the variables $x_k^-, x_k^+, P_k^-, P_k^+$ are saved for the backward pass.</p><h2 id=backward-pass>Backward pass<a hidden class=anchor aria-hidden=true href=#backward-pass>#</a></h2><p>Assign the smoothed estimate and covariance for epoch $N$ to the forward pass variables:
$$
x_N^s = x_N^- \\
P_N^s = P_N^-
$$</p><p>For $k = N - 1, N - 2, \ldots, 0$ apply the following formulas:
$$x_k^s = x_k^+ + C_k (x_{k+1}^s - x_{k+1}^-)$$
$$w_k^s = w_k^- + B_k (x_{k+1}^s - x_{k+1}^-)$$
$$P_k^s = P_k^+ + C_k (P_{k+1}^s - P_{k+1}^-) C_k^T$$
$$Q_k^s = Q_k + B_k (P_{k+1}^s - P_{k+1}^-) B_k^T$$
$$\text{with } C_k = P_k^+ F_k^T \left(P_{k + 1}^-\right)^{-1} \text{ and } B_k = Q_k G_k^T \left( P_{k+1}^- \right)^{-1}$$</p><p>In the end, the variables $x_k^s, P_k^s$ and $w_k^s, Q_k^s$ contain optimal estimates and error covariances of state and noise vectors respectively for $k = 0, 1, \ldots, N$.</p><h1 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h1><p>A step-by-step derivation of Kalman smoother (RTS form [2]) as a solution to an optimization problem was presented.
Expressing this state estimation algorithm as a solution to the optimization problem opens up opportunities to develop other advanced estimation methods (nonlinear, robust, etc.) based on an optimization approach.</p><h1 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h1><ol><li><a href=https://arxiv.org/abs/1303.1993>Optimization viewpoint on Kalman smoothing, with applications to robust and sparse estimation</a></li><li>H. E. Rauch, F. Tung, C. T. Striebel &#171;Maximum Likelihood Estimates of Linear Dynamic Systems&#187;</li></ol></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://nmayorov.github.io>Navigating Uncertainty</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>
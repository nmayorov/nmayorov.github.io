<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Extend Kalman Filter update step as an optimization problem | Navigating Uncertainty</title>
<meta name=keywords content><meta name=description content="The Extended Kalman Filter is classically built as an extension of the linear Kalman filter using system and measurement models linearization. In this regard the update step in EKF is naturally done in a single step. The iterated EKF aims to improve the linearization point by doing several update iterations and recomputing measurement Jacobian each time. It&rsquo;s known to be connected with nonlinear optimization.
In this note I want to derive possible EKF update strategies starting from the optimization viewpoint."><meta name=author content="Nikolay Mayorov"><link rel=canonical href=https://nmayorov.github.io/posts/ekf_update_optimization/><link crossorigin=anonymous href=/assets/css/stylesheet.21e7558a953920031f5c62b4db91512093af342ec251fdf4bc1be327b0e259b4.css integrity="sha256-IedVipU5IAMfXGK025FRIJOvNC7CUf30vBvjJ7DiWbQ=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://nmayorov.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://nmayorov.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://nmayorov.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://nmayorov.github.io/apple-touch-icon.png><link rel=mask-icon href=https://nmayorov.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script><meta property="og:title" content="Extend Kalman Filter update step as an optimization problem"><meta property="og:description" content="The Extended Kalman Filter is classically built as an extension of the linear Kalman filter using system and measurement models linearization. In this regard the update step in EKF is naturally done in a single step. The iterated EKF aims to improve the linearization point by doing several update iterations and recomputing measurement Jacobian each time. It&rsquo;s known to be connected with nonlinear optimization.
In this note I want to derive possible EKF update strategies starting from the optimization viewpoint."><meta property="og:type" content="article"><meta property="og:url" content="https://nmayorov.github.io/posts/ekf_update_optimization/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-05-04T00:00:00+00:00"><meta property="article:modified_time" content="2023-05-04T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Extend Kalman Filter update step as an optimization problem"><meta name=twitter:description content="The Extended Kalman Filter is classically built as an extension of the linear Kalman filter using system and measurement models linearization. In this regard the update step in EKF is naturally done in a single step. The iterated EKF aims to improve the linearization point by doing several update iterations and recomputing measurement Jacobian each time. It&rsquo;s known to be connected with nonlinear optimization.
In this note I want to derive possible EKF update strategies starting from the optimization viewpoint."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://nmayorov.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Extend Kalman Filter update step as an optimization problem","item":"https://nmayorov.github.io/posts/ekf_update_optimization/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Extend Kalman Filter update step as an optimization problem","name":"Extend Kalman Filter update step as an optimization problem","description":"The Extended Kalman Filter is classically built as an extension of the linear Kalman filter using system and measurement models linearization. In this regard the update step in EKF is naturally done in a single step. The iterated EKF aims to improve the linearization point by doing several update iterations and recomputing measurement Jacobian each time. It\u0026rsquo;s known to be connected with nonlinear optimization.\nIn this note I want to derive possible EKF update strategies starting from the optimization viewpoint.","keywords":[],"articleBody":"The Extended Kalman Filter is classically built as an extension of the linear Kalman filter using system and measurement models linearization. In this regard the update step in EKF is naturally done in a single step. The iterated EKF aims to improve the linearization point by doing several update iterations and recomputing measurement Jacobian each time. It’s known to be connected with nonlinear optimization.\nIn this note I want to derive possible EKF update strategies starting from the optimization viewpoint.\nThe update step in the linear Kalman filter In the update step a prior distribution on the state vector $x \\sim \\mathcal{N}(x^-, P^-)$ is optimally combined with a measurement $z = H x + v$ with $v \\sim \\mathcal{N}(0, R)$. To do so the following optimization problem is formulated: $$ \\min_x J(x) = \\frac{1}{2} (x - x^-)^T (P^-)^{-1} (x - x^-) + \\frac{1}{2} (H x - z)^T R^{-1} (H x - z) $$ This is a linear least-squares problem – the quadratic function has a global minimum where its gradient equals zero: $$ (P^-)^{-1} (x - x^-) + H^T R^{-1} (H x - z) = 0 $$ This equation can be transformed as follows (with $x$ denoted as $x^+$): $$ \\left((P^-)^{-1} + H^T R^{-1} H \\right) x^+ = (P^-)^{-1} x^- + H^T R^{-1} z $$ And this is an alternative form of the Kalman correction formula equivalent to $$ x^+ = x^- + K (z - H x^-) \\text{ with } K = P^- H^T (H P^- H^T + R)^{-1} $$\nThe update step in the Extended Kalman Filter In a similar fashion, a prior distribution $X \\sim \\mathcal{N}(X^-, P^-)$ is optimally combined with a nonlinear measurement $Z = h(X) + v$ with $v \\sim \\mathcal{N}(0, R)$, by forming an optimization problem: $$ \\min_X E(X) = \\frac{1}{2} (X - X^-)^T (P^-)^{-1} (X - X^-) + \\frac{1}{2} (h(X) - Z)^T R^{-1} (h(X) - Z) $$ This is a nonlinear least-squares problem which can be solved iteratively by forming a sequence of linear subproblems using linearization around the current estimate.\nLet $X_i$ be an estimate after $i$ iterations with $X_0 \\coloneqq X^-$. An update $x_i$ is sought by substituting $X = X_i + x$ into $E(X)$ and keeping terms linear in $x_i$. It results in the following linear least-squares problem: $$ \\min_x J(x) = \\frac{1}{2} (x - x_i^-)^T (P^-)^{-1} (x - x_i^-) + \\frac{1}{2} (H_i x - z_i)^T R^{-1} (H_i x - z) $$ Where the following variables were introduced: $$ x^- = X^- - X_i \\\\ z_i = Z - h(X_i) \\\\ H_i = \\left.\\dfrac{\\partial h(X)}{\\partial X}\\right\\vert_{X_i} $$ The linear subproblem has the form as in the linear Kalman filter update step for which the solution is known: $$ x_i = x_i^- + K_i (z_i - H_i x_i^-) \\text{ with } {K_i = P^- H_i^T (H_i P^- H_i^T + R)^{-1}} $$\nThe estimate is then updated as $$ X_{i + 1} = X_i + \\alpha_i x_i $$ Where $0 \u003c \\alpha_i \\leq 1$ is selected to ensure the convergence using a procedure known as a line search. The simplest strategy is to set $\\alpha_i = 1$ which works well when $X_i$ is close to the minimizer.\nIn the iterated EKF the full corrections are applied with $\\alpha_i = 1$: $$ X_{i + 1} = X_i + x_i = X_i + X^- - X_i + K_i (Z - h(X_i) - H_i (X^- - X_i)) \\approx X^- + K_i (Z - h(X^-)) $$ The last approximate equality follows from the first-order Taylor expansion of $h(X)$. The whole procedure is already based on this and thus such substitution is justifiable. If implemented using the last equation, the iterated EKF update comes down to updating the point for Jacobian computation $H_i$. It is generally known that a correct Jacobian value is the most important for consistent EKF update and so this scheme makes practical and conceptual sense.\nThe standard EKF update is essentially a single step of the optimization procedure taken with $\\alpha_0 = 1$. It is generally known from optimization theory that such step in «good» conditions can yield estimate very close to the optimum. This can serve as a conceptual explanation why the standard EKF update often works well in practice.\nDiscussion There are 3 choices when implemented the EKF update:\nStandard EKF update – the most simple and computationally efficient Iterated EKF update using one of two slightly different numerical schemes – quite simple but at least doubles required computations (because at least 2 iterations are required to check for the convergence) Proper optimization with the line search (or other means to ensure convergence) – the most rigorous, but requires understanding of optimization algorithms and some tuning. At least doubles required computations In approaches 2 and 3 a care of proper convergence control must be taken to avoid infinite or excessively large amount of iterations.\nI believe that the right approach depends on the application (the estimation problem at hand) and should be selected empirically. I personally have the most experience with the approach 1 and know that it works well for many practical problems with mild measurement nonlinearities. After all, it was successfully used in many practical systems as the basic EKF is a «go-to» algorithm for the estimation problems.\nMore advanced approaches 2 and 3 are worth considering, especially if the computation cost is not a concern. They can’t degrade estimation performance, but might alleviate EKF deficiencies in difficult cases.\nFurther leveraging the optimization approach The optimization approach is powerful because it naturally allows for modification of the cost function. For example, imagine that the measurement noise instead of normal distribution has Laplace distribution. Empirically it means that it is more heavy-tailed with more likely outliers. This situation can be handled by considering a proper loss function (can be thought as negative logarithm of probability): $$ E(X) = \\frac{1}{2} (X - X^-)^T (P^-)^{-1} (X - X^-) + \\sqrt{2} \\sum_{j = 0}^{N - 1} \\frac{|h_j(X) - Z_j|}{\\sigma_j} $$ Here $j$ denotes vector index and $\\sigma_j$ is a standard deviation of independent noise components. This optimization problem is considerably more difficult, but it is well defined and can be solved by a proper optimization algorithm.\nOther cost functions might be proposed to account for required empirical or theoretical properties of the measurements.\nConclusion The update step of the Extended Kalman Filter was considered as an optimization problem. Iterated and standard EKF update schemes are shown to be simplified approaches to solve this problem. The optimization viewpoint is advantageous as it allows for modification of the cost function to account for different measurement model properties.\n","wordCount":"1092","inLanguage":"en","datePublished":"2023-05-04T00:00:00Z","dateModified":"2023-05-04T00:00:00Z","author":{"@type":"Person","name":"Nikolay Mayorov"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://nmayorov.github.io/posts/ekf_update_optimization/"},"publisher":{"@type":"Organization","name":"Navigating Uncertainty","logo":{"@type":"ImageObject","url":"https://nmayorov.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://nmayorov.github.io accesskey=h title="Navigating Uncertainty (Alt + H)">Navigating Uncertainty</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://nmayorov.github.io/archives title="All posts"><span>All posts</span></a></li><li><a href=https://nmayorov.github.io/tags title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://nmayorov.github.io>Home</a>&nbsp;»&nbsp;<a href=https://nmayorov.github.io/posts/>Posts</a></div><h1 class=post-title>Extend Kalman Filter update step as an optimization problem</h1><div class=post-meta>&lt;span title='2023-05-04 00:00:00 +0000 UTC'>May 4, 2023&lt;/span>&amp;nbsp;·&amp;nbsp;6 min&amp;nbsp;·&amp;nbsp;Nikolay Mayorov</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#the-update-step-in-the-linear-kalman-filter aria-label="The update step in the linear Kalman filter">The update step in the linear Kalman filter</a></li><li><a href=#the-update-step-in-the-extended-kalman-filter aria-label="The update step in the Extended Kalman Filter">The update step in the Extended Kalman Filter</a></li><li><a href=#discussion aria-label=Discussion>Discussion</a></li><li><a href=#further-leveraging-the-optimization-approach aria-label="Further leveraging the optimization approach">Further leveraging the optimization approach</a></li><li><a href=#conclusion aria-label=Conclusion>Conclusion</a></li></ul></div></details></div><div class=post-content><p>The Extended Kalman Filter is classically built as an extension of the linear Kalman filter using system and measurement models linearization.
In this regard the update step in EKF is naturally done in a single step.
The iterated EKF aims to improve the linearization point by doing several update iterations and recomputing measurement Jacobian each time.
It&rsquo;s known to be connected with nonlinear optimization.</p><p>In this note I want to derive possible EKF update strategies starting from the optimization viewpoint.</p><h1 id=the-update-step-in-the-linear-kalman-filter>The update step in the linear Kalman filter<a hidden class=anchor aria-hidden=true href=#the-update-step-in-the-linear-kalman-filter>#</a></h1><p>In the update step a prior distribution on the state vector $x \sim \mathcal{N}(x^-, P^-)$ is optimally combined with a measurement $z = H x + v$ with $v \sim \mathcal{N}(0, R)$.
To do so the following optimization problem is formulated:
$$
\min_x J(x) = \frac{1}{2} (x - x^-)^T (P^-)^{-1} (x - x^-) + \frac{1}{2} (H x - z)^T R^{-1} (H x - z)
$$
This is a linear least-squares problem &ndash; the quadratic function has a global minimum where its gradient equals zero:
$$
(P^-)^{-1} (x - x^-) + H^T R^{-1} (H x - z) = 0
$$
This equation can be transformed as follows (with $x$ denoted as $x^+$):
$$
\left((P^-)^{-1} + H^T R^{-1} H \right) x^+ = (P^-)^{-1} x^- + H^T R^{-1} z
$$
And this is an alternative form of the Kalman correction formula equivalent to
$$
x^+ = x^- + K (z - H x^-) \text{ with } K = P^- H^T (H P^- H^T + R)^{-1}
$$</p><h1 id=the-update-step-in-the-extended-kalman-filter>The update step in the Extended Kalman Filter<a hidden class=anchor aria-hidden=true href=#the-update-step-in-the-extended-kalman-filter>#</a></h1><p>In a similar fashion, a prior distribution $X \sim \mathcal{N}(X^-, P^-)$ is optimally combined with a nonlinear measurement $Z = h(X) + v$ with $v \sim \mathcal{N}(0, R)$, by forming an optimization problem:
$$
\min_X E(X) = \frac{1}{2} (X - X^-)^T (P^-)^{-1} (X - X^-) + \frac{1}{2} (h(X) - Z)^T R^{-1} (h(X) - Z)
$$
This is a nonlinear least-squares problem which can be solved iteratively by forming a sequence of linear subproblems using linearization around the current estimate.</p><p>Let $X_i$ be an estimate after $i$ iterations with $X_0 \coloneqq X^-$.
An update $x_i$ is sought by substituting $X = X_i + x$ into $E(X)$ and keeping terms linear in $x_i$.
It results in the following linear least-squares problem:
$$
\min_x J(x) = \frac{1}{2} (x - x_i^-)^T (P^-)^{-1} (x - x_i^-) + \frac{1}{2} (H_i x - z_i)^T R^{-1} (H_i x - z)
$$
Where the following variables were introduced:
$$
x^- = X^- - X_i \\
z_i = Z - h(X_i) \\
H_i = \left.\dfrac{\partial h(X)}{\partial X}\right\vert_{X_i}
$$
The linear subproblem has the form as in the linear Kalman filter update step for which the solution is known:
$$
x_i = x_i^- + K_i (z_i - H_i x_i^-) \text{ with } {K_i = P^- H_i^T (H_i P^- H_i^T + R)^{-1}}
$$</p><p>The estimate is then updated as
$$
X_{i + 1} = X_i + \alpha_i x_i
$$
Where $0 &lt; \alpha_i \leq 1$ is selected to ensure the convergence using a procedure known as a line search.
The simplest strategy is to set $\alpha_i = 1$ which works well when $X_i$ is close to the minimizer.</p><p>In the iterated EKF the full corrections are applied with $\alpha_i = 1$:
$$
X_{i + 1} = X_i + x_i = X_i + X^- - X_i + K_i (Z - h(X_i) - H_i (X^- - X_i)) \approx X^- + K_i (Z - h(X^-))
$$
The last approximate equality follows from the first-order Taylor expansion of $h(X)$.
The whole procedure is already based on this and thus such substitution is justifiable.
If implemented using the last equation, the iterated EKF update comes down to updating the point for Jacobian computation $H_i$.
It is generally known that a correct Jacobian value is the most important for consistent EKF update and so this scheme makes practical and conceptual sense.</p><p>The standard EKF update is essentially a single step of the optimization procedure taken with $\alpha_0 = 1$.
It is generally known from optimization theory that such step in &#171;good&#187; conditions can yield estimate very close to the optimum.
This can serve as a conceptual explanation why the standard EKF update often works well in practice.</p><h1 id=discussion>Discussion<a hidden class=anchor aria-hidden=true href=#discussion>#</a></h1><p>There are 3 choices when implemented the EKF update:</p><ol><li>Standard EKF update &ndash; the most simple and computationally efficient</li><li>Iterated EKF update using one of two slightly different numerical schemes &ndash; quite simple but at least doubles required computations (because at least 2 iterations are required to check for the convergence)</li><li>Proper optimization with the line search (or other means to ensure convergence) &ndash; the most rigorous, but requires understanding of optimization algorithms and some tuning.
At least doubles required computations</li></ol><p>In approaches 2 and 3 a care of proper convergence control must be taken to avoid infinite or excessively large amount of iterations.</p><p>I believe that the right approach depends on the application (the estimation problem at hand) and should be selected empirically.
I personally have the most experience with the approach 1 and know that it works well for many practical problems with mild measurement nonlinearities.
After all, it was successfully used in many practical systems as the basic EKF is a &#171;go-to&#187; algorithm for the estimation problems.</p><p>More advanced approaches 2 and 3 are worth considering, especially if the computation cost is not a concern.
They can&rsquo;t degrade estimation performance, but might alleviate EKF deficiencies in difficult cases.</p><h1 id=further-leveraging-the-optimization-approach>Further leveraging the optimization approach<a hidden class=anchor aria-hidden=true href=#further-leveraging-the-optimization-approach>#</a></h1><p>The optimization approach is powerful because it naturally allows for modification of the cost function.
For example, imagine that the measurement noise instead of normal distribution has Laplace distribution.
Empirically it means that it is more heavy-tailed with more likely outliers.
This situation can be handled by considering a proper loss function (can be thought as negative logarithm of probability):
$$
E(X) = \frac{1}{2} (X - X^-)^T (P^-)^{-1} (X - X^-) + \sqrt{2} \sum_{j = 0}^{N - 1} \frac{|h_j(X) - Z_j|}{\sigma_j}
$$
Here $j$ denotes vector index and $\sigma_j$ is a standard deviation of independent noise components.
This optimization problem is considerably more difficult, but it is well defined and can be solved by a proper optimization algorithm.</p><p>Other cost functions might be proposed to account for required empirical or theoretical properties of the measurements.</p><h1 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h1><p>The update step of the Extended Kalman Filter was considered as an optimization problem.
Iterated and standard EKF update schemes are shown to be simplified approaches to solve this problem.
The optimization viewpoint is advantageous as it allows for modification of the cost function to account for different measurement model properties.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://nmayorov.github.io>Navigating Uncertainty</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>
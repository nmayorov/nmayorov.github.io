<!doctype html><html lang=en><head><title>Extend Kalman Filter update step as an optimization problem Â· My Blog</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=description content="The Extended Kalman Filter is classically built as an extension of the linear Kalman filter using system and measurement models linearization. In this regard the update step in EKF is naturally done in a single step. The iterated EKF aims to improve the linearization point by doing several update iterations and recomputing measurement Jacobian each time. It&rsquo;s known to be connected with nonlinear optimization.
In this note I want to derive possible EKF update strategies starting from the optimization viewpoint."><meta name=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="Extend Kalman Filter update step as an optimization problem"><meta name=twitter:description content="The Extended Kalman Filter is classically built as an extension of the linear Kalman filter using system and measurement models linearization. In this regard the update step in EKF is naturally done in a single step. The iterated EKF aims to improve the linearization point by doing several update iterations and recomputing measurement Jacobian each time. It&rsquo;s known to be connected with nonlinear optimization.
In this note I want to derive possible EKF update strategies starting from the optimization viewpoint."><meta property="og:title" content="Extend Kalman Filter update step as an optimization problem"><meta property="og:description" content="The Extended Kalman Filter is classically built as an extension of the linear Kalman filter using system and measurement models linearization. In this regard the update step in EKF is naturally done in a single step. The iterated EKF aims to improve the linearization point by doing several update iterations and recomputing measurement Jacobian each time. It&rsquo;s known to be connected with nonlinear optimization.
In this note I want to derive possible EKF update strategies starting from the optimization viewpoint."><meta property="og:type" content="article"><meta property="og:url" content="https://nmayorov.github.io/posts/ekf_update_optimization/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-05-04T00:00:00+00:00"><meta property="article:modified_time" content="2023-05-04T00:00:00+00:00"><link rel=canonical href=https://nmayorov.github.io/posts/ekf_update_optimization/><link rel=preload href="/fonts/forkawesome-webfont.woff2?v=1.2.0" as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.c4d7e93a158eda5a65b3df343745d2092a0a1e2170feeec909b8a89443903c6a.css integrity="sha256-xNfpOhWO2lpls980N0XSCSoKHiFw/u7JCbiolEOQPGo=" crossorigin=anonymous media=screen><link rel=icon type=image/png href=/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/images/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5><meta name=generator content="Hugo 0.115.3"></head><body class="preload-transitions colorscheme-light"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=/>My Blog</a>
<input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/posts/>Posts</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://nmayorov.github.io/posts/ekf_update_optimization/>Extend Kalman Filter update step as an optimization problem</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa fa-calendar" aria-hidden=true></i>
<time datetime=2023-05-04T00:00:00Z>May 4, 2023</time></span>
<span class=reading-time><i class="fa fa-clock-o" aria-hidden=true></i>
6-minute read</span></div></div></header><div><p>The Extended Kalman Filter is classically built as an extension of the linear Kalman filter using system and measurement models linearization.
In this regard the update step in EKF is naturally done in a single step.
The iterated EKF aims to improve the linearization point by doing several update iterations and recomputing measurement Jacobian each time.
It&rsquo;s known to be connected with nonlinear optimization.</p><p>In this note I want to derive possible EKF update strategies starting from the optimization viewpoint.</p><h1 id=the-update-step-in-the-linear-kalman-filter>The update step in the linear Kalman filter
<a class=heading-link href=#the-update-step-in-the-linear-kalman-filter><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>In the update step a prior distribution on the state vector $x \sim \mathcal{N}(x^-, P^-)$ is optimally combined with a measurement $z = H x + v$ with $v \sim \mathcal{N}(0, R)$.
To do so the following optimization problem is formulated:
$$
\min_x J(x) = \frac{1}{2} (x - x^-)^T (P^-)^{-1} (x - x^-) + \frac{1}{2} (H x - z)^T R^{-1} (H x - z)
$$
This is a linear least-squares problem &ndash; the quadratic function has a global minimum where its gradient equals zero:
$$
(P^-)^{-1} (x - x^-) + H^T R^{-1} (H x - z) = 0
$$
This equation can be transformed as follows (with $x$ denoted as $x^+$):
$$
\left((P^-)^{-1} + H^T R^{-1} H \right) x^+ = (P^-)^{-1} x^- + H^T R^{-1} z
$$
And this is an alternative form of the Kalman correction formula equivalent to
$$
x^+ = x^- + K (z - H x^-) \text{ with } K = P^- H^T (H P^- H^T + R)^{-1}
$$</p><h1 id=the-update-step-in-the-extended-kalman-filter>The update step in the Extended Kalman Filter
<a class=heading-link href=#the-update-step-in-the-extended-kalman-filter><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>In a similar fashion, a prior distribution $X \sim \mathcal{N}(X^-, P^-)$ is optimally combined with a nonlinear measurement $Z = h(X) + v$ with $v \sim \mathcal{N}(0, R)$, by forming an optimization problem:
$$
\min_X E(X) = \frac{1}{2} (X - X^-)^T (P^-)^{-1} (X - X^-) + \frac{1}{2} (h(X) - Z)^T R^{-1} (h(X) - Z)
$$
This is a nonlinear least-squares problem which can be solved iteratively by forming a sequence of linear subproblems using linearization around the current estimate.</p><p>Let $X_i$ be an estimate after $i$ iterations with $X_0 \coloneqq X^-$.
An update $x_i$ is sought by substituting $X = X_i + x$ into $E(X)$ and keeping terms linear in $x_i$.
It results in the following linear least-squares problem:
$$
\min_x J(x) = \frac{1}{2} (x - x_i^-)^T (P^-)^{-1} (x - x_i^-) + \frac{1}{2} (H_i x - z_i)^T R^{-1} (H_i x - z)
$$
Where the following variables were introduced:
$$
x^- = X^- - X_i \\
z_i = Z - h(X_i) \\
H_i = \left.\dfrac{\partial h(X)}{\partial X}\right\vert_{X_i}
$$
The linear subproblem has the form as in the linear Kalman filter update step for which the solution is known:
$$
x_i = x_i^- + K_i (z_i - H_i x_i^-) \text{ with } {K_i = P^- H_i^T (H_i P^- H_i^T + R)^{-1}}
$$</p><p>The estimate is then updated as
$$
X_{i + 1} = X_i + \alpha_i x_i
$$
Where $0 &lt; \alpha_i \leq 1$ is selected to ensure the convergence using a procedure known as a line search.
The simplest strategy is to set $\alpha_i = 1$ which works well when $X_i$ is close to the minimizer.</p><p>In the iterated EKF the full corrections are applied with $\alpha_i = 1$:
$$
X_{i + 1} = X_i + x_i = X_i + X^- - X_i + K_i (Z - h(X_i) - H_i (X^- - X_i)) \approx X^- + K_i (Z - h(X^-))
$$
The last approximate equality follows from the first-order Taylor expansion of $h(X)$.
The whole procedure is already based on this and thus such substitution is justifiable.
If implemented using the last equation, the iterated EKF update comes down to updating the point for Jacobian computation $H_i$.
It is generally known that a correct Jacobian value is the most important for consistent EKF update and so this scheme makes practical and conceptual sense.</p><p>The standard EKF update is essentially a single step of the optimization procedure taken with $\alpha_0 = 1$.
It is generally known from optimization theory that such step in &#171;good&#187; conditions can yield estimate very close to the optimum.
This can serve as a conceptual explanation why the standard EKF update often works well in practice.</p><h1 id=discussion>Discussion
<a class=heading-link href=#discussion><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>There are 3 choices when implemented the EKF update:</p><ol><li>Standard EKF update &ndash; the most simple and computationally efficient</li><li>Iterated EKF update using one of two slightly different numerical schemes &ndash; quite simple but at least doubles required computations (because at least 2 iterations are required to check for the convergence)</li><li>Proper optimization with the line search (or other means to ensure convergence) &ndash; the most rigorous, but requires understanding of optimization algorithms and some tuning.
At least doubles required computations</li></ol><p>In approaches 2 and 3 a care of proper convergence control must be taken to avoid infinite or excessively large amount of iterations.</p><p>I believe that the right approach depends on the application (the estimation problem at hand) and should be selected empirically.
I personally have the most experience with the approach 1 and know that it works well for many practical problems with mild measurement nonlinearities.
After all, it was successfully used in many practical systems as the basic EKF is a &#171;go-to&#187; algorithm for the estimation problems.</p><p>More advanced approaches 2 and 3 are worth considering, especially if the computation cost is not a concern.
They can&rsquo;t degrade estimation performance, but might alleviate EKF deficiencies in difficult cases.</p><h1 id=further-leveraging-the-optimization-approach>Further leveraging the optimization approach
<a class=heading-link href=#further-leveraging-the-optimization-approach><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>The optimization approach is powerful because it naturally allows for modification of the cost function.
For example, imagine that the measurement noise instead of normal distribution has Laplace distribution.
Empirically it means that it is more heavy-tailed with more likely outliers.
This situation can be handled by considering a proper loss function (can be thought as negative logarithm of probability):
$$
E(X) = \frac{1}{2} (X - X^-)^T (P^-)^{-1} (X - X^-) + \sqrt{2} \sum_{j = 0}^{N - 1} \frac{|h_j(X) - Z_j|}{\sigma_j}
$$
Here $j$ denotes vector index and $\sigma_j$ is a standard deviation of independent noise components.
This optimization problem is considerably more difficult, but it is well defined and can be solved by a proper optimization algorithm.</p><p>Other cost functions might be proposed to account for required empirical or theoretical properties of the measurements.</p><h1 id=conclusion>Conclusion
<a class=heading-link href=#conclusion><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>The update step of the Extended Kalman Filter was considered as an optimization problem.
Iterated and standard EKF update schemes are shown to be simplified approaches to solve this problem.
The optimization viewpoint is advantageous as it allows for modification of the cost function to account for different measurement model properties.</p></div><footer></footer></article><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script></section></div><footer class=footer><section class=container>Â©
2023
Â·
Powered by <a href=https://gohugo.io/>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/>Coder</a>.</section></footer></main><script src=/js/coder.min.236049395dc3682fb2719640872958e12f1f24067bb09c327b233e6290c7edac.js integrity="sha256-I2BJOV3DaC+ycZZAhylY4S8fJAZ7sJwyeyM+YpDH7aw="></script></body></html>
<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Ellipsoid fitting | Navigating Uncertainty</title><meta name=keywords content><meta name=description content="In this note a problem of ellipsoid fitting to a set of points in arbitrary number of dimensions is considered.
The algorithm development was motivated by the task of magnetometer calibration and some example results for that are given at the end.
Problem formulation
A hyper-ellipsoid in $m$ dimensions is characterized by a center $c \in R^m$ and a symmetric positive-definite shape matrix $P \in R^{m \times m}$ which encodes length and direction of its axes.
Coordinates of a point $x \in R^m$ lying on an ellipsoid satisfy the equation:
$$
(x - c)^T P^{-1} (x - c) = 1
$$"><meta name=author content="Nikolay Mayorov"><link rel=canonical href=https://nmayorov.github.io/posts/ellipsoid_fitting/><link crossorigin=anonymous href=/assets/css/stylesheet.e0b72195d639b2201de2c00d87cdeb7cb0955cac70a34e10561442999111648d.css integrity="sha256-4LchldY5siAd4sANh83rfLCVXKxwo04QVhRCmZERZI0=" rel="preload stylesheet" as=style><link rel=icon href=https://nmayorov.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://nmayorov.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://nmayorov.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://nmayorov.github.io/apple-touch-icon.png><link rel=mask-icon href=https://nmayorov.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://nmayorov.github.io/posts/ellipsoid_fitting/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script src=https://cdn.plot.ly/plotly-3.0.1.min.js></script><style>svg{display:inherit}</style><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script><meta property="og:url" content="https://nmayorov.github.io/posts/ellipsoid_fitting/"><meta property="og:site_name" content="Navigating Uncertainty"><meta property="og:title" content="Ellipsoid fitting"><meta property="og:description" content="In this note a problem of ellipsoid fitting to a set of points in arbitrary number of dimensions is considered. The algorithm development was motivated by the task of magnetometer calibration and some example results for that are given at the end.
Problem formulation A hyper-ellipsoid in $m$ dimensions is characterized by a center $c \in R^m$ and a symmetric positive-definite shape matrix $P \in R^{m \times m}$ which encodes length and direction of its axes. Coordinates of a point $x \in R^m$ lying on an ellipsoid satisfy the equation: $$ (x - c)^T P^{-1} (x - c) = 1 $$"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-11-05T00:00:00+00:00"><meta property="article:modified_time" content="2025-11-05T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Ellipsoid fitting"><meta name=twitter:description content="In this note a problem of ellipsoid fitting to a set of points in arbitrary number of dimensions is considered.
The algorithm development was motivated by the task of magnetometer calibration and some example results for that are given at the end.
Problem formulation
A hyper-ellipsoid in $m$ dimensions is characterized by a center $c \in R^m$ and a symmetric positive-definite shape matrix $P \in R^{m \times m}$ which encodes length and direction of its axes.
Coordinates of a point $x \in R^m$ lying on an ellipsoid satisfy the equation:
$$
(x - c)^T P^{-1} (x - c) = 1
$$"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://nmayorov.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Ellipsoid fitting","item":"https://nmayorov.github.io/posts/ellipsoid_fitting/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Ellipsoid fitting","name":"Ellipsoid fitting","description":"In this note a problem of ellipsoid fitting to a set of points in arbitrary number of dimensions is considered. The algorithm development was motivated by the task of magnetometer calibration and some example results for that are given at the end.\nProblem formulation A hyper-ellipsoid in $m$ dimensions is characterized by a center $c \\in R^m$ and a symmetric positive-definite shape matrix $P \\in R^{m \\times m}$ which encodes length and direction of its axes. Coordinates of a point $x \\in R^m$ lying on an ellipsoid satisfy the equation: $$ (x - c)^T P^{-1} (x - c) = 1 $$\n","keywords":[],"articleBody":"In this note a problem of ellipsoid fitting to a set of points in arbitrary number of dimensions is considered. The algorithm development was motivated by the task of magnetometer calibration and some example results for that are given at the end.\nProblem formulation A hyper-ellipsoid in $m$ dimensions is characterized by a center $c \\in R^m$ and a symmetric positive-definite shape matrix $P \\in R^{m \\times m}$ which encodes length and direction of its axes. Coordinates of a point $x \\in R^m$ lying on an ellipsoid satisfy the equation: $$ (x - c)^T P^{-1} (x - c) = 1 $$\nGiven a set of points $\\{x_i, i = 1, 2 \\ldots, n\\}$ supposedly belonging to an ellipsoid, but recorded with noise, the problem is to determine the parameters of such ellipsoid in some optimal sense. One natural approach to mathematically formulate this problem is a nonlinear least-squares minimization problem: $$ \\text{minimize } F(c, P) = \\sum_{i = 1}^n f_i^2(c, P) \\text{ with respect to } c \\text{ and positive-definite } P, \\\\ \\text{where } f_i(c, P) = (x_i - c)^T P^{-1} (x_i - c) - 1 $$\nThe proposed innovation $f_i$ represents a deviation of point’s “level” from the ellipsoid level of 1. It is also formally a deviation from 1 of squared Mahalanobis distance between the ellipsoid center and a point using the ellipsoid shape matrix $P$.\nFor a circle it comes down to the deviation of the squared distance between the center and a point from the squared radius, normalized by the radius:\n$d^2 = (x - x_0)^2 + (y - y_0)^2 - r^2$ — deviation of the squared distance from the squared radius $f = \\dfrac{d^2}{r^2} = \\dfrac{(x - x_0)^2}{r^2} + \\dfrac{(y - y_0)^2}{r^2} - 1$ — the normalized deviation In general such normalized innovations make formulation agnostic to the ellipsoid scale, which is a positive thing for an optimization algorithm.\nApproach to the solution To solve the proposed problem by an existing algorithm like least_squares we need to clarify some issues.\nFirst, to ensure that $P$ is positive-definite we parameterize it by lower-triangular Cholesky factors $P^{-1} = L^T L$. The problem becomes unconstrained in terms of the matrix $L$. It can be stated as finding a transform $z = L(x - c)$ to make points $z$ lie approximately on a unit sphere centered at origin.\nSecond, as seen in practice, when the points occupy only a small sector of the ellipsoid (especially if concentrated in one “hemisphere”), the problem becomes ill-conditioned where many solutions with comparable cost exist. Moreover, such solutions often tend to drift towards large and skewed ellipsoids, which don’t make sense in practice. A remedy to that is to introduce a regularization term which will penalize ellipsoids which strongly deviate from a sphere of a given radius $r$ (algorithm parameter): $$ P \\approx r^2 I \\\\ r^2 P^{-1} \\approx I \\\\ r L \\approx I $$ The constraint comes down to adding $m (m + 1) / 2$ residuals of the form: $$ f_r = r L - I $$ Where the matrix is flattened to make a vector.\nWe also need to properly balance the fitting and tregularization residuals. To do that, we divide the two kind of residuals by square root of their count and also introduce a factor $\\alpha$ to control the extent of the regularization: $$ f_i^\\prime = \\frac{1}{\\sqrt{n}} f_i \\\\ f_r^\\prime = \\sqrt{\\frac{2 \\alpha}{m (m + 1)}} f_r $$ The total expected cost function will be: $$ \\left\u003c F^\\prime \\right\u003e = \\left + \\alpha \\left $$ Where $F_f$ is a single point fit error cost and $F_r$ is the regularization error cost per matrix element. The point is that in this form the error terms are agnostic to the number of points and dimensions, whereas $\\alpha$ controls the level of regularization in a straightforward fashion. The reasonable values of $\\alpha$ are in the range from 0 to 1, with 0 obviously being the case without regularization.\nJacobian formulas It’s preferable to provide an analytic Jacobian of residuals with respect to unknown parameters for least-squares minimization. For convenience in this section the index of a point is omitted and vector component indices are used instead.\nThe Jacobian (gradient vector) with respect to $c$ is a known result from matrix calculus: $$ \\frac{\\partial f}{\\partial c} = 2 L L^T (c - x) $$\nIt is easier to obtain the Jacobian with respect to the elements of $L$ considering an example with 3 dimensions. Let denote $y = x - c$, then $$ z = L y = \\begin{bmatrix} L_{11} y_1 \\\\ L_{12} y_1 + L_{22} y_2 \\\\ L_{13} y_1 + L_{23} y_2 + L_{33} y_3 \\end{bmatrix} \\\\ f = z^T z - 1 = (L_{11} y_1)^2 + (L_{12} y_1 + L_{22} y_2)^2 + (L_{13} y_1 + L_{23} y_2 + L_{33} y_3)^3 - 1 $$ Then it’s easy to compute derivatives arranging them in a matrix for convenient representation: $$ \\frac{\\partial f}{\\partial L} = 2 \\begin{bmatrix} z_1 y_1 \u0026 \u0026 \\\\ z_2 y_1 \u0026 z_2 y_2 \u0026 \\\\ z_3 y_1 \u0026 z_3 y_2 \u0026 z_3 y_3 \\end{bmatrix} \\\\ \\frac{\\partial f}{\\partial L_{ij}} = 2 z_i y_j $$\nThe regularization residual is linear in $L$ and thus the Jacobian is constant: $$ \\frac{\\partial f_r}{\\partial L_{ij}} = r $$\nParameter initialization The center $c$ is initialized as a mean value of all given points (centroid). For the scale matrix $L$ there are two options:\nIf the regularization radius $r$ is given, $L$ is initialized from that If not given, $L$ is initialized as diagonal from a bounding box with sides along coordinate axes In both cases no initial axes rotations are assumed.\nImplementation in Python Here is a possible implementation of this algorithm relying on least_squares. For simplicity the core optimization is run with default parameters. The code should not be considered “library quality”, but it is likely to work in most cases.\nimport numpy as np import scipy def fit_ellipsoid(X, alpha=0, expected_radius=None): \"\"\"Fit ellipsoid to points. The function finds parameters of hyper-ellipsoid which in least-squares sense describes a given set of points. The ellipsoid characterized by its center `c` and positive-definite shape matrix `P = (L^T L)^-1`, where `L` is lower triangular matrix. The residuals in least-squares minimization are formed as normalized squared distances of the form `f = (x - c)^T L^T L (x - c) - 1`. The algorithm uses optional regularization which penalizes deviation of the ellipsoid from a sphere of a given radius. Parameters ---------- X : array_like, shape (n_points, n_dimensions) Array of points. Each row represents a point. alpha : float, optional Ellipsoid shape regularization parameter. Default is 0. expected_radius : float or None, optional Expected radius of a sphere from which the ellipsoid shape must not deviate too much. If None, regularization is disabled. Returns ------- c : ndarray, shape (n_dimensions,) Ellipsoid center. P : ndarray, shape (n_dimensions, n_dimensions) Positive-definite ellipsoid shape matrix encoding direction and length of axes. L : ndarray, shape (n_dimensions, n_dimensions) Lower-triangular transform matrix. L_inv : ndarray, shape (n_dimensions, n_dimensions) Inverse of L. opt_result : scipy.optimize.OptimizationResult Raw optimization result from scipy.optimize.least_squares. \"\"\" X = np.asarray(X) n, m = X.shape row_ind, col_ind = np.tril_indices(m) I_flat = np.eye(m)[row_ind, col_ind] def unpack_x(x): c = x[:m] l = x[m:] L = np.zeros((m, m)) L[row_ind, col_ind] = l return c, l, L def fun(x): c, l, L = unpack_x(x) Y = X - c Z = (L @ Y.T).T return np.hstack( [ 1 / n**0.5 * (np.sum(Z**2, axis=1) - 1), (alpha / len(l)) ** 0.5 * (expected_radius * l - I_flat), ] ) def jac(x): c, l, L = unpack_x(x) Y = X - c Z = (L @ Y.T).T ZY = np.einsum(\"...i,...j-\u003e...ij\", Z, Y) return np.block( [ [-2 / n**0.5 * Z @ L, 2 / n**0.5 * ZY[:, row_ind, col_ind]], [ np.zeros((len(l), m)), (alpha / len(l)) ** 0.5 * expected_radius * np.eye(len(l)), ], ] ) x0 = np.hstack([c, L[row_ind, col_ind]]) opt_result = scipy.optimize.least_squares(fun, x0, jac=jac, verbose=2) c, _, L = unpack_x(opt_result.x) L_inv = scipy.linalg.solve_triangular(L, np.eye(m), lower=True) P = L_inv @ L_inv.T return c, P, L, L_inv, opt_result Examples of fitting Considering geometric nature of the problem it seems sufficient to demonstrate and assess fitting quality visually.\nFitting ellipse on synthetic data Here we simply generate points belonging to an ellipse with semi-axes of 4 and 2 tilted by 45 degrees and consider the algorithm behavior depending on which points are passed to the algorithm. All runs were executed with expected_radius = 3.\nWe can see that when the points fully define the shape and location of the ellipse the fit is good and regularization of 0.01 and 0.1 doesn’t distort the shape significantly, nor it is necessary. In case of the points occupying only one short arc, the problem is ill-conditioned and the fitted shape strongly depends on a regularization parameter, whereas “correct” fit seems impossible to find.\nIn general to get proper and meaningful results we must avoid cases depicted on the third plot.\nFitting ellipsoid to magnetometer measurements One of well-known applications of ellipsoid fitting is magnetometer calibration where the aim is to estimate its bias (“hard iron”) and transform matrix (“soft iron”). The idea is that in a calibrated sensor the Earth magnetic field must occupy a sphere when the magnetometer rotates around. To estimate the aforementioned intrinsic parameters we fit an ellipsoid to the recorded set of measured vectors, provided that the sensor underwent sufficient rotations. The estimated matrix $L$ will in fact serve as required “soft iron” calibration matrix.\nNote, that there are some intricacies of 2D vs 3D calibration and practical algorithms might rely on some other tricks and heuristics. Here examples are shown for two simple cases where the sensor was rotated around all axes in full ranges. No regularization were used as it seems unnecessary.\n","wordCount":"1628","inLanguage":"en","datePublished":"2025-11-05T00:00:00Z","dateModified":"2025-11-05T00:00:00Z","author":{"@type":"Person","name":"Nikolay Mayorov"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://nmayorov.github.io/posts/ellipsoid_fitting/"},"publisher":{"@type":"Organization","name":"Navigating Uncertainty","logo":{"@type":"ImageObject","url":"https://nmayorov.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://nmayorov.github.io/ accesskey=h title="Navigating Uncertainty (Alt + H)">Navigating Uncertainty</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://nmayorov.github.io/open_source title="Open source"><span>Open source</span></a></li><li><a href=https://nmayorov.github.io/archives title="All posts"><span>All posts</span></a></li><li><a href=https://nmayorov.github.io/tags title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://nmayorov.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://nmayorov.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Ellipsoid fitting</h1><div class=post-meta><span title='2025-11-05 00:00:00 +0000 UTC'>November 5, 2025</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;Nikolay Mayorov</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#problem-formulation aria-label="Problem formulation">Problem formulation</a></li><li><a href=#approach-to-the-solution aria-label="Approach to the solution">Approach to the solution</a><ul><li><a href=#jacobian-formulas aria-label="Jacobian formulas">Jacobian formulas</a></li><li><a href=#parameter-initialization aria-label="Parameter initialization">Parameter initialization</a></li></ul></li><li><a href=#implementation-in-python aria-label="Implementation in Python">Implementation in Python</a></li><li><a href=#examples-of-fitting aria-label="Examples of fitting">Examples of fitting</a><ul><li><a href=#fitting-ellipse-on-synthetic-data aria-label="Fitting ellipse on synthetic data">Fitting ellipse on synthetic data</a></li><li><a href=#fitting-ellipsoid-to-magnetometer-measurements aria-label="Fitting ellipsoid to magnetometer measurements">Fitting ellipsoid to magnetometer measurements</a></li></ul></li></ul></div></details></div><div class=post-content><p>In this note a problem of ellipsoid fitting to a set of points in arbitrary number of dimensions is considered.
The algorithm development was motivated by the task of magnetometer calibration and some example results for that are given at the end.</p><h1 id=problem-formulation>Problem formulation<a hidden class=anchor aria-hidden=true href=#problem-formulation>#</a></h1><p>A hyper-ellipsoid in $m$ dimensions is characterized by a center $c \in R^m$ and a symmetric positive-definite shape matrix $P \in R^{m \times m}$ which encodes length and direction of its axes.
Coordinates of a point $x \in R^m$ lying on an ellipsoid satisfy the equation:
$$
(x - c)^T P^{-1} (x - c) = 1
$$</p><p>Given a set of points $\{x_i, i = 1, 2 \ldots, n\}$ supposedly belonging to an ellipsoid, but recorded with noise, the problem is to determine the parameters of such ellipsoid in some optimal sense.
One natural approach to mathematically formulate this problem is a nonlinear least-squares minimization problem:
$$
\text{minimize } F(c, P) = \sum_{i = 1}^n f_i^2(c, P) \text{ with respect to } c \text{ and positive-definite } P, \\
\text{where } f_i(c, P) = (x_i - c)^T P^{-1} (x_i - c) - 1
$$</p><p>The proposed innovation $f_i$ represents a deviation of point&rsquo;s &ldquo;level&rdquo; from the ellipsoid level of 1.
It is also formally a deviation from 1 of squared Mahalanobis distance between the ellipsoid center and a point using the ellipsoid shape matrix $P$.</p><p>For a circle it comes down to the deviation of the squared distance between the center and a point from the squared radius, normalized by the radius:</p><ul><li>$d^2 = (x - x_0)^2 + (y - y_0)^2 - r^2$ &mdash; deviation of the squared distance from the squared radius</li><li>$f = \dfrac{d^2}{r^2} = \dfrac{(x - x_0)^2}{r^2} + \dfrac{(y - y_0)^2}{r^2} - 1$ &mdash; the normalized deviation</li></ul><p>In general such normalized innovations make formulation agnostic to the ellipsoid scale, which is a positive thing for an optimization algorithm.</p><h1 id=approach-to-the-solution>Approach to the solution<a hidden class=anchor aria-hidden=true href=#approach-to-the-solution>#</a></h1><p>To solve the proposed problem by an existing algorithm like <a href=https://scipy.github.io/devdocs/reference/generated/scipy.optimize.least_squares.html#scipy.optimize.least_squares>least_squares</a> we need to clarify some issues.</p><p>First, to ensure that $P$ is positive-definite we parameterize it by lower-triangular Cholesky factors $P^{-1} = L^T L$.
The problem becomes unconstrained in terms of the matrix $L$.
It can be stated as finding a transform $z = L(x - c)$ to make points $z$ lie approximately on a unit sphere centered at origin.</p><p>Second, as seen in practice, when the points occupy only a small sector of the ellipsoid (especially if concentrated in one &ldquo;hemisphere&rdquo;), the problem becomes ill-conditioned where many solutions with comparable cost exist.
Moreover, such solutions often tend to drift towards large and skewed ellipsoids, which don&rsquo;t make sense in practice.
A remedy to that is to introduce a regularization term which will penalize ellipsoids which strongly deviate from a sphere of a given radius $r$ (algorithm parameter):
$$
P \approx r^2 I \\
r^2 P^{-1} \approx I \\
r L \approx I
$$
The constraint comes down to adding $m (m + 1) / 2$ residuals of the form:
$$
f_r = r L - I
$$
Where the matrix is flattened to make a vector.</p><p>We also need to properly balance the fitting and tregularization residuals.
To do that, we divide the two kind of residuals by square root of their count and also introduce a factor $\alpha$ to control the extent of the regularization:
$$
f_i^\prime = \frac{1}{\sqrt{n}} f_i \\
f_r^\prime = \sqrt{\frac{2 \alpha}{m (m + 1)}} f_r
$$
The total expected cost function will be:
$$
\left&lt; F^\prime \right> = \left&lt;F_f\right> + \alpha \left&lt;F_r\right>
$$
Where $F_f$ is a single point fit error cost and $F_r$ is the regularization error cost per matrix element.
The point is that in this form the error terms are agnostic to the number of points and dimensions, whereas $\alpha$ controls the level of regularization in a straightforward fashion.
The reasonable values of $\alpha$ are in the range from 0 to 1, with 0 obviously being the case without regularization.</p><h2 id=jacobian-formulas>Jacobian formulas<a hidden class=anchor aria-hidden=true href=#jacobian-formulas>#</a></h2><p>It&rsquo;s preferable to provide an analytic Jacobian of residuals with respect to unknown parameters for least-squares minimization.
For convenience in this section the index of a point is omitted and vector component indices are used instead.</p><p>The Jacobian (gradient vector) with respect to $c$ is a known result from matrix calculus:
$$
\frac{\partial f}{\partial c} = 2 L L^T (c - x)
$$</p><p>It is easier to obtain the Jacobian with respect to the elements of $L$ considering an example with 3 dimensions.
Let denote $y = x - c$, then
$$
z = L y = \begin{bmatrix}
L_{11} y_1 \\
L_{12} y_1 + L_{22} y_2 \\
L_{13} y_1 + L_{23} y_2 + L_{33} y_3
\end{bmatrix} \\
f = z^T z - 1 = (L_{11} y_1)^2 + (L_{12} y_1 + L_{22} y_2)^2 + (L_{13} y_1 + L_{23} y_2 + L_{33} y_3)^3 - 1
$$
Then it&rsquo;s easy to compute derivatives arranging them in a matrix for convenient representation:
$$
\frac{\partial f}{\partial L} = 2 \begin{bmatrix}
z_1 y_1 & & \\
z_2 y_1 & z_2 y_2 & \\
z_3 y_1 & z_3 y_2 & z_3 y_3
\end{bmatrix} \\
\frac{\partial f}{\partial L_{ij}} = 2 z_i y_j
$$</p><p>The regularization residual is linear in $L$ and thus the Jacobian is constant:
$$
\frac{\partial f_r}{\partial L_{ij}} = r
$$</p><h2 id=parameter-initialization>Parameter initialization<a hidden class=anchor aria-hidden=true href=#parameter-initialization>#</a></h2><p>The center $c$ is initialized as a mean value of all given points (centroid).
For the scale matrix $L$ there are two options:</p><ul><li>If the regularization radius $r$ is given, $L$ is initialized from that</li><li>If not given, $L$ is initialized as diagonal from a bounding box with sides along coordinate axes</li></ul><p>In both cases no initial axes rotations are assumed.</p><h1 id=implementation-in-python>Implementation in Python<a hidden class=anchor aria-hidden=true href=#implementation-in-python>#</a></h1><p>Here is a possible implementation of this algorithm relying on <a href=https://scipy.github.io/devdocs/reference/generated/scipy.optimize.least_squares.html#scipy.optimize.least_squares>least_squares</a>.
For simplicity the core optimization is run with default parameters.
The code should not be considered &ldquo;library quality&rdquo;, but it is likely to work in most cases.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> scipy
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>fit_ellipsoid</span>(X, alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>, expected_radius<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;Fit ellipsoid to points.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    The function finds parameters of hyper-ellipsoid which in least-squares sense
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    describes a given set of points. The ellipsoid characterized by its center `c` and
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    positive-definite shape matrix `P = (L^T L)^-1`, where `L` is lower triangular
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    matrix. The residuals in least-squares minimization are formed as normalized
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    squared distances of the form `f = (x - c)^T L^T L (x - c) - 1`.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    The algorithm uses optional regularization which penalizes deviation of the
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    ellipsoid from a sphere of a given radius.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Parameters
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    ----------
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    X : array_like, shape (n_points, n_dimensions)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Array of points. Each row represents a point.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    alpha : float, optional
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Ellipsoid shape regularization parameter. Default is 0.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    expected_radius : float or None, optional
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Expected radius of a sphere from which the ellipsoid shape must not deviate too
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        much. If None, regularization is disabled.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Returns
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    -------
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    c : ndarray, shape (n_dimensions,)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Ellipsoid center.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    P : ndarray, shape (n_dimensions, n_dimensions)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Positive-definite ellipsoid shape matrix encoding direction and length of axes.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    L : ndarray, shape (n_dimensions, n_dimensions)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Lower-triangular transform matrix.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    L_inv : ndarray, shape (n_dimensions, n_dimensions)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Inverse of L.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    opt_result : scipy.optimize.OptimizationResult
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Raw optimization result from scipy.optimize.least_squares.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    X <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>asarray(X)
</span></span><span style=display:flex><span>    n, m <span style=color:#f92672>=</span> X<span style=color:#f92672>.</span>shape
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    row_ind, col_ind <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>tril_indices(m)
</span></span><span style=display:flex><span>    I_flat <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>eye(m)[row_ind, col_ind]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>unpack_x</span>(x):
</span></span><span style=display:flex><span>        c <span style=color:#f92672>=</span> x[:m]
</span></span><span style=display:flex><span>        l <span style=color:#f92672>=</span> x[m:]
</span></span><span style=display:flex><span>        L <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros((m, m))
</span></span><span style=display:flex><span>        L[row_ind, col_ind] <span style=color:#f92672>=</span> l
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> c, l, L
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>fun</span>(x):
</span></span><span style=display:flex><span>        c, l, L <span style=color:#f92672>=</span> unpack_x(x)
</span></span><span style=display:flex><span>        Y <span style=color:#f92672>=</span> X <span style=color:#f92672>-</span> c
</span></span><span style=display:flex><span>        Z <span style=color:#f92672>=</span> (L <span style=color:#f92672>@</span> Y<span style=color:#f92672>.</span>T)<span style=color:#f92672>.</span>T
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>hstack(
</span></span><span style=display:flex><span>            [
</span></span><span style=display:flex><span>                <span style=color:#ae81ff>1</span> <span style=color:#f92672>/</span> n<span style=color:#f92672>**</span><span style=color:#ae81ff>0.5</span> <span style=color:#f92672>*</span> (np<span style=color:#f92672>.</span>sum(Z<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span>, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>) <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>),
</span></span><span style=display:flex><span>                (alpha <span style=color:#f92672>/</span> len(l)) <span style=color:#f92672>**</span> <span style=color:#ae81ff>0.5</span> <span style=color:#f92672>*</span> (expected_radius <span style=color:#f92672>*</span> l <span style=color:#f92672>-</span> I_flat),
</span></span><span style=display:flex><span>            ]
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>jac</span>(x):
</span></span><span style=display:flex><span>        c, l, L <span style=color:#f92672>=</span> unpack_x(x)
</span></span><span style=display:flex><span>        Y <span style=color:#f92672>=</span> X <span style=color:#f92672>-</span> c
</span></span><span style=display:flex><span>        Z <span style=color:#f92672>=</span> (L <span style=color:#f92672>@</span> Y<span style=color:#f92672>.</span>T)<span style=color:#f92672>.</span>T
</span></span><span style=display:flex><span>        ZY <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>einsum(<span style=color:#e6db74>&#34;...i,...j-&gt;...ij&#34;</span>, Z, Y)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>block(
</span></span><span style=display:flex><span>            [
</span></span><span style=display:flex><span>                [<span style=color:#f92672>-</span><span style=color:#ae81ff>2</span> <span style=color:#f92672>/</span> n<span style=color:#f92672>**</span><span style=color:#ae81ff>0.5</span> <span style=color:#f92672>*</span> Z <span style=color:#f92672>@</span> L, <span style=color:#ae81ff>2</span> <span style=color:#f92672>/</span> n<span style=color:#f92672>**</span><span style=color:#ae81ff>0.5</span> <span style=color:#f92672>*</span> ZY[:, row_ind, col_ind]],
</span></span><span style=display:flex><span>                [
</span></span><span style=display:flex><span>                    np<span style=color:#f92672>.</span>zeros((len(l), m)),
</span></span><span style=display:flex><span>                    (alpha <span style=color:#f92672>/</span> len(l)) <span style=color:#f92672>**</span> <span style=color:#ae81ff>0.5</span> <span style=color:#f92672>*</span> expected_radius <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>eye(len(l)),
</span></span><span style=display:flex><span>                ],
</span></span><span style=display:flex><span>            ]
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    x0 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>hstack([c, L[row_ind, col_ind]])
</span></span><span style=display:flex><span>    opt_result <span style=color:#f92672>=</span> scipy<span style=color:#f92672>.</span>optimize<span style=color:#f92672>.</span>least_squares(fun, x0, jac<span style=color:#f92672>=</span>jac, verbose<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    c, _, L <span style=color:#f92672>=</span> unpack_x(opt_result<span style=color:#f92672>.</span>x)
</span></span><span style=display:flex><span>    L_inv <span style=color:#f92672>=</span> scipy<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>solve_triangular(L, np<span style=color:#f92672>.</span>eye(m), lower<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    P <span style=color:#f92672>=</span> L_inv <span style=color:#f92672>@</span> L_inv<span style=color:#f92672>.</span>T
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> c, P, L, L_inv, opt_result
</span></span></code></pre></div><h1 id=examples-of-fitting>Examples of fitting<a hidden class=anchor aria-hidden=true href=#examples-of-fitting>#</a></h1><p>Considering geometric nature of the problem it seems sufficient to demonstrate and assess fitting quality visually.</p><h2 id=fitting-ellipse-on-synthetic-data>Fitting ellipse on synthetic data<a hidden class=anchor aria-hidden=true href=#fitting-ellipse-on-synthetic-data>#</a></h2><p>Here we simply generate points belonging to an ellipse with semi-axes of 4 and 2 tilted by 45 degrees and consider the algorithm behavior depending on which points are passed to the algorithm.
All runs were executed with <code>expected_radius = 3</code>.</p><p><a href=figs/ellipse_full.svg><img alt=ellipse_full loading=lazy src=/posts/ellipsoid_fitting/figs/ellipse_full.svg></a>
<a href=figs/ellipse_2_sectors.svg><img alt=ellipse_full loading=lazy src=/posts/ellipsoid_fitting/figs/ellipse_2_sectors.svg></a>
<a href=figs/ellipse_1_sector.svg><img alt=ellipse_full loading=lazy src=/posts/ellipsoid_fitting/figs/ellipse_1_sector.svg></a></p><p>We can see that when the points fully define the shape and location of the ellipse the fit is good and regularization of 0.01 and 0.1 doesn&rsquo;t distort the shape significantly, nor it is necessary.
In case of the points occupying only one short arc, the problem is ill-conditioned and the fitted shape strongly depends on a regularization parameter, whereas &ldquo;correct&rdquo; fit seems impossible to find.</p><p>In general to get proper and meaningful results we must avoid cases depicted on the third plot.</p><h2 id=fitting-ellipsoid-to-magnetometer-measurements>Fitting ellipsoid to magnetometer measurements<a hidden class=anchor aria-hidden=true href=#fitting-ellipsoid-to-magnetometer-measurements>#</a></h2><p>One of well-known applications of ellipsoid fitting is magnetometer calibration where the aim is to estimate its bias (&ldquo;hard iron&rdquo;) and transform matrix (&ldquo;soft iron&rdquo;).
The idea is that in a calibrated sensor the Earth magnetic field must occupy a sphere when the magnetometer rotates around.
To estimate the aforementioned intrinsic parameters we fit an ellipsoid to the recorded set of measured vectors, provided that the sensor underwent sufficient rotations.
The estimated matrix $L$ will in fact serve as required &ldquo;soft iron&rdquo; calibration matrix.</p><p>Note, that there are some intricacies of 2D vs 3D calibration and practical algorithms might rely on some other tricks and heuristics.
Here examples are shown for two simple cases where the sensor was rotated around all axes in full ranges.
No regularization were used as it seems unnecessary.</p><p><div id=figs/calibration_1.json class=plotly></div><script>async function loadPlotly(){try{const n=await fetch("figs/calibration_1.json"),t=await n.json();var e=t.layout||{};e.modebar={orientation:"h"},e.margin={l:0,r:0,t:40,b:40},Plotly.newPlot("figs/calibration_1.json",t.data,e,{displaylogo:!1,responsive:!0})}catch(e){console.error("Error loading plotly json:",e)}}loadPlotly()</script><div id=figs/calibration_2.json class=plotly></div><script>async function loadPlotly(){try{const n=await fetch("figs/calibration_2.json"),t=await n.json();var e=t.layout||{};e.modebar={orientation:"h"},e.margin={l:0,r:0,t:40,b:40},Plotly.newPlot("figs/calibration_2.json",t.data,e,{displaylogo:!1,responsive:!0})}catch(e){console.error("Error loading plotly json:",e)}}loadPlotly()</script></p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://nmayorov.github.io/>Navigating Uncertainty</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>
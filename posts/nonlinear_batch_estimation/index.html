<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Nonlinear batch estimation | My Blog</title><meta name=keywords content><meta name=description content="In this note I present a development of nonlinear batch estimation algorithm.
Model description The state estimation problem in a nonlinear system is considered. The formulations is analogous to the linear case, but with nonlinear transition and measurement equations. We use uppercase letters to denote variables participating in the nonlinear model. Time transition and measurement equations for which are $$ Z_k = f_k(X_k) + V_k \\ X_{k+1} = h_k(X_k, W_k) $$ All the other assumptions remain the same."><meta name=author content="Nikolay Mayorov"><link rel=canonical href=https://nmayorov.github.io/posts/nonlinear_batch_estimation/><link crossorigin=anonymous href=/assets/css/stylesheet.21e7558a953920031f5c62b4db91512093af342ec251fdf4bc1be327b0e259b4.css integrity="sha256-IedVipU5IAMfXGK025FRIJOvNC7CUf30vBvjJ7DiWbQ=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://nmayorov.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://nmayorov.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://nmayorov.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://nmayorov.github.io/apple-touch-icon.png><link rel=mask-icon href=https://nmayorov.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script><meta property="og:title" content="Nonlinear batch estimation"><meta property="og:description" content="In this note I present a development of nonlinear batch estimation algorithm.
Model description The state estimation problem in a nonlinear system is considered. The formulations is analogous to the linear case, but with nonlinear transition and measurement equations. We use uppercase letters to denote variables participating in the nonlinear model. Time transition and measurement equations for which are $$ Z_k = f_k(X_k) + V_k \\ X_{k+1} = h_k(X_k, W_k) $$ All the other assumptions remain the same."><meta property="og:type" content="article"><meta property="og:url" content="https://nmayorov.github.io/posts/nonlinear_batch_estimation/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-11-30T00:00:00+00:00"><meta property="article:modified_time" content="2022-11-30T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Nonlinear batch estimation"><meta name=twitter:description content="In this note I present a development of nonlinear batch estimation algorithm.
Model description The state estimation problem in a nonlinear system is considered. The formulations is analogous to the linear case, but with nonlinear transition and measurement equations. We use uppercase letters to denote variables participating in the nonlinear model. Time transition and measurement equations for which are $$ Z_k = f_k(X_k) + V_k \\ X_{k+1} = h_k(X_k, W_k) $$ All the other assumptions remain the same."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://nmayorov.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Nonlinear batch estimation","item":"https://nmayorov.github.io/posts/nonlinear_batch_estimation/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Nonlinear batch estimation","name":"Nonlinear batch estimation","description":"In this note I present a development of nonlinear batch estimation algorithm.\nModel description The state estimation problem in a nonlinear system is considered. The formulations is analogous to the linear case, but with nonlinear transition and measurement equations. We use uppercase letters to denote variables participating in the nonlinear model. Time transition and measurement equations for which are $$ Z_k = f_k(X_k) + V_k \\\\ X_{k+1} = h_k(X_k, W_k) $$ All the other assumptions remain the same.","keywords":[],"articleBody":"In this note I present a development of nonlinear batch estimation algorithm.\nModel description The state estimation problem in a nonlinear system is considered. The formulations is analogous to the linear case, but with nonlinear transition and measurement equations. We use uppercase letters to denote variables participating in the nonlinear model. Time transition and measurement equations for which are $$ Z_k = f_k(X_k) + V_k \\\\ X_{k+1} = h_k(X_k, W_k) $$ All the other assumptions remain the same. The task is to estimate $X_k$ for epochs $k = 0, 1, \\ldots, N$. This will be done by solving an optimization problem.\nOptimization problem Similarly to the linear case the cost function is defined as follows $$ \\begin{split} E(X, W) =\u0026 \\frac{1}{2} (X_0 - X_0^-)^T P_0^- (X_0 - X_0^-) \\\\ +\u0026 \\frac{1}{2} \\sum_{k = 0}^{N-1} (h_k(X_k) - Z_k)^T R_k^{-1} (h_k(X_k) - Z_k) \\\\ +\u0026 \\frac{1}{2} \\sum_{k = 0}^{N-1} W_k^T Q_k^{-1} W_k \\end{split} $$ It must be optimized taking into account the time transition equations: $$ \\min_{X, W} \\space E(X, W) \\text{ subject to} \\space X_{k + 1} = f_k(X_k, W_k) $$ It can be classified as a nonlinear least-squares problem with nonlinear equality constraints on the variables.\nOutline of the solution method Consider a general optimization problem of the form: $$ \\min_x f(x) \\text{ subject to } c(x) = 0 $$ One established method of solving it called sequential quadratic programming (SQP) goes like this:\nAt current estimate $x_i$ form a local quadratic approximation of the Lagrange function for the problem and a linear approximation of the constraints Solve this quadratic programming task to obtain step $p_k$ to the next estimate Form the next estimate $x_{i+1} = x_i + \\alpha_i p_i$ Repeat until convergence To form the mentioned local quadratic approximation, the gradient (gradient of $f$ might be used with the equivalent result) and the Hessian of the Lagrange function needs to be formed. The method we apply differs in how the Hessian approximation is formed and might be called a Gauss-Newton approximation to the SQP method.\nIn a nonlinear least-squares problem the cost function has the form $$ f(x) = \\frac{1}{2} r(x)^T r(x) $$ The Jacobian matrix of $r(x)$ is defined as $$ J(x) = \\begin{bmatrix} \\nabla r_1(x) \u0026 \\nabla r_2(x) \u0026 \\ldots \u0026 \\nabla r_n(x) \\end{bmatrix}^T $$ Then the gradient and Gauss-Newton approximation of the Hessian are: $$ \\nabla f(x) = J^T r(x) \\\\ \\nabla^2 f(x) \\approx J(x)^T J(x) $$ Opposed to the SQP method we also neglect the second derivatives of the constraints, i. e. form a quadratic model for the cost function, not for the Lagrangian.\nWith these approximations the quadratic subproblem is $$ \\min_{p_i} (r_i + J_i p_i)^T (r_i + J_i p_i) \\text{ subject to } c_i + A_i p_i = 0 \\\\ \\text{with } r_i = r(x_i), J_i = J(x_i), c_i = c(x_i), A_i = \\nabla c(x) \\vert_{x_i} $$ Logic of the algorithm iterations remain the same.\nIt must be said that the proposed method is not widely known. However my intuition and limited experience tells me that it is a legit approach similar to Gauss-Newton method of solving unconstrained nonlinear least squares.\nFor our specific problem the linear subproblem formulated above is solved by a Kalman smoother.\nMerit function and line search To guarantee convergence of a nonlinear optimization method a step $p$ obtained from a linear subproblem must be applied with a properly selected multiplier as $\\alpha p$. In constrained optimization there are 2 objectives: minimize the cost function and satisfy the constraints. One approach to account for both ot them is to consider a merit function ([1], section 11.2) with $\\mu \u003e 0$: $$ \\phi(x; \\mu) = f(x) + \\mu \\lVert c(x) \\rVert_1 $$ The parameter $\\alpha$ is selected to achieve a sufficient decrease of $\\phi(x; \\mu)$ (the Armijo condition) for some $0 \u003c \\eta \u003c 1$: $$ \\phi(x_i + \\alpha_i p_i; \\mu) \\leq \\phi(x_i; \\mu) + \\eta \\alpha_i D(\\phi(x_i; \\mu); p_i) $$ Where $D(g(x); l)$ is a directional derivative of $g(x)$ in direction $l$: $$ D(g(x); l) \\coloneqq \\lim_{\\epsilon \\rightarrow 0} \\frac{g(x + \\epsilon l) - g(x)}{\\epsilon} $$ The derivative of the introduced merit function in the direction $p_i$ (which satisfies linearized constraints!) can be shown to be ([1], section 18.3) $$ D(\\phi(x_i; \\mu); p_i) = \\nabla f_i^T p_i - \\mu \\lVert c_i \\rVert_1 $$ In order to $p_i$ be the descent direction for the merit function the directional derivative must be negative. This can be achieved by selecting large enough $\\mu$. Informally it means that if necessary the optimizer must take steps which don’t reduce the cost function, but only constraint violation. The specific strategy of selecting $\\mu$ suggested in [1] (section 18.3) is explained next.\nUsing the quadratic model $q(p)$ of $f(x)$ and linear model of $c(x)$ near $x_i$ it is possible to estimate the change of the merit function for step $p_i$: $$ \\phi(x_i + p_i; \\mu) - \\phi(x_i) \\approx q_i(p_i) + \\mu \\lVert c_i + A_i p_i \\rVert_1 - q_i(0) - \\mu \\lVert c_i \\rVert_1 = q_i(p_i) - q_i(0) - \\mu \\lVert c_i \\rVert_1 $$ The aim is to have this change sufficiently negative $$ q_i(p_i) - q_i(0) - \\mu \\lVert c_i \\rVert_1 \\leq -\\rho \\mu \\lVert c_i \\rVert_1 $$ for some $0 \u003c \\rho \u003c 1$. From this the inequality for $\\mu$ follows $$ \\mu \\geq \\frac{q_i(p_i) - q_i(0)}{(1 - \\rho) \\lVert c_i \\rVert_1} = \\frac{\\nabla f_i^T p_i + (1/2) p_i^T H_i p_i}{(1 - \\rho) \\lVert c_i \\rVert_1} $$ Where $H_i$ is the Hessian of the Lagrange function or its approximation evaluated at $x_i$.\nIf $\\mu$ satisfies the above inequality and $H_i$ is positive semidefinite we have for the directional derivative $$ D(\\phi(x_i; \\mu); p_i) \\leq -\\rho \\mu \\lVert c_i \\rVert_1 \u003c 0 $$ Meaning that $p_i$ is a descent direction for the merit function, which is a requirement for the line search procedure. Note that the above inequality holds even when $\\mu$ is computed without the Hessian part. However usage of the second order information results in better $\\mu$ selection and larger steps.\nSummary of the algorithm for $\\mu$ selection If the current $\\mu$ already satisfies the inequality it must not be changed. This simple algorithm follows:\nStart with $\\mu_0 = 1$ On each iteration set $\\mu_i = \\max\\left(\\mu_{i - 1}, \\dfrac{q_i(p_i) - q(0)}{(1 - \\rho) \\lVert c_i \\rVert_1} \\right)$ Summary of the line search algorithm To find $\\alpha_i$ which satisfies the sufficient decrease condition of the merit function the following algorithm is used:\nSet $\\alpha_i = 1$ Compute the directional derivatives $D_i \\coloneqq D(\\phi(x_i; \\mu); p_i)$ While $\\phi(x_i + \\alpha_i p_i; \\mu) \u003e \\phi(x_i; \\mu) + \\eta \\alpha_i D_i$ set $\\alpha_i \\leftarrow \\tau \\alpha_i$ for some $0 \u003c \\tau \u003c 1$ In the end the estimate is updated as $$ x_{i + 1} = x_i + \\alpha_i p_i $$\nNumerical values of the algorithm constants The following reasonable values might be used: $$ \\eta = 0.5 \\\\ \\rho = 0.5 \\\\ \\tau = 0.5 $$\nConstructing the linearized subproblem First introduce convenient notation. Let\n$\\hat{X}_k, \\hat{W}_k$ be current estimates of $X_k$ and $W_k$ $x_k, w_k$ be correction to the current estimates The linearization relation is defined as: $$ X_k = \\hat{X}_k + x_k \\\\ W_k = \\hat{W}_k + w_k $$\nThe linearized version of the measurement residual: $$ h(X_k) - Z_k = h(\\hat{X}_k + x_k) - Z_k \\approx h(\\hat{X}_k) + H_k x_k - Z_k = H_k x_k - z_k \\\\ \\text{with } H_k = \\left.\\frac{\\partial h(X)}{\\partial X}\\right\\vert_{\\hat{X}_k} \\text{ and } z_k = Z_k - h(\\hat{X}_k) $$\nThe linearized version of the time propagation equation: $$ X_{k + 1} = f_k(X_k, W_k) \\\\ \\hat{X}_{k + 1} + x_{k + 1} = f_k(\\hat{X}_k + x_k, \\hat{W}_k + w_k) \\approx f_k(\\hat{X}_k, \\hat{W}_k) + F_k x_k + G_k w_k \\\\ x_{k + 1} = F_k x_k + G_k w_k + u_k \\\\ \\text{with } F_k = \\left.\\frac{\\partial f(X, W)}{\\partial X}\\right\\vert_{\\hat{X}_k, \\hat{W}_k} , G_k = \\left.\\frac{\\partial f(X, W)}{\\partial W}\\right\\vert_{\\hat{X}_k, \\hat{W}_k} , u_k = f_k(\\hat{X}_k, \\hat{W}_k) - \\hat{X}_{k + 1} $$\nSummary of the linearized subproblem The linearized cost function $$ \\begin{split} J(x, w) =\u0026 \\frac{1}{2} (x_0 - x_0^-)^T P_0^{-1} (x_0 - x_0^-) \\\\ +\u0026 \\frac{1}{2} \\sum_{k = 0}^{N - 1} (H_k x_k - z_k)^T R_k^{-1} (H_k x_k - z_k) \\\\ +\u0026 \\frac{1}{2} \\sum_{k = 0}^{N - 1} (w_k - w_k^-)^T Q_k^{-1} (w_k - w_k^-) \\end{split} $$ is minimized subject to constraints: $$ x_{k + 1} = F_k x_k + G_k w_k + u_k $$\nWhere all matrices and vectors have the following values: $$ x_0^- = X_0^- - \\hat{X}_0 \\\\ z_k = Z_k - h(\\hat{X}_k)\\\\ H_k = \\left.\\frac{\\partial h(X)}{\\partial X}\\right\\vert_{\\hat{X}_k} \\\\ w_k^- = -\\hat{W}_k \\\\ F_k = \\left.\\frac{\\partial f(X, W)}{\\partial X}\\right\\vert_{\\hat{X}_k, \\hat{W}_k} \\\\ G_k = \\left.\\frac{\\partial f(X, W)}{\\partial W}\\right\\vert_{\\hat{X}_k, \\hat{W}_k} \\\\ u_k = f_k(\\hat{X}_k, \\hat{W}_k) - \\hat{X}_{k + 1} $$\nSolution $x_k$ and $w_k$ to it is found using the linear smoother algorithm summarized here. Then the estimates are updated as: $$ \\hat{X}_k \\leftarrow \\hat{X}_k + \\alpha x_k \\\\ \\hat{W}_k \\leftarrow \\hat{W}_k + \\alpha w_k $$ Where $0 \u003c \\alpha \\leq 1$ is selected as described in the previous section.\nInitialization Numerical optimization methods are sensitive to the initial guess – they search a local minimum in its vicinity. In our problem we can get a good initial guess on $X_k$ by estimating them with an Extended Kalman Filter. For the noises $W_k$ a natural guess is just zero.\nTermination The iterations terminate if two criteria are satisfied\nThe relative reduction of the cost functions is less than $t_f$: $$E(\\hat{X}^{i}, \\hat{W}^{i}) \u003c (1 + t_f) E(\\hat{X}^{i - 1}, \\hat{W}^{i - 1}),$$ where superscript denotes iteration index Relative violation of the time transition equations is less than $t_c$ for each epoch $k$: $$\\left|\\hat{X}_{k + 1} - f_k(\\hat{X}_k, \\hat{W}_k) \\right| \u003c t_c \\max\\left(\\left|\\hat{X}_{k + 1}\\right|, 1 \\right),$$ where operations and inequalities are applied elementwise Parameters $t_f$ and $t_c$ are passed to the optimization subroutine.\nError covariance estimation Error covariance estimates for $X_k$ and $W_k$ are taken from the linear smoother output from the last iteration.\nConclusion I’ve presented a nonlinear batch estimation algorithm which solves a nonlinear optimization problem with a help of linear Kalman smoother for iteration subproblems. The algorithm has solid theoretical foundations and gives reliable and easy to interpret estimates when converged.\nReferences J. Nocedal, S. J. Wright «Numerical Optimization, 2nd edition\u003e ","wordCount":"1704","inLanguage":"en","datePublished":"2022-11-30T00:00:00Z","dateModified":"2022-11-30T00:00:00Z","author":{"@type":"Person","name":"Nikolay Mayorov"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://nmayorov.github.io/posts/nonlinear_batch_estimation/"},"publisher":{"@type":"Organization","name":"My Blog","logo":{"@type":"ImageObject","url":"https://nmayorov.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://nmayorov.github.io accesskey=h title="My Blog (Alt + H)">My Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://nmayorov.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://nmayorov.github.io/tags title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://nmayorov.github.io>Home</a>&nbsp;»&nbsp;<a href=https://nmayorov.github.io/posts/>Posts</a></div><h1 class=post-title>Nonlinear batch estimation</h1><div class=post-meta><span title='2022-11-30 00:00:00 +0000 UTC'>November 30, 2022</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;Nikolay Mayorov</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#model-description aria-label="Model description">Model description</a></li><li><a href=#optimization-problem aria-label="Optimization problem">Optimization problem</a></li><li><a href=#outline-of-the-solution-method aria-label="Outline of the solution method">Outline of the solution method</a></li><li><a href=#merit-function-and-line-search aria-label="Merit function and line search">Merit function and line search</a><ul><li><a href=#summary-of-the-algorithm-for-mu-selection aria-label="Summary of the algorithm for $\mu$ selection">Summary of the algorithm for $\mu$ selection</a></li><li><a href=#summary-of-the-line-search-algorithm aria-label="Summary of the line search algorithm">Summary of the line search algorithm</a></li><li><a href=#numerical-values-of-the-algorithm-constants aria-label="Numerical values of the algorithm constants">Numerical values of the algorithm constants</a></li></ul></li><li><a href=#constructing-the-linearized-subproblem aria-label="Constructing the linearized subproblem">Constructing the linearized subproblem</a><ul><li><a href=#summary-of-the-linearized-subproblem aria-label="Summary of the linearized subproblem">Summary of the linearized subproblem</a></li></ul></li><li><a href=#initialization aria-label=Initialization>Initialization</a></li><li><a href=#termination aria-label=Termination>Termination</a></li><li><a href=#error-covariance-estimation aria-label="Error covariance estimation">Error covariance estimation</a></li><li><a href=#conclusion aria-label=Conclusion>Conclusion</a></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><p>In this note I present a development of nonlinear batch estimation algorithm.</p><h1 id=model-description>Model description<a hidden class=anchor aria-hidden=true href=#model-description>#</a></h1><p>The state estimation problem in a nonlinear system is considered.
The formulations is analogous to the <a href=https://nmayorov.github.io/posts/rts_as_optimization/#problem-formulation>linear case</a>, but with nonlinear transition and measurement equations.
We use uppercase letters to denote variables participating in the nonlinear model.
Time transition and measurement equations for which are
$$
Z_k = f_k(X_k) + V_k \\
X_{k+1} = h_k(X_k, W_k)
$$
All the other assumptions remain the same.
The task is to estimate $X_k$ for epochs $k = 0, 1, \ldots, N$.
This will be done by solving an optimization problem.</p><h1 id=optimization-problem>Optimization problem<a hidden class=anchor aria-hidden=true href=#optimization-problem>#</a></h1><p>Similarly to the linear case the cost function is defined as follows
$$
\begin{split}
E(X, W) =& \frac{1}{2} (X_0 - X_0^-)^T P_0^- (X_0 - X_0^-) \\
+& \frac{1}{2} \sum_{k = 0}^{N-1} (h_k(X_k) - Z_k)^T R_k^{-1} (h_k(X_k) - Z_k) \\
+& \frac{1}{2} \sum_{k = 0}^{N-1} W_k^T Q_k^{-1} W_k
\end{split}
$$
It must be optimized taking into account the time transition equations:
$$
\min_{X, W} \space E(X, W) \text{ subject to} \space X_{k + 1} = f_k(X_k, W_k)
$$
It can be classified as a nonlinear least-squares problem with nonlinear equality constraints on the variables.</p><h1 id=outline-of-the-solution-method>Outline of the solution method<a hidden class=anchor aria-hidden=true href=#outline-of-the-solution-method>#</a></h1><p>Consider a general optimization problem of the form:
$$
\min_x f(x) \text{ subject to } c(x) = 0
$$
One established method of solving it called sequential quadratic programming (SQP) goes like this:</p><ol><li>At current estimate $x_i$ form a local quadratic approximation of the Lagrange function for the problem and a linear approximation of the constraints</li><li>Solve this quadratic programming task to obtain step $p_k$ to the next estimate</li><li>Form the next estimate $x_{i+1} = x_i + \alpha_i p_i$</li><li>Repeat until convergence</li></ol><p>To form the mentioned local quadratic approximation, the gradient (gradient of $f$ might be used with the equivalent result) and the Hessian of the Lagrange function needs to be formed.
The method we apply differs in how the Hessian approximation is formed and might be called a Gauss-Newton approximation to the SQP method.</p><p>In a nonlinear least-squares problem the cost function has the form
$$
f(x) = \frac{1}{2} r(x)^T r(x)
$$
The Jacobian matrix of $r(x)$ is defined as
$$
J(x) = \begin{bmatrix} \nabla r_1(x) & \nabla r_2(x) & \ldots & \nabla r_n(x) \end{bmatrix}^T
$$
Then the gradient and Gauss-Newton approximation of the Hessian are:
$$
\nabla f(x) = J^T r(x) \\
\nabla^2 f(x) \approx J(x)^T J(x)
$$
Opposed to the SQP method we also neglect the second derivatives of the constraints, i. e. form a quadratic model for the cost function, not for the Lagrangian.</p><p>With these approximations the quadratic subproblem is
$$
\min_{p_i} (r_i + J_i p_i)^T (r_i + J_i p_i) \text{ subject to } c_i + A_i p_i = 0 \\
\text{with } r_i = r(x_i), J_i = J(x_i), c_i = c(x_i), A_i = \nabla c(x) \vert_{x_i}
$$
Logic of the algorithm iterations remain the same.</p><p>It must be said that the proposed method is not widely known.
However my intuition and limited experience tells me that it is a legit approach similar to Gauss-Newton method of solving unconstrained nonlinear least squares.</p><p>For our specific problem the linear subproblem formulated above is solved by a Kalman smoother.</p><h1 id=merit-function-and-line-search>Merit function and line search<a hidden class=anchor aria-hidden=true href=#merit-function-and-line-search>#</a></h1><p>To guarantee convergence of a nonlinear optimization method a step $p$ obtained from a linear subproblem must be applied with a properly selected multiplier as $\alpha p$.
In constrained optimization there are 2 objectives: minimize the cost function and satisfy the constraints.
One approach to account for both ot them is to consider a merit function ([1], section 11.2) with $\mu > 0$:
$$
\phi(x; \mu) = f(x) + \mu \lVert c(x) \rVert_1
$$
The parameter $\alpha$ is selected to achieve a sufficient decrease of $\phi(x; \mu)$ (the Armijo condition) for some $0 &lt; \eta &lt; 1$:
$$
\phi(x_i + \alpha_i p_i; \mu) \leq \phi(x_i; \mu) + \eta \alpha_i D(\phi(x_i; \mu); p_i)
$$
Where $D(g(x); l)$ is a directional derivative of $g(x)$ in direction $l$:
$$
D(g(x); l) \coloneqq \lim_{\epsilon \rightarrow 0} \frac{g(x + \epsilon l) - g(x)}{\epsilon}
$$
The derivative of the introduced merit function in the direction $p_i$ (which satisfies linearized constraints!) can be shown to be ([1], section 18.3)
$$
D(\phi(x_i; \mu); p_i) = \nabla f_i^T p_i - \mu \lVert c_i \rVert_1
$$
In order to $p_i$ be the descent direction for the merit function the directional derivative must be negative.
This can be achieved by selecting large enough $\mu$.
Informally it means that if necessary the optimizer must take steps which don&rsquo;t reduce the cost function, but only constraint violation.
The specific strategy of selecting $\mu$ suggested in [1] (section 18.3) is explained next.</p><p>Using the quadratic model $q(p)$ of $f(x)$ and linear model of $c(x)$ near $x_i$ it is possible to estimate the change of the merit function for step $p_i$:
$$
\phi(x_i + p_i; \mu) - \phi(x_i) \approx q_i(p_i) + \mu \lVert c_i + A_i p_i \rVert_1 - q_i(0) - \mu \lVert c_i \rVert_1 = q_i(p_i) - q_i(0) - \mu \lVert c_i \rVert_1
$$
The aim is to have this change sufficiently negative
$$
q_i(p_i) - q_i(0) - \mu \lVert c_i \rVert_1 \leq -\rho \mu \lVert c_i \rVert_1
$$
for some $0 &lt; \rho &lt; 1$. From this the inequality for $\mu$ follows
$$
\mu \geq \frac{q_i(p_i) - q_i(0)}{(1 - \rho) \lVert c_i \rVert_1} = \frac{\nabla f_i^T p_i + (1/2) p_i^T H_i p_i}{(1 - \rho) \lVert c_i \rVert_1}
$$
Where $H_i$ is the Hessian of the Lagrange function or its approximation evaluated at $x_i$.</p><p>If $\mu$ satisfies the above inequality and $H_i$ is positive semidefinite we have for the directional derivative
$$
D(\phi(x_i; \mu); p_i) \leq -\rho \mu \lVert c_i \rVert_1 &lt; 0
$$
Meaning that $p_i$ is a descent direction for the merit function, which is a requirement for the line search procedure.
Note that the above inequality holds even when $\mu$ is computed without the Hessian part.
However usage of the second order information results in better $\mu$ selection and larger steps.</p><h2 id=summary-of-the-algorithm-for-mu-selection>Summary of the algorithm for $\mu$ selection<a hidden class=anchor aria-hidden=true href=#summary-of-the-algorithm-for-mu-selection>#</a></h2><p>If the current $\mu$ already satisfies the inequality it must not be changed.
This simple algorithm follows:</p><ol><li>Start with $\mu_0 = 1$</li><li>On each iteration set $\mu_i = \max\left(\mu_{i - 1}, \dfrac{q_i(p_i) - q(0)}{(1 - \rho) \lVert c_i \rVert_1} \right)$</li></ol><h2 id=summary-of-the-line-search-algorithm>Summary of the line search algorithm<a hidden class=anchor aria-hidden=true href=#summary-of-the-line-search-algorithm>#</a></h2><p>To find $\alpha_i$ which satisfies the sufficient decrease condition of the merit function the following algorithm is used:</p><ol><li>Set $\alpha_i = 1$</li><li>Compute the directional derivatives $D_i \coloneqq D(\phi(x_i; \mu); p_i)$</li><li>While $\phi(x_i + \alpha_i p_i; \mu) > \phi(x_i; \mu) + \eta \alpha_i D_i$ set $\alpha_i \leftarrow \tau \alpha_i$ for some $0 &lt; \tau &lt; 1$</li></ol><p>In the end the estimate is updated as
$$
x_{i + 1} = x_i + \alpha_i p_i
$$</p><h2 id=numerical-values-of-the-algorithm-constants>Numerical values of the algorithm constants<a hidden class=anchor aria-hidden=true href=#numerical-values-of-the-algorithm-constants>#</a></h2><p>The following reasonable values might be used:
$$
\eta = 0.5 \\
\rho = 0.5 \\
\tau = 0.5
$$</p><h1 id=constructing-the-linearized-subproblem>Constructing the linearized subproblem<a hidden class=anchor aria-hidden=true href=#constructing-the-linearized-subproblem>#</a></h1><p>First introduce convenient notation. Let</p><ul><li>$\hat{X}_k, \hat{W}_k$ be current estimates of $X_k$ and $W_k$</li><li>$x_k, w_k$ be correction to the current estimates</li></ul><p>The linearization relation is defined as:
$$
X_k = \hat{X}_k + x_k \\
W_k = \hat{W}_k + w_k
$$</p><p>The linearized version of the measurement residual:
$$
h(X_k) - Z_k = h(\hat{X}_k + x_k) - Z_k \approx h(\hat{X}_k) + H_k x_k - Z_k = H_k x_k - z_k \\
\text{with } H_k = \left.\frac{\partial h(X)}{\partial X}\right\vert_{\hat{X}_k} \text{ and } z_k = Z_k - h(\hat{X}_k)
$$</p><p>The linearized version of the time propagation equation:
$$
X_{k + 1} = f_k(X_k, W_k) \\
\hat{X}_{k + 1} + x_{k + 1} = f_k(\hat{X}_k + x_k, \hat{W}_k + w_k) \approx f_k(\hat{X}_k, \hat{W}_k) + F_k x_k + G_k w_k \\
x_{k + 1} = F_k x_k + G_k w_k + u_k \\
\text{with } F_k = \left.\frac{\partial f(X, W)}{\partial X}\right\vert_{\hat{X}_k, \hat{W}_k}
, G_k = \left.\frac{\partial f(X, W)}{\partial W}\right\vert_{\hat{X}_k, \hat{W}_k}
, u_k = f_k(\hat{X}_k, \hat{W}_k) - \hat{X}_{k + 1}
$$</p><h2 id=summary-of-the-linearized-subproblem>Summary of the linearized subproblem<a hidden class=anchor aria-hidden=true href=#summary-of-the-linearized-subproblem>#</a></h2><p>The linearized cost function
$$
\begin{split}
J(x, w) =& \frac{1}{2} (x_0 - x_0^-)^T P_0^{-1} (x_0 - x_0^-) \\
+& \frac{1}{2} \sum_{k = 0}^{N - 1} (H_k x_k - z_k)^T R_k^{-1} (H_k x_k - z_k) \\
+& \frac{1}{2} \sum_{k = 0}^{N - 1} (w_k - w_k^-)^T Q_k^{-1} (w_k - w_k^-)
\end{split}
$$
is minimized subject to constraints:
$$
x_{k + 1} = F_k x_k + G_k w_k + u_k
$$</p><p>Where all matrices and vectors have the following values:
$$
x_0^- = X_0^- - \hat{X}_0 \\
z_k = Z_k - h(\hat{X}_k)\\
H_k = \left.\frac{\partial h(X)}{\partial X}\right\vert_{\hat{X}_k} \\
w_k^- = -\hat{W}_k \\
F_k = \left.\frac{\partial f(X, W)}{\partial X}\right\vert_{\hat{X}_k, \hat{W}_k} \\
G_k = \left.\frac{\partial f(X, W)}{\partial W}\right\vert_{\hat{X}_k, \hat{W}_k} \\
u_k = f_k(\hat{X}_k, \hat{W}_k) - \hat{X}_{k + 1}
$$</p><p>Solution $x_k$ and $w_k$ to it is found using the linear smoother algorithm summarized <a href=https://nmayorov.github.io/posts/rts_as_optimization/#summary-of-the-algorithm>here</a>.
Then the estimates are updated as:
$$
\hat{X}_k \leftarrow \hat{X}_k + \alpha x_k \\
\hat{W}_k \leftarrow \hat{W}_k + \alpha w_k
$$
Where $0 &lt; \alpha \leq 1$ is selected as described in the previous section.</p><h1 id=initialization>Initialization<a hidden class=anchor aria-hidden=true href=#initialization>#</a></h1><p>Numerical optimization methods are sensitive to the initial guess &ndash; they search a local minimum in its vicinity.
In our problem we can get a good initial guess on $X_k$ by estimating them with an Extended Kalman Filter.
For the noises $W_k$ a natural guess is just zero.</p><h1 id=termination>Termination<a hidden class=anchor aria-hidden=true href=#termination>#</a></h1><p>The iterations terminate if two criteria are satisfied</p><ol><li>The relative reduction of the cost functions is less than $t_f$:
$$E(\hat{X}^{i}, \hat{W}^{i}) &lt; (1 + t_f) E(\hat{X}^{i - 1}, \hat{W}^{i - 1}),$$
where superscript denotes iteration index</li><li>Relative violation of the time transition equations is less than $t_c$ for each epoch $k$:
$$\left|\hat{X}_{k + 1} - f_k(\hat{X}_k, \hat{W}_k) \right| &lt; t_c \max\left(\left|\hat{X}_{k + 1}\right|, 1 \right),$$
where operations and inequalities are applied elementwise</li></ol><p>Parameters $t_f$ and $t_c$ are passed to the optimization subroutine.</p><h1 id=error-covariance-estimation>Error covariance estimation<a hidden class=anchor aria-hidden=true href=#error-covariance-estimation>#</a></h1><p>Error covariance estimates for $X_k$ and $W_k$ are taken from the linear smoother output from the last iteration.</p><h1 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h1><p>I&rsquo;ve presented a nonlinear batch estimation algorithm which solves a nonlinear optimization problem with a help of linear Kalman smoother for iteration subproblems.
The algorithm has solid theoretical foundations and gives reliable and easy to interpret estimates when converged.</p><h1 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h1><ol><li>J. Nocedal, S. J. Wright &#171;Numerical Optimization, 2nd edition></li></ol></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://nmayorov.github.io>My Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>
<!doctype html><html lang=en><head><title>Robust Extended Kalman Filter update. Part 1: basic theory Â· My Blog</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=description content="When viewed as an optimization problem the Extended Kalman Filter update step can be generalized to incorporate a robust loss function in order to protect against outlier measurements. In this note the initial theoretical development is given.
Introducing a robust loss function Link to heading In the standard EKF update cost function the measurement residuals appear squared, which is known to be not robust against outliers. We can alleviate this problem by wrapping the measurement residual part with a loss function $\rho$ as follows: $$ E(X) = \frac{1}{2} (X - X^-)^T (P^-)^{-1} (X - X^-) + \frac{1}{2} \rho\left( (h(X) - Z)^T R^{-1} (h(X) - Z) \right) $$ The function $\rho$ must be approximately linear for small arguments (to achieve optimality of Kalman update) and sublinear for large arguments (to be robust agains outlier measurements)."><meta name=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="Robust Extended Kalman Filter update. Part 1: basic theory"><meta name=twitter:description content="When viewed as an optimization problem the Extended Kalman Filter update step can be generalized to incorporate a robust loss function in order to protect against outlier measurements. In this note the initial theoretical development is given.
Introducing a robust loss function Link to heading In the standard EKF update cost function the measurement residuals appear squared, which is known to be not robust against outliers. We can alleviate this problem by wrapping the measurement residual part with a loss function $\rho$ as follows: $$ E(X) = \frac{1}{2} (X - X^-)^T (P^-)^{-1} (X - X^-) + \frac{1}{2} \rho\left( (h(X) - Z)^T R^{-1} (h(X) - Z) \right) $$ The function $\rho$ must be approximately linear for small arguments (to achieve optimality of Kalman update) and sublinear for large arguments (to be robust agains outlier measurements)."><meta property="og:title" content="Robust Extended Kalman Filter update. Part 1: basic theory"><meta property="og:description" content="When viewed as an optimization problem the Extended Kalman Filter update step can be generalized to incorporate a robust loss function in order to protect against outlier measurements. In this note the initial theoretical development is given.
Introducing a robust loss function Link to heading In the standard EKF update cost function the measurement residuals appear squared, which is known to be not robust against outliers. We can alleviate this problem by wrapping the measurement residual part with a loss function $\rho$ as follows: $$ E(X) = \frac{1}{2} (X - X^-)^T (P^-)^{-1} (X - X^-) + \frac{1}{2} \rho\left( (h(X) - Z)^T R^{-1} (h(X) - Z) \right) $$ The function $\rho$ must be approximately linear for small arguments (to achieve optimality of Kalman update) and sublinear for large arguments (to be robust agains outlier measurements)."><meta property="og:type" content="article"><meta property="og:url" content="https://nmayorov.github.io/posts/robust_ekf/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-07-13T00:00:00+00:00"><meta property="article:modified_time" content="2023-07-13T00:00:00+00:00"><link rel=canonical href=https://nmayorov.github.io/posts/robust_ekf/><link rel=preload href="/fonts/forkawesome-webfont.woff2?v=1.2.0" as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.c4d7e93a158eda5a65b3df343745d2092a0a1e2170feeec909b8a89443903c6a.css integrity="sha256-xNfpOhWO2lpls980N0XSCSoKHiFw/u7JCbiolEOQPGo=" crossorigin=anonymous media=screen><link rel=icon type=image/png href=/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/images/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5><meta name=generator content="Hugo 0.115.3"></head><body class="preload-transitions colorscheme-light"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=/>My Blog</a>
<input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/posts/>Posts</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://nmayorov.github.io/posts/robust_ekf/>Robust Extended Kalman Filter update. Part 1: basic theory</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa fa-calendar" aria-hidden=true></i>
<time datetime=2023-07-13T00:00:00Z>July 13, 2023</time></span>
<span class=reading-time><i class="fa fa-clock-o" aria-hidden=true></i>
7-minute read</span></div></div></header><div><p>When viewed as an <a href=https://nmayorov.github.io/posts/ekf_update_optimization/>optimization problem</a> the Extended Kalman Filter update step can be generalized to incorporate a robust loss function in order to protect against outlier measurements.
In this note the initial theoretical development is given.</p><h1 id=introducing-a-robust-loss-function>Introducing a robust loss function
<a class=heading-link href=#introducing-a-robust-loss-function><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>In the standard EKF update cost function the measurement residuals appear squared, which is known to be not robust against outliers.
We can alleviate this problem by wrapping the measurement residual part with a loss function $\rho$ as follows:
$$
E(X) = \frac{1}{2} (X - X^-)^T (P^-)^{-1} (X - X^-) + \frac{1}{2} \rho\left( (h(X) - Z)^T R^{-1} (h(X) - Z) \right)
$$
The function $\rho$ must be approximately linear for small arguments (to achieve optimality of Kalman update) and sublinear for large arguments (to be robust agains outlier measurements).
Let&rsquo;s state its properties formally:</p><ul><li>$\rho(u) = u + O(u^2)$</li><li>$\rho(u) \leq u$</li><li>$\rho^\prime(u) > 0$</li><li>$\rho^{\prime\prime}(u) \leq 0$</li></ul><p>Let&rsquo;s consider 3 possible function as an example:</p><ol><li>Trivial or linear cost function corresponding to the standard formulation &ndash; $\rho(u) = u$.</li><li>&ldquo;Soft-L1&rdquo; loss function &ndash; $\rho(u) = 2 (\sqrt{1 + u} - 1)$.
For large $u = z^2$ we get $\rho(z^2) \approx |z|$ which corresponds to the robust L1-regression</li><li>Cauchy loss function &ndash; $\rho(u) = \log(1 + u)$.
Such function penalties outliers significantly more heavily than the previous one</li></ol><p>These are depicted below:</p><p><a href=figs/loss_functions.svg><img src=figs/loss_functions.svg alt=loss_functions></a></p><h1 id=adjusting-loss-function-by-the-number-of-measurements>Adjusting loss function by the number of measurements
<a class=heading-link href=#adjusting-loss-function-by-the-number-of-measurements><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>There is an important subtlety that depending on the number of measurements the loss function must be adjusted.
With $m$ measurements, the random variable $(h(X) - Z)^T R^{-1} (h(X) - Z)$ (assuming $X$ to be fixed at the optimum) has a theoretical chi-squared distribution with $m$ degrees of freedom (assuming noise to be normally distributed).
Thus it tends to be larger with more measurements and a single universal loss function is not applicable.</p><p>To tackle this issue we should consider a scaled loss function of the form
$$
\rho_s(u) = s \rho\left(\dfrac{u}{s}\right)
$$
For $m = 1$ the soft bound between inliers and outliers is 1 (1-$\sigma$ of the standard normal distribution).
For an arbitrary $m$ we require that the probability of the scaled $\chi^2_m$ variable to fall within 1 is the same:
$$
P\left(\dfrac{\chi^2_m}{s(m)} &lt; 1 \right) = P(\chi^2_1 &lt; 1) \\
P\left(\chi^2_m &lt; s(m) \right) = P(\chi^2(1) &lt; 1) \\
$$
From this we can compute the required scaling as
$$
s(m) = \mathrm{ICDF}_{\chi^2_m}(\mathrm{CDF}_{\chi^2_1}(1))
$$
Where $\mathrm{CDF}$ and $\mathrm{ICDF}$ stands for cumulative distribution function and its inverse respectively.</p><p>Here is the list of approximate values of $s(m)$ for small $m$:
$$
s(1) = 1; s(2) = 2.3; s(3) = 3.5; s(4) = 4.7; s(5) = 5.9
$$</p><h1 id=applying-loss-function-to-individual-measurements>Applying loss function to individual measurements
<a class=heading-link href=#applying-loss-function-to-individual-measurements><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>If the measurements are independent (possibly after the decorrelation procedure) we can formulate another form of the cost function:
$$
E(X) = \frac{1}{2} (X - X^-)^T (P^-)^{-1} (X - X^-) + \frac{1}{2} \sum_j \rho \left( \left( \dfrac{f_j(X) - Z_j}{\sigma_j} \right)^2 \right)
$$</p><p>Whether the measurements should be treated independently depends on an application.
For example, GNSS position fixes are tend to be corrupted as a whole, thus such measurements should be put under the loss function as a unit.
On the other hand, GNSS pseudo-ranges should be treated separately because they are more or less independent and rejecting all measurements is wasteflul and unreasonable.</p><h1 id=handling-the-loss-function-in-the-optimization-process>Handling the loss function in the optimization process
<a class=heading-link href=#handling-the-loss-function-in-the-optimization-process><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>Here we consider a more general case with block measurements with an arbitrary $R$:
$$
E(X) = \frac{1}{2} (X - X^-)^T (P^-)^{-1} (X - X^-) + \frac{1}{2} \rho\left( (h(X) - Z)^T R^{-1} (h(X) - Z) \right)
$$</p><p>The optimization is done iteratively by building quadratic function models around the current estimate.
To do that we compute the gradient and Hessian of $E$ evaluated at $X$:
$$
\nabla E(X) = -(P^-)^{-1} x^- - \rho^\prime H^T R^{-1} z \\
\nabla^2 E(X) = (P^-)^{-1} + H^T (\rho^\prime R^{-1} + 2 \rho^{\prime \prime} R^{-1} z z^T R^{-1}) H - \rho^\prime \sum_j (R^{-1} z)_j \nabla^2 h_j
$$
Here the following helper notation has been used:
$$
x^- = X^- - X \\
z = Z - f(X) \\
H = \left.\dfrac{\partial h}{\partial X}\right\vert_X
$$</p><p>We apply the Gauss-Newton approximation and neglect the term with the second derivatives of $h$ in the Hessian:
$$
\nabla^2 E(X) \approx (P^-)^{-1} + H^T (\rho^\prime R^{-1} + 2 \rho^{\prime \prime} R^{-1} z z^T R^{-1}) H
$$
The quadratic subproblem to determine step $x$ is then:
$$
\min_x q(x) = \frac{1}{2} x^T \nabla^2 E(X) x + x^T \nabla E(X) + \mathrm{const}
$$
We find the minimum by setting the gradient to zero:
$$
\nabla^2 E(X) x + \nabla E(X) = 0
$$
Substituting expressions for $\nabla E(X)$ and $\nabla^2 E(X)$ we get the equation:
$$
\left( (P^-)^{-1} + H^T (\rho^\prime R^{-1} + 2 \rho^{\prime \prime} R^{-1} z z^T R^{-1}) H \right) x = (P^-)^{-1} x^- + \rho^\prime H^T R^{-1} z
$$</p><p>For $\rho(u) = u$ it is simplified to the standard linear Kalman update step:
$$
\left( (P^-)^{-1} + H^T R^{-1} H \right) x = (P^-)^{-1} x^- + H^T R^{-1} z
$$
Now we want to achieve the same form for an arbitrary loss function $\rho(u)$.</p><h2 id=transformation-to-the-standard-kalman-form>Transformation to the standard Kalman form
<a class=heading-link href=#transformation-to-the-standard-kalman-form><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>For simplicity let&rsquo;s assume that $R = I$, which can be achieved by transforming $Z$ and $f$ by Cholesky factors of $R$ (shown at the end).
Consider the block:
$$
H^T (\rho^\prime I + 2 \rho^{\prime \prime} z z^T ) H = \rho^\prime H^T \left(I + \frac{2 \rho^{\prime \prime}}{\rho^\prime} z z^T \right) H
$$
Note that $\rho^\prime > 0$ and the division is valid.</p><p>We want to write it as $H_S^T H_S$ with $H_S = S H$.
The matrix $S$ must satisfy then
$$
S^T S = \rho^\prime \left(I + \frac{2 \rho^{\prime \prime}}{\rho^\prime} z z^T \right)
$$
We search it in the following form:
$$
S = \sqrt{\rho^\prime} \left(I - \alpha \dfrac{z z^T}{\lVert z \rVert^2} \right)
$$
And by substitution get the equation for $\alpha$:
$$
-\dfrac{2 \alpha}{\lVert z \rVert^2} + \dfrac{\alpha^2}{\lVert z \rVert^2} = \dfrac{2 \rho^{\prime \prime}}{\rho^\prime} \\ [2pt]
\alpha^2 - 2 \alpha - \frac{2 \rho^{\prime \prime} \lVert z \rVert^2}{\rho^\prime} = 0
$$
A numerically stable way to find the smallest root of this equation is as follows:
$$
\alpha = - \dfrac{2 \rho^{\prime \prime} \lVert z \rVert^2 / \rho^\prime}{1 + \sqrt{1 + 2 \rho^{\prime \prime} \lVert z \rVert^2 / \rho^\prime}}
$$
To avoid taking the square root of a negative number and bound $\alpha$ below 1 (to prevent scaling by zero) we use a guarded formula with a small value of $\epsilon$:
$$
\beta = \max \left(2 \rho^{\prime \prime} \lVert z \rVert^2 / \rho^\prime, -1 + \epsilon^2 \right) \\
\alpha = -\frac{\beta}{1 + \sqrt{1 + \beta}}
$$
Then $\alpha$ will satisfy
$$
0 \leq \alpha \leq 1 - \epsilon
$$</p><p>Next we need to scale the residual vector $z$ as $z_s = \gamma z$ to satisfy:
$$
H_s^T z_s = \rho^\prime H^T z \\
\gamma H^T S z = \rho^\prime H^T z
$$
From which we can find
$$
\gamma = \frac{\sqrt{\rho^\prime}}{1 - \alpha}
$$</p><h2 id=summary-of-the-result>Summary of the result
<a class=heading-link href=#summary-of-the-result><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>We came up with the following result. The equation
$$
\left( (P^-)^{-1} + H^T (\rho^\prime R^{-1} + 2 \rho^{\prime \prime} R^{-1} z z^T R^{-1}) H \right) x = (P^-)^{-1} x^- + \rho^\prime H^T R^{-1} z
$$
can be written as
$$
\left( (P^-)^{-1} + H_s^T H_s \right) x = (P^-)^{-1} x^- + H_s^T z_s
$$
Where the scaled observation matrix and the residual vector are computed as
$$
H_s = \sqrt{\rho^\prime} \left(I - \alpha \dfrac{z_n z_n^T}{\lVert z_n \rVert^2} \right) H_n \\
z_s = \frac{\sqrt{\rho^\prime}}{1 - \alpha} z_n \\ [2pt]
H_n = L^{-1} H \\
z_n = L^{-1} z
$$
Where $L$ is a Cholesky factor of $R$:
$$
R = L L^T
$$
And the scalar $\alpha$ is computed by the algorithm:
$$
\beta = \max \left(2 \rho^{\prime \prime} \lVert z_n \rVert^2 / \rho^\prime, -1 + \epsilon^2 \right) \\
\alpha = -\frac{\beta}{1 + \sqrt{1 + \beta}}
$$</p><p>The equation in the transformed form corresponds to the linear Kalman correction formula.</p><h1 id=processing-independent-measurements>Processing independent measurements
<a class=heading-link href=#processing-independent-measurements><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>When measurements are independent, then the scaling is done for each component of $z$ and each row of $H$ independently.
The scaling formulas remain the same, only works with scalars.</p><h1 id=algorithm-outline>Algorithm outline
<a class=heading-link href=#algorithm-outline><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>The robust Extended Kalman update step as an optimization procedure follows the same logic as described in the <a href=https://nmayorov.github.io/posts/ekf_update_optimization/>previous post</a>.
But on each iterations the residual vector $z$ and the measurement matrix $H$ are scaled to account for a robust loss function as described above.
Because the optimization problem became more difficult and differs significantly from the standard EKF update, the iterations must be done with line search step control until convergence.</p></div><footer></footer></article><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script></section></div><footer class=footer><section class=container>Â©
2023
Â·
Powered by <a href=https://gohugo.io/>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/>Coder</a>.</section></footer></main><script src=/js/coder.min.236049395dc3682fb2719640872958e12f1f24067bb09c327b233e6290c7edac.js integrity="sha256-I2BJOV3DaC+ycZZAhylY4S8fJAZ7sJwyeyM+YpDH7aw="></script></body></html>
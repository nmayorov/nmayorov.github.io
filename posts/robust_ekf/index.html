<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Robust Extended Kalman Filter update. Part 1: basic theory | Navigating Uncertainty</title><meta name=keywords content><meta name=description content="When viewed as an optimization problem the Extended Kalman Filter update step can be generalized to incorporate a robust loss function in order to protect against outlier measurements. In this note the initial theoretical development is given.
Introducing a robust loss function In the standard EKF update cost function the measurement residuals appear squared, which is known to be not robust against outliers. We can alleviate this problem by wrapping the measurement residual part with a loss function $\rho$ as follows: $$ E(X) = \frac{1}{2} (X - X^-)^T (P^-)^{-1} (X - X^-) + \frac{1}{2} \rho\left( (h(X) - Z)^T R^{-1} (h(X) - Z) \right) $$ The function $\rho$ must be approximately linear for small arguments (to achieve optimality of Kalman update) and sublinear for large arguments (to be robust agains outlier measurements)."><meta name=author content="Nikolay Mayorov"><link rel=canonical href=https://nmayorov.github.io/posts/robust_ekf/><link crossorigin=anonymous href=/assets/css/stylesheet.21e7558a953920031f5c62b4db91512093af342ec251fdf4bc1be327b0e259b4.css integrity="sha256-IedVipU5IAMfXGK025FRIJOvNC7CUf30vBvjJ7DiWbQ=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://nmayorov.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://nmayorov.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://nmayorov.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://nmayorov.github.io/apple-touch-icon.png><link rel=mask-icon href=https://nmayorov.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script><meta property="og:title" content="Robust Extended Kalman Filter update. Part 1: basic theory"><meta property="og:description" content="When viewed as an optimization problem the Extended Kalman Filter update step can be generalized to incorporate a robust loss function in order to protect against outlier measurements. In this note the initial theoretical development is given.
Introducing a robust loss function In the standard EKF update cost function the measurement residuals appear squared, which is known to be not robust against outliers. We can alleviate this problem by wrapping the measurement residual part with a loss function $\rho$ as follows: $$ E(X) = \frac{1}{2} (X - X^-)^T (P^-)^{-1} (X - X^-) + \frac{1}{2} \rho\left( (h(X) - Z)^T R^{-1} (h(X) - Z) \right) $$ The function $\rho$ must be approximately linear for small arguments (to achieve optimality of Kalman update) and sublinear for large arguments (to be robust agains outlier measurements)."><meta property="og:type" content="article"><meta property="og:url" content="https://nmayorov.github.io/posts/robust_ekf/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-07-13T00:00:00+00:00"><meta property="article:modified_time" content="2023-07-13T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Robust Extended Kalman Filter update. Part 1: basic theory"><meta name=twitter:description content="When viewed as an optimization problem the Extended Kalman Filter update step can be generalized to incorporate a robust loss function in order to protect against outlier measurements. In this note the initial theoretical development is given.
Introducing a robust loss function In the standard EKF update cost function the measurement residuals appear squared, which is known to be not robust against outliers. We can alleviate this problem by wrapping the measurement residual part with a loss function $\rho$ as follows: $$ E(X) = \frac{1}{2} (X - X^-)^T (P^-)^{-1} (X - X^-) + \frac{1}{2} \rho\left( (h(X) - Z)^T R^{-1} (h(X) - Z) \right) $$ The function $\rho$ must be approximately linear for small arguments (to achieve optimality of Kalman update) and sublinear for large arguments (to be robust agains outlier measurements)."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://nmayorov.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Robust Extended Kalman Filter update. Part 1: basic theory","item":"https://nmayorov.github.io/posts/robust_ekf/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Robust Extended Kalman Filter update. Part 1: basic theory","name":"Robust Extended Kalman Filter update. Part 1: basic theory","description":"When viewed as an optimization problem the Extended Kalman Filter update step can be generalized to incorporate a robust loss function in order to protect against outlier measurements. In this note the initial theoretical development is given.\nIntroducing a robust loss function In the standard EKF update cost function the measurement residuals appear squared, which is known to be not robust against outliers. We can alleviate this problem by wrapping the measurement residual part with a loss function $\\rho$ as follows: $$ E(X) = \\frac{1}{2} (X - X^-)^T (P^-)^{-1} (X - X^-) + \\frac{1}{2} \\rho\\left( (h(X) - Z)^T R^{-1} (h(X) - Z) \\right) $$ The function $\\rho$ must be approximately linear for small arguments (to achieve optimality of Kalman update) and sublinear for large arguments (to be robust agains outlier measurements).","keywords":[],"articleBody":"When viewed as an optimization problem the Extended Kalman Filter update step can be generalized to incorporate a robust loss function in order to protect against outlier measurements. In this note the initial theoretical development is given.\nIntroducing a robust loss function In the standard EKF update cost function the measurement residuals appear squared, which is known to be not robust against outliers. We can alleviate this problem by wrapping the measurement residual part with a loss function $\\rho$ as follows: $$ E(X) = \\frac{1}{2} (X - X^-)^T (P^-)^{-1} (X - X^-) + \\frac{1}{2} \\rho\\left( (h(X) - Z)^T R^{-1} (h(X) - Z) \\right) $$ The function $\\rho$ must be approximately linear for small arguments (to achieve optimality of Kalman update) and sublinear for large arguments (to be robust agains outlier measurements). Let’s state its properties formally:\n$\\rho(u) = u + O(u^2)$ $\\rho(u) \\leq u$ $\\rho^\\prime(u) \u003e 0$ $\\rho^{\\prime\\prime}(u) \\leq 0$ Let’s consider 3 possible function as an example:\nTrivial or linear cost function corresponding to the standard formulation – $\\rho(u) = u$. “Soft-L1” loss function – $\\rho(u) = 2 (\\sqrt{1 + u} - 1)$. For large $u = z^2$ we get $\\rho(z^2) \\approx |z|$ which corresponds to the robust L1-regression Cauchy loss function – $\\rho(u) = \\log(1 + u)$. Such function penalties outliers significantly more heavily than the previous one These are depicted below:\nAdjusting loss function by the number of measurements There is an important subtlety that depending on the number of measurements the loss function must be adjusted. With $m$ measurements, the random variable $(h(X) - Z)^T R^{-1} (h(X) - Z)$ (assuming $X$ to be fixed at the optimum) has a theoretical chi-squared distribution with $m$ degrees of freedom (assuming noise to be normally distributed). Thus it tends to be larger with more measurements and a single universal loss function is not applicable.\nTo tackle this issue we should consider a scaled loss function of the form $$ \\rho_s(u) = s \\rho\\left(\\dfrac{u}{s}\\right) $$ For $m = 1$ the soft bound between inliers and outliers is 1 (1-$\\sigma$ of the standard normal distribution). For an arbitrary $m$ we require that the probability of the scaled $\\chi^2_m$ variable to fall within 1 is the same: $$ P\\left(\\dfrac{\\chi^2_m}{s(m)} \u003c 1 \\right) = P(\\chi^2_1 \u003c 1) \\\\ P\\left(\\chi^2_m \u003c s(m) \\right) = P(\\chi^2(1) \u003c 1) \\\\ $$ From this we can compute the required scaling as $$ s(m) = \\mathrm{ICDF}_{\\chi^2_m}(\\mathrm{CDF}_{\\chi^2_1}(1)) $$ Where $\\mathrm{CDF}$ and $\\mathrm{ICDF}$ stands for cumulative distribution function and its inverse respectively.\nHere is the list of approximate values of $s(m)$ for small $m$: $$ s(1) = 1; s(2) = 2.3; s(3) = 3.5; s(4) = 4.7; s(5) = 5.9 $$\nApplying loss function to individual measurements If the measurements are independent (possibly after the decorrelation procedure) we can formulate another form of the cost function: $$ E(X) = \\frac{1}{2} (X - X^-)^T (P^-)^{-1} (X - X^-) + \\frac{1}{2} \\sum_j \\rho \\left( \\left( \\dfrac{f_j(X) - Z_j}{\\sigma_j} \\right)^2 \\right) $$\nWhether the measurements should be treated independently depends on an application. For example, GNSS position fixes are tend to be corrupted as a whole, thus such measurements should be put under the loss function as a unit. On the other hand, GNSS pseudo-ranges should be treated separately because they are more or less independent and rejecting all measurements is wasteflul and unreasonable.\nHandling the loss function in the optimization process Here we consider a more general case with block measurements with an arbitrary $R$: $$ E(X) = \\frac{1}{2} (X - X^-)^T (P^-)^{-1} (X - X^-) + \\frac{1}{2} \\rho\\left( (h(X) - Z)^T R^{-1} (h(X) - Z) \\right) $$\nThe optimization is done iteratively by building quadratic function models around the current estimate. To do that we compute the gradient and Hessian of $E$ evaluated at $X$: $$ \\nabla E(X) = -(P^-)^{-1} x^- - \\rho^\\prime H^T R^{-1} z \\\\ \\nabla^2 E(X) = (P^-)^{-1} + H^T (\\rho^\\prime R^{-1} + 2 \\rho^{\\prime \\prime} R^{-1} z z^T R^{-1}) H - \\rho^\\prime \\sum_j (R^{-1} z)_j \\nabla^2 h_j $$ Here the following helper notation has been used: $$ x^- = X^- - X \\\\ z = Z - f(X) \\\\ H = \\left.\\dfrac{\\partial h}{\\partial X}\\right\\vert_X $$\nWe apply the Gauss-Newton approximation and neglect the term with the second derivatives of $h$ in the Hessian: $$ \\nabla^2 E(X) \\approx (P^-)^{-1} + H^T (\\rho^\\prime R^{-1} + 2 \\rho^{\\prime \\prime} R^{-1} z z^T R^{-1}) H $$ The quadratic subproblem to determine step $x$ is then: $$ \\min_x q(x) = \\frac{1}{2} x^T \\nabla^2 E(X) x + x^T \\nabla E(X) + \\mathrm{const} $$ We find the minimum by setting the gradient to zero: $$ \\nabla^2 E(X) x + \\nabla E(X) = 0 $$ Substituting expressions for $\\nabla E(X)$ and $\\nabla^2 E(X)$ we get the equation: $$ \\left( (P^-)^{-1} + H^T (\\rho^\\prime R^{-1} + 2 \\rho^{\\prime \\prime} R^{-1} z z^T R^{-1}) H \\right) x = (P^-)^{-1} x^- + \\rho^\\prime H^T R^{-1} z $$\nFor $\\rho(u) = u$ it is simplified to the standard linear Kalman update step: $$ \\left( (P^-)^{-1} + H^T R^{-1} H \\right) x = (P^-)^{-1} x^- + H^T R^{-1} z $$ Now we want to achieve the same form for an arbitrary loss function $\\rho(u)$.\nTransformation to the standard Kalman form For simplicity let’s assume that $R = I$, which can be achieved by transforming $Z$ and $f$ by Cholesky factors of $R$ (shown at the end). Consider the block: $$ H^T (\\rho^\\prime I + 2 \\rho^{\\prime \\prime} z z^T ) H = \\rho^\\prime H^T \\left(I + \\frac{2 \\rho^{\\prime \\prime}}{\\rho^\\prime} z z^T \\right) H $$ Note that $\\rho^\\prime \u003e 0$ and the division is valid.\nWe want to write it as $H_S^T H_S$ with $H_S = S H$. The matrix $S$ must satisfy then $$ S^T S = \\rho^\\prime \\left(I + \\frac{2 \\rho^{\\prime \\prime}}{\\rho^\\prime} z z^T \\right) $$ We search it in the following form: $$ S = \\sqrt{\\rho^\\prime} \\left(I - \\alpha \\dfrac{z z^T}{\\lVert z \\rVert^2} \\right) $$ And by substitution get the equation for $\\alpha$: $$ -\\dfrac{2 \\alpha}{\\lVert z \\rVert^2} + \\dfrac{\\alpha^2}{\\lVert z \\rVert^2} = \\dfrac{2 \\rho^{\\prime \\prime}}{\\rho^\\prime} \\\\ [2pt] \\alpha^2 - 2 \\alpha - \\frac{2 \\rho^{\\prime \\prime} \\lVert z \\rVert^2}{\\rho^\\prime} = 0 $$ A numerically stable way to find the smallest root of this equation is as follows: $$ \\alpha = - \\dfrac{2 \\rho^{\\prime \\prime} \\lVert z \\rVert^2 / \\rho^\\prime}{1 + \\sqrt{1 + 2 \\rho^{\\prime \\prime} \\lVert z \\rVert^2 / \\rho^\\prime}} $$ To avoid taking the square root of a negative number and bound $\\alpha$ below 1 (to prevent scaling by zero) we use a guarded formula with a small value of $\\epsilon$: $$ \\beta = \\max \\left(2 \\rho^{\\prime \\prime} \\lVert z \\rVert^2 / \\rho^\\prime, -1 + \\epsilon^2 \\right) \\\\ \\alpha = -\\frac{\\beta}{1 + \\sqrt{1 + \\beta}} $$ Then $\\alpha$ will satisfy $$ 0 \\leq \\alpha \\leq 1 - \\epsilon $$\nNext we need to scale the residual vector $z$ as $z_s = \\gamma z$ to satisfy: $$ H_s^T z_s = \\rho^\\prime H^T z \\\\ \\gamma H^T S z = \\rho^\\prime H^T z $$ From which we can find $$ \\gamma = \\frac{\\sqrt{\\rho^\\prime}}{1 - \\alpha} $$\nSummary of the result We came up with the following result. The equation $$ \\left( (P^-)^{-1} + H^T (\\rho^\\prime R^{-1} + 2 \\rho^{\\prime \\prime} R^{-1} z z^T R^{-1}) H \\right) x = (P^-)^{-1} x^- + \\rho^\\prime H^T R^{-1} z $$ can be written as $$ \\left( (P^-)^{-1} + H_s^T H_s \\right) x = (P^-)^{-1} x^- + H_s^T z_s $$ Where the scaled observation matrix and the residual vector are computed as $$ H_s = \\sqrt{\\rho^\\prime} \\left(I - \\alpha \\dfrac{z_n z_n^T}{\\lVert z_n \\rVert^2} \\right) H_n \\\\ z_s = \\frac{\\sqrt{\\rho^\\prime}}{1 - \\alpha} z_n \\\\ [2pt] H_n = L^{-1} H \\\\ z_n = L^{-1} z $$ Where $L$ is a Cholesky factor of $R$: $$ R = L L^T $$ And the scalar $\\alpha$ is computed by the algorithm: $$ \\beta = \\max \\left(2 \\rho^{\\prime \\prime} \\lVert z_n \\rVert^2 / \\rho^\\prime, -1 + \\epsilon^2 \\right) \\\\ \\alpha = -\\frac{\\beta}{1 + \\sqrt{1 + \\beta}} $$\nThe equation in the transformed form corresponds to the linear Kalman correction formula.\nProcessing independent measurements When measurements are independent, then the scaling is done for each component of $z$ and each row of $H$ independently. The scaling formulas remain the same, only works with scalars.\nAlgorithm outline The robust Extended Kalman update step as an optimization procedure follows the same logic as described in the previous post. But on each iterations the residual vector $z$ and the measurement matrix $H$ are scaled to account for a robust loss function as described above. Because the optimization problem became more difficult and differs significantly from the standard EKF update, the iterations must be done with line search step control until convergence.\n","wordCount":"1438","inLanguage":"en","datePublished":"2023-07-13T00:00:00Z","dateModified":"2023-07-13T00:00:00Z","author":{"@type":"Person","name":"Nikolay Mayorov"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://nmayorov.github.io/posts/robust_ekf/"},"publisher":{"@type":"Organization","name":"Navigating Uncertainty","logo":{"@type":"ImageObject","url":"https://nmayorov.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://nmayorov.github.io accesskey=h title="Navigating Uncertainty (Alt + H)">Navigating Uncertainty</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://nmayorov.github.io/archives title="All posts"><span>All posts</span></a></li><li><a href=https://nmayorov.github.io/tags title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://nmayorov.github.io>Home</a>&nbsp;»&nbsp;<a href=https://nmayorov.github.io/posts/>Posts</a></div><h1 class=post-title>Robust Extended Kalman Filter update. Part 1: basic theory</h1><div class=post-meta><span title='2023-07-13 00:00:00 +0000 UTC'>July 13, 2023</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;Nikolay Mayorov</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#introducing-a-robust-loss-function aria-label="Introducing a robust loss function">Introducing a robust loss function</a></li><li><a href=#adjusting-loss-function-by-the-number-of-measurements aria-label="Adjusting loss function by the number of measurements">Adjusting loss function by the number of measurements</a></li><li><a href=#applying-loss-function-to-individual-measurements aria-label="Applying loss function to individual measurements">Applying loss function to individual measurements</a></li><li><a href=#handling-the-loss-function-in-the-optimization-process aria-label="Handling the loss function in the optimization process">Handling the loss function in the optimization process</a><ul><li><a href=#transformation-to-the-standard-kalman-form aria-label="Transformation to the standard Kalman form">Transformation to the standard Kalman form</a></li><li><a href=#summary-of-the-result aria-label="Summary of the result">Summary of the result</a></li></ul></li><li><a href=#processing-independent-measurements aria-label="Processing independent measurements">Processing independent measurements</a></li><li><a href=#algorithm-outline aria-label="Algorithm outline">Algorithm outline</a></li></ul></div></details></div><div class=post-content><p>When viewed as an <a href=https://nmayorov.github.io/posts/ekf_update_optimization/>optimization problem</a> the Extended Kalman Filter update step can be generalized to incorporate a robust loss function in order to protect against outlier measurements.
In this note the initial theoretical development is given.</p><h1 id=introducing-a-robust-loss-function>Introducing a robust loss function<a hidden class=anchor aria-hidden=true href=#introducing-a-robust-loss-function>#</a></h1><p>In the standard EKF update cost function the measurement residuals appear squared, which is known to be not robust against outliers.
We can alleviate this problem by wrapping the measurement residual part with a loss function $\rho$ as follows:
$$
E(X) = \frac{1}{2} (X - X^-)^T (P^-)^{-1} (X - X^-) + \frac{1}{2} \rho\left( (h(X) - Z)^T R^{-1} (h(X) - Z) \right)
$$
The function $\rho$ must be approximately linear for small arguments (to achieve optimality of Kalman update) and sublinear for large arguments (to be robust agains outlier measurements).
Let&rsquo;s state its properties formally:</p><ul><li>$\rho(u) = u + O(u^2)$</li><li>$\rho(u) \leq u$</li><li>$\rho^\prime(u) > 0$</li><li>$\rho^{\prime\prime}(u) \leq 0$</li></ul><p>Let&rsquo;s consider 3 possible function as an example:</p><ol><li>Trivial or linear cost function corresponding to the standard formulation &ndash; $\rho(u) = u$.</li><li>&ldquo;Soft-L1&rdquo; loss function &ndash; $\rho(u) = 2 (\sqrt{1 + u} - 1)$.
For large $u = z^2$ we get $\rho(z^2) \approx |z|$ which corresponds to the robust L1-regression</li><li>Cauchy loss function &ndash; $\rho(u) = \log(1 + u)$.
Such function penalties outliers significantly more heavily than the previous one</li></ol><p>These are depicted below:</p><p><a href=figs/loss_functions.svg><img loading=lazy src=figs/loss_functions.svg alt=loss_functions></a></p><h1 id=adjusting-loss-function-by-the-number-of-measurements>Adjusting loss function by the number of measurements<a hidden class=anchor aria-hidden=true href=#adjusting-loss-function-by-the-number-of-measurements>#</a></h1><p>There is an important subtlety that depending on the number of measurements the loss function must be adjusted.
With $m$ measurements, the random variable $(h(X) - Z)^T R^{-1} (h(X) - Z)$ (assuming $X$ to be fixed at the optimum) has a theoretical chi-squared distribution with $m$ degrees of freedom (assuming noise to be normally distributed).
Thus it tends to be larger with more measurements and a single universal loss function is not applicable.</p><p>To tackle this issue we should consider a scaled loss function of the form
$$
\rho_s(u) = s \rho\left(\dfrac{u}{s}\right)
$$
For $m = 1$ the soft bound between inliers and outliers is 1 (1-$\sigma$ of the standard normal distribution).
For an arbitrary $m$ we require that the probability of the scaled $\chi^2_m$ variable to fall within 1 is the same:
$$
P\left(\dfrac{\chi^2_m}{s(m)} &lt; 1 \right) = P(\chi^2_1 &lt; 1) \\
P\left(\chi^2_m &lt; s(m) \right) = P(\chi^2(1) &lt; 1) \\
$$
From this we can compute the required scaling as
$$
s(m) = \mathrm{ICDF}_{\chi^2_m}(\mathrm{CDF}_{\chi^2_1}(1))
$$
Where $\mathrm{CDF}$ and $\mathrm{ICDF}$ stands for cumulative distribution function and its inverse respectively.</p><p>Here is the list of approximate values of $s(m)$ for small $m$:
$$
s(1) = 1; s(2) = 2.3; s(3) = 3.5; s(4) = 4.7; s(5) = 5.9
$$</p><h1 id=applying-loss-function-to-individual-measurements>Applying loss function to individual measurements<a hidden class=anchor aria-hidden=true href=#applying-loss-function-to-individual-measurements>#</a></h1><p>If the measurements are independent (possibly after the decorrelation procedure) we can formulate another form of the cost function:
$$
E(X) = \frac{1}{2} (X - X^-)^T (P^-)^{-1} (X - X^-) + \frac{1}{2} \sum_j \rho \left( \left( \dfrac{f_j(X) - Z_j}{\sigma_j} \right)^2 \right)
$$</p><p>Whether the measurements should be treated independently depends on an application.
For example, GNSS position fixes are tend to be corrupted as a whole, thus such measurements should be put under the loss function as a unit.
On the other hand, GNSS pseudo-ranges should be treated separately because they are more or less independent and rejecting all measurements is wasteflul and unreasonable.</p><h1 id=handling-the-loss-function-in-the-optimization-process>Handling the loss function in the optimization process<a hidden class=anchor aria-hidden=true href=#handling-the-loss-function-in-the-optimization-process>#</a></h1><p>Here we consider a more general case with block measurements with an arbitrary $R$:
$$
E(X) = \frac{1}{2} (X - X^-)^T (P^-)^{-1} (X - X^-) + \frac{1}{2} \rho\left( (h(X) - Z)^T R^{-1} (h(X) - Z) \right)
$$</p><p>The optimization is done iteratively by building quadratic function models around the current estimate.
To do that we compute the gradient and Hessian of $E$ evaluated at $X$:
$$
\nabla E(X) = -(P^-)^{-1} x^- - \rho^\prime H^T R^{-1} z \\
\nabla^2 E(X) = (P^-)^{-1} + H^T (\rho^\prime R^{-1} + 2 \rho^{\prime \prime} R^{-1} z z^T R^{-1}) H - \rho^\prime \sum_j (R^{-1} z)_j \nabla^2 h_j
$$
Here the following helper notation has been used:
$$
x^- = X^- - X \\
z = Z - f(X) \\
H = \left.\dfrac{\partial h}{\partial X}\right\vert_X
$$</p><p>We apply the Gauss-Newton approximation and neglect the term with the second derivatives of $h$ in the Hessian:
$$
\nabla^2 E(X) \approx (P^-)^{-1} + H^T (\rho^\prime R^{-1} + 2 \rho^{\prime \prime} R^{-1} z z^T R^{-1}) H
$$
The quadratic subproblem to determine step $x$ is then:
$$
\min_x q(x) = \frac{1}{2} x^T \nabla^2 E(X) x + x^T \nabla E(X) + \mathrm{const}
$$
We find the minimum by setting the gradient to zero:
$$
\nabla^2 E(X) x + \nabla E(X) = 0
$$
Substituting expressions for $\nabla E(X)$ and $\nabla^2 E(X)$ we get the equation:
$$
\left( (P^-)^{-1} + H^T (\rho^\prime R^{-1} + 2 \rho^{\prime \prime} R^{-1} z z^T R^{-1}) H \right) x = (P^-)^{-1} x^- + \rho^\prime H^T R^{-1} z
$$</p><p>For $\rho(u) = u$ it is simplified to the standard linear Kalman update step:
$$
\left( (P^-)^{-1} + H^T R^{-1} H \right) x = (P^-)^{-1} x^- + H^T R^{-1} z
$$
Now we want to achieve the same form for an arbitrary loss function $\rho(u)$.</p><h2 id=transformation-to-the-standard-kalman-form>Transformation to the standard Kalman form<a hidden class=anchor aria-hidden=true href=#transformation-to-the-standard-kalman-form>#</a></h2><p>For simplicity let&rsquo;s assume that $R = I$, which can be achieved by transforming $Z$ and $f$ by Cholesky factors of $R$ (shown at the end).
Consider the block:
$$
H^T (\rho^\prime I + 2 \rho^{\prime \prime} z z^T ) H = \rho^\prime H^T \left(I + \frac{2 \rho^{\prime \prime}}{\rho^\prime} z z^T \right) H
$$
Note that $\rho^\prime > 0$ and the division is valid.</p><p>We want to write it as $H_S^T H_S$ with $H_S = S H$.
The matrix $S$ must satisfy then
$$
S^T S = \rho^\prime \left(I + \frac{2 \rho^{\prime \prime}}{\rho^\prime} z z^T \right)
$$
We search it in the following form:
$$
S = \sqrt{\rho^\prime} \left(I - \alpha \dfrac{z z^T}{\lVert z \rVert^2} \right)
$$
And by substitution get the equation for $\alpha$:
$$
-\dfrac{2 \alpha}{\lVert z \rVert^2} + \dfrac{\alpha^2}{\lVert z \rVert^2} = \dfrac{2 \rho^{\prime \prime}}{\rho^\prime} \\ [2pt]
\alpha^2 - 2 \alpha - \frac{2 \rho^{\prime \prime} \lVert z \rVert^2}{\rho^\prime} = 0
$$
A numerically stable way to find the smallest root of this equation is as follows:
$$
\alpha = - \dfrac{2 \rho^{\prime \prime} \lVert z \rVert^2 / \rho^\prime}{1 + \sqrt{1 + 2 \rho^{\prime \prime} \lVert z \rVert^2 / \rho^\prime}}
$$
To avoid taking the square root of a negative number and bound $\alpha$ below 1 (to prevent scaling by zero) we use a guarded formula with a small value of $\epsilon$:
$$
\beta = \max \left(2 \rho^{\prime \prime} \lVert z \rVert^2 / \rho^\prime, -1 + \epsilon^2 \right) \\
\alpha = -\frac{\beta}{1 + \sqrt{1 + \beta}}
$$
Then $\alpha$ will satisfy
$$
0 \leq \alpha \leq 1 - \epsilon
$$</p><p>Next we need to scale the residual vector $z$ as $z_s = \gamma z$ to satisfy:
$$
H_s^T z_s = \rho^\prime H^T z \\
\gamma H^T S z = \rho^\prime H^T z
$$
From which we can find
$$
\gamma = \frac{\sqrt{\rho^\prime}}{1 - \alpha}
$$</p><h2 id=summary-of-the-result>Summary of the result<a hidden class=anchor aria-hidden=true href=#summary-of-the-result>#</a></h2><p>We came up with the following result. The equation
$$
\left( (P^-)^{-1} + H^T (\rho^\prime R^{-1} + 2 \rho^{\prime \prime} R^{-1} z z^T R^{-1}) H \right) x = (P^-)^{-1} x^- + \rho^\prime H^T R^{-1} z
$$
can be written as
$$
\left( (P^-)^{-1} + H_s^T H_s \right) x = (P^-)^{-1} x^- + H_s^T z_s
$$
Where the scaled observation matrix and the residual vector are computed as
$$
H_s = \sqrt{\rho^\prime} \left(I - \alpha \dfrac{z_n z_n^T}{\lVert z_n \rVert^2} \right) H_n \\
z_s = \frac{\sqrt{\rho^\prime}}{1 - \alpha} z_n \\ [2pt]
H_n = L^{-1} H \\
z_n = L^{-1} z
$$
Where $L$ is a Cholesky factor of $R$:
$$
R = L L^T
$$
And the scalar $\alpha$ is computed by the algorithm:
$$
\beta = \max \left(2 \rho^{\prime \prime} \lVert z_n \rVert^2 / \rho^\prime, -1 + \epsilon^2 \right) \\
\alpha = -\frac{\beta}{1 + \sqrt{1 + \beta}}
$$</p><p>The equation in the transformed form corresponds to the linear Kalman correction formula.</p><h1 id=processing-independent-measurements>Processing independent measurements<a hidden class=anchor aria-hidden=true href=#processing-independent-measurements>#</a></h1><p>When measurements are independent, then the scaling is done for each component of $z$ and each row of $H$ independently.
The scaling formulas remain the same, only works with scalars.</p><h1 id=algorithm-outline>Algorithm outline<a hidden class=anchor aria-hidden=true href=#algorithm-outline>#</a></h1><p>The robust Extended Kalman update step as an optimization procedure follows the same logic as described in the <a href=https://nmayorov.github.io/posts/ekf_update_optimization/>previous post</a>.
But on each iterations the residual vector $z$ and the measurement matrix $H$ are scaled to account for a robust loss function as described above.
Because the optimization problem became more difficult and differs significantly from the standard EKF update, the iterations must be done with line search step control until convergence.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://nmayorov.github.io>Navigating Uncertainty</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>
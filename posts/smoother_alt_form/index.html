<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Alternative form of Kalman smoother | Navigating Uncertainty</title>
<meta name=keywords content><meta name=description content="In this post (recommended to read beforehand) I&rsquo;ve shown a derivation of the Kalman smoother as a solution to an optimization problem. The resulting formulas are surprisingly elegant, however their applicability depends on the assumption that apriori filter covariance matrices $P_k^-$ are positive definite and invertible. This assumption might be limiting in practical problems and thus another form of the Kalman smoother is derived here.
Motivating example A singular covariance matrix may arise in the following practical scenario."><meta name=author content="Nikolay Mayorov"><link rel=canonical href=https://nmayorov.github.io/posts/smoother_alt_form/><link crossorigin=anonymous href=/assets/css/stylesheet.a801c217e3c96db9c9df36cb1d2216a9bc5d9fa77b72e2afcdf47fb847f8f015.css integrity="sha256-qAHCF+PJbbnJ3zbLHSIWqbxdn6d7cuKvzfR/uEf48BU=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://nmayorov.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://nmayorov.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://nmayorov.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://nmayorov.github.io/apple-touch-icon.png><link rel=mask-icon href=https://nmayorov.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://nmayorov.github.io/posts/smoother_alt_form/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script><meta property="og:title" content="Alternative form of Kalman smoother"><meta property="og:description" content="In this post (recommended to read beforehand) I&rsquo;ve shown a derivation of the Kalman smoother as a solution to an optimization problem. The resulting formulas are surprisingly elegant, however their applicability depends on the assumption that apriori filter covariance matrices $P_k^-$ are positive definite and invertible. This assumption might be limiting in practical problems and thus another form of the Kalman smoother is derived here.
Motivating example A singular covariance matrix may arise in the following practical scenario."><meta property="og:type" content="article"><meta property="og:url" content="https://nmayorov.github.io/posts/smoother_alt_form/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-01-05T00:00:00+00:00"><meta property="article:modified_time" content="2023-01-05T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Alternative form of Kalman smoother"><meta name=twitter:description content="In this post (recommended to read beforehand) I&rsquo;ve shown a derivation of the Kalman smoother as a solution to an optimization problem. The resulting formulas are surprisingly elegant, however their applicability depends on the assumption that apriori filter covariance matrices $P_k^-$ are positive definite and invertible. This assumption might be limiting in practical problems and thus another form of the Kalman smoother is derived here.
Motivating example A singular covariance matrix may arise in the following practical scenario."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://nmayorov.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Alternative form of Kalman smoother","item":"https://nmayorov.github.io/posts/smoother_alt_form/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Alternative form of Kalman smoother","name":"Alternative form of Kalman smoother","description":"In this post (recommended to read beforehand) I\u0026rsquo;ve shown a derivation of the Kalman smoother as a solution to an optimization problem. The resulting formulas are surprisingly elegant, however their applicability depends on the assumption that apriori filter covariance matrices $P_k^-$ are positive definite and invertible. This assumption might be limiting in practical problems and thus another form of the Kalman smoother is derived here.\nMotivating example A singular covariance matrix may arise in the following practical scenario.","keywords":[],"articleBody":"In this post (recommended to read beforehand) I’ve shown a derivation of the Kalman smoother as a solution to an optimization problem. The resulting formulas are surprisingly elegant, however their applicability depends on the assumption that apriori filter covariance matrices $P_k^-$ are positive definite and invertible. This assumption might be limiting in practical problems and thus another form of the Kalman smoother is derived here.\nMotivating example A singular covariance matrix may arise in the following practical scenario. Imagine that we want to process measurements which relate states at the current and previous epochs – $x_k$ and $x_{k-1}$. A possible example might be processing of distance increments from an odometeter in a navigation algorithm. Such measurements are not directly supported by Kalman filter or smoother algorithms. However we can cast them into the Kalman framework by considering an augmented vector $$ x^a_{k + 1} = \\begin{bmatrix} x_{k + 1} \\\\ x_k \\end{bmatrix} $$ The approach is also known as «stochastic cloning».\nNow consider a time transition from $x_k$ to $x^a_{k + 1}$: $$ x^a_{k + 1} = F^a_k x_k + G^a_k w_k \\\\ \\text{with } F^a_k = \\begin{bmatrix} F_k \\\\\\ I \\end{bmatrix}, G_k^a = \\begin{bmatrix} G_k \\\\ 0 \\end{bmatrix} $$ For the covariance we then get $$ P^a_{k + 1} = F_k^a P_k (F_k^a)^T + G^a_k Q_k (G^a_k)^T = \\begin{bmatrix} F_k P_k F_k^T + G_k Q_k G_k^T \u0026 F_k P_k \\\\ P_k F_k^T \u0026 P_k \\end{bmatrix} $$ If $G_k Q_k G_k^T$ is singular, then the covariance $P^a_{k + 1}$ will be singular too. For example if there is no noise we get $$ P^a_{k + 1} = \\begin{bmatrix} F_k P_k F_k^T \u0026 F_k P_k \\\\ P_k F_k^T \u0026 P_k \\end{bmatrix} $$ Multiplying the second block row by $F_k$ we get identical block rows, which shows that $P^a_{k + 1}$ is singular.\nAlso even if we don’t use stochastic cloning, the covariance matrix may gradually approach singularity from the perspective of floating point numbers as measurements are processed. In this case we want to avoid computing its inverse, which is another solid motivation to develop an alternative form where inverses are not needed.\nProblem formulation and the boundary value problem Let’s recap the optimization problem.\nWe want to make a small generalization that the initial apriori covariance $P_0^-$ might be singular. In this case we can’t add a term $1/2 (x_0 - x_0^-)^T (P_0^-)^{-1} (x_0 - x_0^-)$ to the cost function. But any covariance matrix can be represented as $$ P_0 = G_{-1} Q_{-1} G_{-1}^T $$ where $Q_{-1}$ is positive definite (subscripts -1 is purely a notational choice). Keeping in mind such factorization we express the initial state $x_0$ as $$ x_0 = x_0^- + G_{-1} w_{-1} $$ where noise $w_{-1}$ is assumed to have covariance $Q_{-1}$.\nThe state estimates are searched by considering the cost function: $$ \\begin{split} J(x, w) =\u0026 \\frac{1}{2} w_{-1}^T Q_{-1}^{-1} w_{-1} \\\\ +\u0026 \\frac{1}{2} \\sum_{k = 0}^{N - 1} (H_k x_k - z_k)^T R_k^{-1} (H_k x_k - z_k) \\\\ +\u0026 \\frac{1}{2} \\sum_{k = 0}^{N - 1} w_k^T Q_k^{-1} w_k \\end{split} $$ Which is minimized subject to the state transition equations: $$ \\min_{x, w} J(x, w) \\text{ subject to } \\\\ x_0 = x_0^- + G_{-1} w_{-1} \\\\ x_{k + 1} = F_k x_k + G_k w_k $$\nThe problem is solved by finding the stationary point of the Lagrange function. It gives the following discrete-time boundary value problem: $$ x_{k + 1} = F_k x_k + G_k Q_k G_k^T \\lambda_{k+1} \\\\ \\lambda_k = F_k^T \\lambda_{k + 1} + H_k^T R_k^{-1} \\left(z_k - H_k x_k\\right) \\\\ P_0^- \\lambda_0 = x_0 - x_0^- \\\\ \\lambda_N = 0 $$ Where $\\lambda_k$ are introduced Lagrange multipliers. From them the noise vectors are computed as $$ w_k = Q_k G_k^T \\lambda_{k+1} \\\\ $$\nNote that our approach to consider the singular initial covariance came down to writing the boundary condition $\\lambda_0 = \\left(P_0^-\\right)^{-1} \\left(x_0 - x_0^-\\right)$ as $P_0^- \\lambda_0 = x_0 - x_0^-$. The rigorous derivation gave the only plausible result. Also note that at this point there is no notion of covariance matrices (besides $P_0$), let alone their positive definitiveness.\nSome matrix identities related to Kalman filtering As a starting point we take the standard Kalman correction formula with the optimal gain matrix $K$: $$ K = P^- H^T (H P^- H^T + R)^{-1} \\\\ P^+ = (I - K H) P^- $$ The formula is applicable for any covariance matrix $P^-$ as long as $R$ is positive definite, which we already assume in our problem.\nThe last equation can be written as $$ P^- - P^+ = K H P^- = P^- H^T K^T $$\nLet’s combine it with the gain equation as follows: $$ K (H P^- H^T + R) = P^- H^T \\\\ K H P^- H^T = P^- H^T - P^+ H^T $$ Subtracting the equations we get the following alternative expression for $K$: $$ K = P^+ H^T R^{-1} $$\nUsing this alternative expression we can also write $$ P^- - P^+ = P^+ H^T R^{-1} H P^- = P^- H^T R^{-1} H P^+ $$\nNow let’s prove the identity: $$ (I + H^T R^{-1} H P^-)^{-1} = I - H^T R^{-1} H P^+ = I - H^T K^T $$ To do that we show that the multiplications on both sides give the identity matrix: $$ \\begin{split} (I + H^T R^{-1} H P^-) (I - H^T R^{-1} H P^+) = I + H^T R^{-1} H (P^- - P^+ - P^- H^T R^{-1} H P^+) = I \\\\ (I - H^T R^{-1} H P^+) (I + H^T R^{-1} H P^-) = I + H^T R^{-1} H (P^- - P^+ - P^+ H^T R^{-1} H P^-) = I \\end{split} $$\nBy taking its transpose we get $$ (I + P^- H^T R^{-1} H)^{-1} = I - P^+ H^T R^{-1} H = I - K H $$\nSimilarly another formula can be proven $$ (R + H P^- H^T)^{-1} = R^{-1} - R^{-1} H P^+ H^T R^{-1} $$\nEquipped with the derived identities we can carry on with solving the boundary-value problem.\nEquivalent problem First we prove the equivalence of the problems: $$ x_{k + 1} = F_k x_k + G_k Q_k G_k^T \\lambda_{k+1} \\\\ \\lambda_k = F_k^T \\lambda_{k + 1} + H_k^T R_k^{-1} \\left(z_k - H_k x_k\\right) \\\\ P_0^- \\lambda_0 = x_0 - x_0^- \\\\ \\lambda_N = 0 \\\\ \\downdownarrows \\upuparrows \\\\ x_k = x_k^- + P_k^- \\lambda_k \\\\ \\lambda_k = F_k^T \\lambda_{k + 1} + H_k^T R_k^{-1} \\left(z_k - H_k x_k\\right) \\\\ \\lambda_N = 0 $$ Meaning that any solution to the first problem is the solution to the second problem and vice versa. Or that the set of solutions are identical.\nFor convenience let’s call $x_k = x_k^- + P_k^- \\lambda_k$ the «magic equation».\nProof from up to down We need to prove that the magic equation follows from the difference equations.\nWe do it by induction. For $k = 0$ it’s true as the boundary condition. Now assume that it’s true for some $k$ and substitute the expression for $\\lambda_k$ into it: $$ x_k = x_k^- + P_k^- F_k^T \\lambda_{k + 1} + P_k^- H_k^T R_k^{-1} z_k - P_k^- H_k^T R_k^{-1} H_k x_k \\\\ (I + P_k^- H_k^T R_k^{-1} H_k) x_k = x_k^- + P_k^- F_k^T \\lambda_{k + 1} + P_k^- H_k^T R_k^{-1} z_k \\\\ $$ Now using the inverse formula for the matrix multiplier on the left we get $$ x_k = (I - K_k H_k) x_k^- + P_k^+ F_k^T \\lambda_{k + 1} + P_k^+ H_k^T R_k^{-1} z_k $$ Here we spot the expression for $K_k$ before $z_k$ and rewrite the equation as $$ x_k = x_k^- + K_k (z_k - H_k x_k^-) + P^+_k F_k^T \\lambda_{k+1} = x_k^+ + P_k^+ F_k^T \\lambda_{k+1} $$ And finally substitute this expression into recursion for $x$: $$ x_{k + 1} = F_k x_k^+ + (F_k^T P_k^+ F_k + G_k Q_k G_k^T) \\lambda_{k + 1} \\\\ x_{k + 1} = x_{k + 1}^- + P_{k + 1}^- \\lambda_{k + 1} $$ Which proves the step of the induction and with that the equation for all $k$.\nProof from down to up We need to prove that the difference equation for $x$ follows from the magic equation and the difference equation for $\\lambda$.\nSubstitute the expression for $\\lambda_k$ into the magic equation to get (derived above already) $$ x_k = x_k^+ + P_k^+ F_k^T \\lambda_{k+1} \\\\ x_k^+ = x_k - P_k^+ F_k^T \\lambda_{k+1} $$ Now substitute $x_k^+$ into the magic equation for $k + 1$: $$ \\begin{split} x_{k + 1} = x_{k + 1}^- + P_{k + 1}^- \\lambda_{k + 1} = F_k x_k^+ + P^-_{k + 1} \\lambda_{k + 1} = F_k x_k + (P_{k + 1}^- - F_k P_k^+ F_k^T) \\lambda_{k + 1} = \\\\ = F_k x_k + G_k Q_k G_k^T \\lambda_{k + 1} \\end{split} $$\nSolution to the problem With the proof from the previous section we can solve the more simple equivalent problem $$ x_k = x_k^- + P_k^- \\lambda_k \\\\ \\lambda_k = F_k^T \\lambda_{k + 1} + H_k^T R_k^{-1} \\left(z_k - H_k x_k\\right) \\\\ \\lambda_N = 0 $$ and be sure to get the solution of the original problem.\nRTS form To derive the RTS form we consider the already derived corollary equation along with the magic equation: $$ x_k = x_k^- + P_k^- \\lambda_k \\\\ x_k = x_k^+ + P_k^+ F_k^T \\lambda_{k+1} $$ If all $P_k^-$ are invertible the difference equation for $\\lambda$ follows from them: $$ x_k^- + P_k^- \\lambda_k = x_k^+ + P_k^+ F_k^T \\lambda_{k+1} \\\\ \\lambda_k = (P_k^-)^{-1} P_k^+ F_k^T \\lambda_{k+1} + (P_k^-)^{-1} (x_k^+ - x_k^-) \\\\ \\lambda_k = F_k^T \\lambda_{k + 1} + H_k^T R_k^{-1} \\left(z_k - H_k x_k\\right) $$ It means the two equations are equivalent to the original problem.\nRewriting them slightly different $$ x_{k + 1} = x_{k + 1}^- + P_k^- \\lambda_{k + 1} \\\\ x_k = x_k^+ + P_k^+ F_k^T \\lambda_{k+1} $$ we can eliminate $\\lambda_{k + 1}$ and come up with the unique solution: $$ x_N^s = x_N^- \\\\ x_k^s = x_k^+ + P_k^+ F_k^T \\left(P_{k + 1}^-\\right)^{-1} \\left(x_{k + 1}^s - x_{k + 1}^-\\right) $$ and $$ \\lambda_k^s = (P_k^-)^{-1} (x_k^s - x_k^-) \\\\ w_k^s = Q_k G_k^T \\lambda_{k+1}^s $$\nForm with explicit recursion for $\\lambda$ If $P_k^-$ is singular then the two equations from the RTS derivation are not equivalent to the original problem. Meaning that there is no unique solution to them and an arbitrary solution is not guaranteed to be the solution of the original problem.\nWe go back to the original equation for $\\lambda_k$ and substitute $x_k = x_k^- + P_k^- \\lambda_k$ into it: $$ \\lambda_k = F_k^T \\lambda_{k+1} + H_k^T R_k^{-1} (z_ k - H_k x_k^- - H_k P_k^- \\lambda_k) \\\\ (I + H_k^T R_k^{-1} H_k P_k^-) \\lambda_k = F_k^T \\lambda_{k+1} + H_k^T R_k^{-1} (z_k - H_k x_k^-) $$ Taking the inverse of the matrix multiplier on the left we get $$ \\lambda_k = (I - K_k^T H_k^T) F_k^T \\lambda_{k + 1} + (I - H_k^T R_k^{-1} H_k P^+_k) H_k^T R_k^{-1} (z_k - H_k x_k^-) \\\\ \\lambda_k = (I - K_k^T H_k^T) F_k^T \\lambda_{k + 1} + H_k^T (H_k P^-_k H_k^T + R_k)^{-1} (z_k - H_k x_k^-) \\\\ $$ Introduce some notation:\n$\\Psi_k = F_k (I - K_k H_k)$ – can be though of as a total matrix for the state propagation, combing filter correction and prediction steps $S_k = H_k P^-_k H_k^T + R_k$ – covariance matrix of the innovation vector $z_k - H_k x_k^-$ Using it the recursion can be written as $$ \\lambda_N^s = 0 \\\\ \\lambda_k^s = \\Psi_k^T \\lambda^s_{k + 1} + H_k^T S_k^{-1} (z_k - H_k x_k^-) $$\nFrom $\\lambda^s_k$ we can compute optimal state and noise estimates as $$ x_k^s = x_k^- + P_k^- \\lambda^s_k = x_k^+ + P_k^+ F_k^T \\lambda_{k + 1}^s \\\\ w_k^s = Q_k G_k^T \\lambda^s_{k + 1} $$\nUniqueness of the solution We see that the solution is unique, because $\\lambda_k^s$ obeys a deterministic recursion process starting from $\\lambda_N^s = 0$. And $x_k^s$ and $w_k^s$ are uniquely determined from $\\lambda_k^s$. This fact doesn’t depend on whether $P_k^-$ is singular or not.\nOne thing that may seem a bit confusing is that when $P_k^-$ is singular the equation $x_k = x_k^- + P_k^- \\lambda_k$ is satisfied for an infinite set of $\\lambda_k$. However $\\lambda_k$ determine the noise vectors $w_k$ which purpose is to make state transition equations consistent. Thus arbitrary adjustment of individual $\\lambda_k$ is not possible, they are determined by the recursion equation uniquely.\nCovariance computation To compute covariance of the estimate errors we recursively update covariance of $\\lambda_k^s$ as: $$ \\Lambda^s_k = \\Psi^T_k \\Lambda^s_{k + 1} \\Psi_k + H_k^T S_k^{-1} H_k \\\\ \\Lambda^s_N = 0 $$ Then using the same reasoning as for the RTS form we get the following expressions for error covariances: $$ P_k^s = P_k^- - P_k^- \\Lambda^s_k P_k^- = P_k^+ - P_k^+ F_k^T \\Lambda^s_{k + 1} F_k P_k^+ \\\\ Q_k^s = Q_k - Q_k G_k^T \\Lambda^s_{k + 1} G_k Q_k $$\nAlgorithmic aspects: multiple or missing measurements In practical algorithms its better to assume that at each epoch there is an arbitrary number of independent measurements (possibly zero). In this regard its better to split $\\lambda_k^s$ and $\\Lambda_k^s$ computation into (backward) «prediction» and «correction» steps similar to Kalman filter.\nThe prediction step: $$ \\lambda_{k}^- = F_k^T \\lambda_{k + 1}^+ \\\\ \\Lambda_{k}^- = F_k^T \\Lambda_{k + 1}^+ F_k $$ The correction step: $$ \\lambda_{k}^+ = (I - K_k H_k)^T \\lambda_{k}^- + H_k^T S_k^{-1} (z_k - H_k x_k^-) \\\\ \\Lambda_{k}^+ = (I - K_k H_k)^T \\Lambda_{k}^- (I - K_k H_k) + H_k^T S_k^{-1} H_k $$ It is repeated for each measurement, applying the formulas to the current estimate. The gain vectors, innovations and their covariances are saved from the filter pass. The crucial point is that the formulas are applied in the reverse to the order in which the measurements were processed in the filter. In the end the result is the «smoothed» estimate $\\lambda_k^s$ which is used to compute $x_k^s$ and $w_k^s$.\nThe correctness of such approach can be understood by considering intermediate states between the measurements.\nConclusion An alternative form of the Kalman smoother which doesn’t assume or require positive definite covariance matrices was derived. This form is somewhat more involved to implement. However for practical algorithms the main requirements are robustness and universality and this alternative algorithm seems to fullfil them better.\n","wordCount":"2383","inLanguage":"en","datePublished":"2023-01-05T00:00:00Z","dateModified":"2023-01-05T00:00:00Z","author":{"@type":"Person","name":"Nikolay Mayorov"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://nmayorov.github.io/posts/smoother_alt_form/"},"publisher":{"@type":"Organization","name":"Navigating Uncertainty","logo":{"@type":"ImageObject","url":"https://nmayorov.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://nmayorov.github.io/ accesskey=h title="Navigating Uncertainty (Alt + H)">Navigating Uncertainty</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://nmayorov.github.io/archives title="All posts"><span>All posts</span></a></li><li><a href=https://nmayorov.github.io/tags title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://nmayorov.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://nmayorov.github.io/posts/>Posts</a></div><h1 class=post-title>Alternative form of Kalman smoother</h1><div class=post-meta><span title='2023-01-05 00:00:00 +0000 UTC'>January 5, 2023</span>&nbsp;·&nbsp;12 min&nbsp;·&nbsp;Nikolay Mayorov</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#motivating-example aria-label="Motivating example">Motivating example</a></li><li><a href=#problem-formulation-and-the-boundary-value-problem aria-label="Problem formulation and the boundary value problem">Problem formulation and the boundary value problem</a></li><li><a href=#some-matrix-identities-related-to-kalman-filtering aria-label="Some matrix identities related to Kalman filtering">Some matrix identities related to Kalman filtering</a></li><li><a href=#equivalent-problem aria-label="Equivalent problem">Equivalent problem</a><ul><li><a href=#proof-from-up-to-down aria-label="Proof from up to down">Proof from up to down</a></li><li><a href=#proof-from-down-to-up aria-label="Proof from down to up">Proof from down to up</a></li></ul></li><li><a href=#solution-to-the-problem aria-label="Solution to the problem">Solution to the problem</a><ul><li><a href=#rts-form aria-label="RTS form">RTS form</a></li><li><a href=#form-with-explicit-recursion-for-lambda aria-label="Form with explicit recursion for $\lambda$">Form with explicit recursion for $\lambda$</a></li><li><a href=#uniqueness-of-the-solution aria-label="Uniqueness of the solution">Uniqueness of the solution</a></li></ul></li><li><a href=#covariance-computation aria-label="Covariance computation">Covariance computation</a></li><li><a href=#algorithmic-aspects-multiple-or-missing-measurements aria-label="Algorithmic aspects: multiple or missing measurements">Algorithmic aspects: multiple or missing measurements</a></li><li><a href=#conclusion aria-label=Conclusion>Conclusion</a></li></ul></div></details></div><div class=post-content><p>In <a href=https://nmayorov.github.io/posts/rts_as_optimization/>this post</a> (recommended to read beforehand) I&rsquo;ve shown a derivation of the Kalman smoother as a solution to an optimization problem.
The resulting formulas are surprisingly elegant, however their applicability depends on the assumption that apriori filter covariance matrices $P_k^-$ are positive definite and invertible.
This assumption might be limiting in practical problems and thus another form of the Kalman smoother is derived here.</p><h1 id=motivating-example>Motivating example<a hidden class=anchor aria-hidden=true href=#motivating-example>#</a></h1><p>A singular covariance matrix may arise in the following practical scenario.
Imagine that we want to process measurements which relate states at the current and previous epochs &ndash; $x_k$ and $x_{k-1}$.
A possible example might be processing of distance increments from an odometeter in a navigation algorithm.
Such measurements are not directly supported by Kalman filter or smoother algorithms.
However we can cast them into the Kalman framework by considering an augmented vector
$$
x^a_{k + 1} = \begin{bmatrix}
x_{k + 1} \\
x_k
\end{bmatrix}
$$
The approach is also known as &#171;stochastic cloning&#187;.</p><p>Now consider a time transition from $x_k$ to $x^a_{k + 1}$:
$$
x^a_{k + 1} = F^a_k x_k + G^a_k w_k \\
\text{with } F^a_k = \begin{bmatrix}
F_k \\\
I
\end{bmatrix}, G_k^a = \begin{bmatrix}
G_k \\
0
\end{bmatrix}
$$
For the covariance we then get
$$
P^a_{k + 1} = F_k^a P_k (F_k^a)^T + G^a_k Q_k (G^a_k)^T = \begin{bmatrix}
F_k P_k F_k^T + G_k Q_k G_k^T & F_k P_k \\
P_k F_k^T & P_k
\end{bmatrix}
$$
If $G_k Q_k G_k^T$ is singular, then the covariance $P^a_{k + 1}$ will be singular too.
For example if there is no noise we get
$$
P^a_{k + 1} = \begin{bmatrix}
F_k P_k F_k^T & F_k P_k \\
P_k F_k^T & P_k
\end{bmatrix}
$$
Multiplying the second block row by $F_k$ we get identical block rows, which shows that $P^a_{k + 1}$ is singular.</p><p>Also even if we don&rsquo;t use stochastic cloning, the covariance matrix may gradually approach singularity from the perspective of floating point numbers as measurements are processed.
In this case we want to avoid computing its inverse, which is another solid motivation to develop an alternative form where inverses are not needed.</p><h1 id=problem-formulation-and-the-boundary-value-problem>Problem formulation and the boundary value problem<a hidden class=anchor aria-hidden=true href=#problem-formulation-and-the-boundary-value-problem>#</a></h1><p>Let&rsquo;s recap the optimization problem.</p><p>We want to make a small generalization that the initial apriori covariance $P_0^-$ might be singular.
In this case we can&rsquo;t add a term $1/2 (x_0 - x_0^-)^T (P_0^-)^{-1} (x_0 - x_0^-)$ to the cost function.
But any covariance matrix can be represented as
$$
P_0 = G_{-1} Q_{-1} G_{-1}^T
$$
where $Q_{-1}$ is positive definite (subscripts -1 is purely a notational choice).
Keeping in mind such factorization we express the initial state $x_0$ as
$$
x_0 = x_0^- + G_{-1} w_{-1}
$$
where noise $w_{-1}$ is assumed to have covariance $Q_{-1}$.</p><p>The state estimates are searched by considering the cost function:
$$
\begin{split}
J(x, w) =& \frac{1}{2} w_{-1}^T Q_{-1}^{-1} w_{-1} \\
+& \frac{1}{2} \sum_{k = 0}^{N - 1} (H_k x_k - z_k)^T R_k^{-1} (H_k x_k - z_k) \\
+& \frac{1}{2} \sum_{k = 0}^{N - 1} w_k^T Q_k^{-1} w_k
\end{split}
$$
Which is minimized subject to the state transition equations:
$$
\min_{x, w} J(x, w) \text{ subject to } \\
x_0 = x_0^- + G_{-1} w_{-1} \\
x_{k + 1} = F_k x_k + G_k w_k
$$</p><p>The problem is solved by finding the stationary point of the Lagrange function.
It gives the following discrete-time boundary value problem:
$$
x_{k + 1} = F_k x_k + G_k Q_k G_k^T \lambda_{k+1} \\
\lambda_k = F_k^T \lambda_{k + 1} + H_k^T R_k^{-1} \left(z_k - H_k x_k\right) \\
P_0^- \lambda_0 = x_0 - x_0^- \\
\lambda_N = 0
$$
Where $\lambda_k$ are introduced Lagrange multipliers. From them the noise vectors are computed as
$$
w_k = Q_k G_k^T \lambda_{k+1} \\
$$</p><p>Note that our approach to consider the singular initial covariance came down to writing the boundary condition $\lambda_0 = \left(P_0^-\right)^{-1} \left(x_0 - x_0^-\right)$ as $P_0^- \lambda_0 = x_0 - x_0^-$.
The rigorous derivation gave the only plausible result.
Also note that at this point there is no notion of covariance matrices (besides $P_0$), let alone their positive definitiveness.</p><h1 id=some-matrix-identities-related-to-kalman-filtering>Some matrix identities related to Kalman filtering<a hidden class=anchor aria-hidden=true href=#some-matrix-identities-related-to-kalman-filtering>#</a></h1><p>As a starting point we take the standard Kalman correction formula with the optimal gain matrix $K$:
$$
K = P^- H^T (H P^- H^T + R)^{-1} \\
P^+ = (I - K H) P^-
$$
The formula is applicable for any covariance matrix $P^-$ as long as $R$ is positive definite, which we already assume in our problem.</p><p>The last equation can be written as
$$
P^- - P^+ = K H P^- = P^- H^T K^T
$$</p><p>Let&rsquo;s combine it with the gain equation as follows:
$$
K (H P^- H^T + R) = P^- H^T \\
K H P^- H^T = P^- H^T - P^+ H^T
$$
Subtracting the equations we get the following alternative expression for $K$:
$$
K = P^+ H^T R^{-1}
$$</p><p>Using this alternative expression we can also write
$$
P^- - P^+ = P^+ H^T R^{-1} H P^- = P^- H^T R^{-1} H P^+
$$</p><p>Now let&rsquo;s prove the identity:
$$
(I + H^T R^{-1} H P^-)^{-1} = I - H^T R^{-1} H P^+ = I - H^T K^T
$$
To do that we show that the multiplications on both sides give the identity matrix:
$$
\begin{split}
(I + H^T R^{-1} H P^-) (I - H^T R^{-1} H P^+) = I + H^T R^{-1} H (P^- - P^+ - P^- H^T R^{-1} H P^+) = I \\
(I - H^T R^{-1} H P^+) (I + H^T R^{-1} H P^-) = I + H^T R^{-1} H (P^- - P^+ - P^+ H^T R^{-1} H P^-) = I
\end{split}
$$</p><p>By taking its transpose we get
$$
(I + P^- H^T R^{-1} H)^{-1} = I - P^+ H^T R^{-1} H = I - K H
$$</p><p>Similarly another formula can be proven
$$
(R + H P^- H^T)^{-1} = R^{-1} - R^{-1} H P^+ H^T R^{-1}
$$</p><p>Equipped with the derived identities we can carry on with solving the boundary-value problem.</p><h1 id=equivalent-problem>Equivalent problem<a hidden class=anchor aria-hidden=true href=#equivalent-problem>#</a></h1><p>First we prove the equivalence of the problems:
$$
x_{k + 1} = F_k x_k + G_k Q_k G_k^T \lambda_{k+1} \\
\lambda_k = F_k^T \lambda_{k + 1} + H_k^T R_k^{-1} \left(z_k - H_k x_k\right) \\
P_0^- \lambda_0 = x_0 - x_0^- \\
\lambda_N = 0 \\
\downdownarrows \upuparrows \\
x_k = x_k^- + P_k^- \lambda_k \\
\lambda_k = F_k^T \lambda_{k + 1} + H_k^T R_k^{-1} \left(z_k - H_k x_k\right) \\
\lambda_N = 0
$$
Meaning that any solution to the first problem is the solution to the second problem and vice versa.
Or that the set of solutions are identical.</p><p>For convenience let&rsquo;s call $x_k = x_k^- + P_k^- \lambda_k$ the &#171;magic equation&#187;.</p><h2 id=proof-from-up-to-down>Proof from up to down<a hidden class=anchor aria-hidden=true href=#proof-from-up-to-down>#</a></h2><p>We need to prove that the magic equation follows from the difference equations.</p><p>We do it by induction.
For $k = 0$ it&rsquo;s true as the boundary condition.
Now assume that it&rsquo;s true for some $k$ and substitute the expression for $\lambda_k$ into it:
$$
x_k = x_k^- + P_k^- F_k^T \lambda_{k + 1} + P_k^- H_k^T R_k^{-1} z_k - P_k^- H_k^T R_k^{-1} H_k x_k \\
(I + P_k^- H_k^T R_k^{-1} H_k) x_k = x_k^- + P_k^- F_k^T \lambda_{k + 1} + P_k^- H_k^T R_k^{-1} z_k \\
$$
Now using the inverse formula for the matrix multiplier on the left we get
$$
x_k = (I - K_k H_k) x_k^- + P_k^+ F_k^T \lambda_{k + 1} + P_k^+ H_k^T R_k^{-1} z_k
$$
Here we spot the expression for $K_k$ before $z_k$ and rewrite the equation as
$$
x_k = x_k^- + K_k (z_k - H_k x_k^-) + P^+_k F_k^T \lambda_{k+1} = x_k^+ + P_k^+ F_k^T \lambda_{k+1}
$$
And finally substitute this expression into recursion for $x$:
$$
x_{k + 1} = F_k x_k^+ + (F_k^T P_k^+ F_k + G_k Q_k G_k^T) \lambda_{k + 1} \\
x_{k + 1} = x_{k + 1}^- + P_{k + 1}^- \lambda_{k + 1}
$$
Which proves the step of the induction and with that the equation for all $k$.</p><h2 id=proof-from-down-to-up>Proof from down to up<a hidden class=anchor aria-hidden=true href=#proof-from-down-to-up>#</a></h2><p>We need to prove that the difference equation for $x$ follows from the magic equation and the difference equation for $\lambda$.</p><p>Substitute the expression for $\lambda_k$ into the magic equation to get (derived above already)
$$
x_k = x_k^+ + P_k^+ F_k^T \lambda_{k+1} \\
x_k^+ = x_k - P_k^+ F_k^T \lambda_{k+1}
$$
Now substitute $x_k^+$ into the magic equation for $k + 1$:
$$
\begin{split}
x_{k + 1} = x_{k + 1}^- + P_{k + 1}^- \lambda_{k + 1} = F_k x_k^+ + P^-_{k + 1} \lambda_{k + 1}
= F_k x_k + (P_{k + 1}^- - F_k P_k^+ F_k^T) \lambda_{k + 1} = \\ = F_k x_k + G_k Q_k G_k^T \lambda_{k + 1}
\end{split}
$$</p><h1 id=solution-to-the-problem>Solution to the problem<a hidden class=anchor aria-hidden=true href=#solution-to-the-problem>#</a></h1><p>With the proof from the previous section we can solve the more simple equivalent problem
$$
x_k = x_k^- + P_k^- \lambda_k \\
\lambda_k = F_k^T \lambda_{k + 1} + H_k^T R_k^{-1} \left(z_k - H_k x_k\right) \\
\lambda_N = 0
$$
and be sure to get the solution of the original problem.</p><h2 id=rts-form>RTS form<a hidden class=anchor aria-hidden=true href=#rts-form>#</a></h2><p>To derive the RTS form we consider the already derived corollary equation along with the magic equation:
$$
x_k = x_k^- + P_k^- \lambda_k \\
x_k = x_k^+ + P_k^+ F_k^T \lambda_{k+1}
$$
If all $P_k^-$ are invertible the difference equation for $\lambda$ follows from them:
$$
x_k^- + P_k^- \lambda_k = x_k^+ + P_k^+ F_k^T \lambda_{k+1} \\
\lambda_k = (P_k^-)^{-1} P_k^+ F_k^T \lambda_{k+1} + (P_k^-)^{-1} (x_k^+ - x_k^-) \\
\lambda_k = F_k^T \lambda_{k + 1} + H_k^T R_k^{-1} \left(z_k - H_k x_k\right)
$$
It means the two equations are equivalent to the original problem.</p><p>Rewriting them slightly different
$$
x_{k + 1} = x_{k + 1}^- + P_k^- \lambda_{k + 1} \\
x_k = x_k^+ + P_k^+ F_k^T \lambda_{k+1}
$$
we can eliminate $\lambda_{k + 1}$ and come up with the unique solution:
$$
x_N^s = x_N^- \\
x_k^s = x_k^+ + P_k^+ F_k^T \left(P_{k + 1}^-\right)^{-1} \left(x_{k + 1}^s - x_{k + 1}^-\right)
$$
and
$$
\lambda_k^s = (P_k^-)^{-1} (x_k^s - x_k^-) \\
w_k^s = Q_k G_k^T \lambda_{k+1}^s
$$</p><h2 id=form-with-explicit-recursion-for-lambda>Form with explicit recursion for $\lambda$<a hidden class=anchor aria-hidden=true href=#form-with-explicit-recursion-for-lambda>#</a></h2><p>If $P_k^-$ is singular then the two equations from the RTS derivation are not equivalent to the original problem.
Meaning that there is no unique solution to them and an arbitrary solution is not guaranteed to be the solution of the original problem.</p><p>We go back to the original equation for $\lambda_k$ and substitute $x_k = x_k^- + P_k^- \lambda_k$ into it:
$$
\lambda_k = F_k^T \lambda_{k+1} + H_k^T R_k^{-1} (z_ k - H_k x_k^- - H_k P_k^- \lambda_k) \\
(I + H_k^T R_k^{-1} H_k P_k^-) \lambda_k = F_k^T \lambda_{k+1} + H_k^T R_k^{-1} (z_k - H_k x_k^-)
$$
Taking the inverse of the matrix multiplier on the left we get
$$
\lambda_k = (I - K_k^T H_k^T) F_k^T \lambda_{k + 1} + (I - H_k^T R_k^{-1} H_k P^+_k) H_k^T R_k^{-1} (z_k - H_k x_k^-) \\
\lambda_k = (I - K_k^T H_k^T) F_k^T \lambda_{k + 1} + H_k^T (H_k P^-_k H_k^T + R_k)^{-1} (z_k - H_k x_k^-) \\
$$
Introduce some notation:</p><ul><li>$\Psi_k = F_k (I - K_k H_k)$ &ndash; can be though of as a total matrix for the state propagation, combing filter correction and prediction steps</li><li>$S_k = H_k P^-_k H_k^T + R_k$ &ndash; covariance matrix of the innovation vector $z_k - H_k x_k^-$</li></ul><p>Using it the recursion can be written as
$$
\lambda_N^s = 0 \\
\lambda_k^s = \Psi_k^T \lambda^s_{k + 1} + H_k^T S_k^{-1} (z_k - H_k x_k^-)
$$</p><p>From $\lambda^s_k$ we can compute optimal state and noise estimates as
$$
x_k^s = x_k^- + P_k^- \lambda^s_k = x_k^+ + P_k^+ F_k^T \lambda_{k + 1}^s \\
w_k^s = Q_k G_k^T \lambda^s_{k + 1}
$$</p><h2 id=uniqueness-of-the-solution>Uniqueness of the solution<a hidden class=anchor aria-hidden=true href=#uniqueness-of-the-solution>#</a></h2><p>We see that the solution is unique, because $\lambda_k^s$ obeys a deterministic recursion process starting from $\lambda_N^s = 0$.
And $x_k^s$ and $w_k^s$ are uniquely determined from $\lambda_k^s$.
This fact doesn&rsquo;t depend on whether $P_k^-$ is singular or not.</p><p>One thing that may seem a bit confusing is that when $P_k^-$ is singular the equation $x_k = x_k^- + P_k^- \lambda_k$ is satisfied for an infinite set of $\lambda_k$.
However $\lambda_k$ determine the noise vectors $w_k$ which purpose is to make state transition equations consistent.
Thus arbitrary adjustment of individual $\lambda_k$ is not possible, they are determined by the recursion equation uniquely.</p><h1 id=covariance-computation>Covariance computation<a hidden class=anchor aria-hidden=true href=#covariance-computation>#</a></h1><p>To compute covariance of the estimate errors we recursively update covariance of $\lambda_k^s$ as:
$$
\Lambda^s_k = \Psi^T_k \Lambda^s_{k + 1} \Psi_k + H_k^T S_k^{-1} H_k \\
\Lambda^s_N = 0
$$
Then using the same reasoning as for the RTS form we get the following expressions for error covariances:
$$
P_k^s = P_k^- - P_k^- \Lambda^s_k P_k^- = P_k^+ - P_k^+ F_k^T \Lambda^s_{k + 1} F_k P_k^+ \\
Q_k^s = Q_k - Q_k G_k^T \Lambda^s_{k + 1} G_k Q_k
$$</p><h1 id=algorithmic-aspects-multiple-or-missing-measurements>Algorithmic aspects: multiple or missing measurements<a hidden class=anchor aria-hidden=true href=#algorithmic-aspects-multiple-or-missing-measurements>#</a></h1><p>In practical algorithms its better to assume that at each epoch there is an arbitrary number of independent measurements (possibly zero).
In this regard its better to split $\lambda_k^s$ and $\Lambda_k^s$ computation into (backward) &#171;prediction&#187; and &#171;correction&#187; steps similar to Kalman filter.</p><p>The prediction step:
$$
\lambda_{k}^- = F_k^T \lambda_{k + 1}^+ \\
\Lambda_{k}^- = F_k^T \Lambda_{k + 1}^+ F_k
$$
The correction step:
$$
\lambda_{k}^+ = (I - K_k H_k)^T \lambda_{k}^- + H_k^T S_k^{-1} (z_k - H_k x_k^-) \\
\Lambda_{k}^+ = (I - K_k H_k)^T \Lambda_{k}^- (I - K_k H_k) + H_k^T S_k^{-1} H_k
$$
It is repeated for each measurement, applying the formulas to the current estimate.
The gain vectors, innovations and their covariances are saved from the filter pass.
The crucial point is that the formulas are applied in the <em>reverse</em> to the order in which the measurements were processed in the filter.
In the end the result is the &#171;smoothed&#187; estimate $\lambda_k^s$ which is used to compute $x_k^s$ and $w_k^s$.</p><p>The correctness of such approach can be understood by considering intermediate states between the measurements.</p><h1 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h1><p>An alternative form of the Kalman smoother which doesn&rsquo;t assume or require positive definite covariance matrices was derived.
This form is somewhat more involved to implement.
However for practical algorithms the main requirements are robustness and universality and this alternative algorithm seems to fullfil them better.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://nmayorov.github.io/>Navigating Uncertainty</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>